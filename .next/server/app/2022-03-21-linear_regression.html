<!DOCTYPE html><!--xTAnN8PQ3b6_LdzNENhZ7--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/51e5ba5c7de07f80.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5eacd01f773eed7f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-1c8036505c2e140d.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-c0bfd06c366bd1e0.js" async=""></script><script src="/_next/static/chunks/main-app-dcd8b77c1bc52c83.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js" async=""></script><title>Sehoon&#x27;s Workspace</title><meta name="description" content="Welcome to my page!"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-screen flex flex-col font-sans"><div hidden=""><!--$--><!--/$--></div><div class="flex gap-10"><article class="flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none"><header class="mb-8 not-prose border-b pb-8"><h1 class="text-4xl font-bold mb-4">[ML] Linear Regression 정리</h1><div class="flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400"><time dateTime="2022-03-21">March 21, 2022</time></div></header><p>학교 수업으로 타학과 전공인 &#x27;인공지능과 딥러닝&#x27;을 수강하게 되었다.
오일석 교수님의 [Machine Learning 기계학습] 을 기반으로 진행되는 강의이다.</p>
<p>Perceptron을 설명하면서 Linear regression을 직접 코딩을 통해 실습하는 시간을 가졌는데, 아래의 내용은 그 실습 내용을 정리한 것이다.</p>
<h1 id="l" class="text-3xl font-bold mt-8 mb-4">Linear regression</h1>
<p>Author: Seungjae Lee(이승재)</p>
<p>모두를 위한 딥러닝을 참고하였습니다.</p>
<h2 id="t" class="text-2xl font-bold mt-8 mb-4">Theoretical Overview</h2>
<p>$ H(x) = Wx + b $</p>
<p>$ cost(W, b) = \frac{1}{m} \sum^m_{i=1} \left( H(x^{(i)}) - y^{(i)} \right)^2 $</p>
<ul>
<li>$H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가</li>
<li>$cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가</li>
</ul>
<h2 id="i" class="text-2xl font-bold mt-8 mb-4">Import</h2>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
</code></pre>
<h2 id="f" class="text-2xl font-bold mt-8 mb-4">For reproducibility</h2>
<pre><code>torch.manual_seed(1)
</code></pre>
<h2 id="d" class="text-2xl font-bold mt-8 mb-4">Data</h2>
<p>다음 예제를 위해 예시 데이터를 사용하여보자.
(We will use fake data for this example.)</p>
<pre><code>x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
print(x_train)
print(x_train.shape)
print(y_train)
print(y_train.shape)
</code></pre>
<p>기본적으로 Pytorch는 NCHW 형태이다.</p>
<h2 id="w" class="text-2xl font-bold mt-8 mb-4">Weight Initialization</h2>
<pre><code>W = torch.zeros(1, requires_grad=True)
print(W)
b = torch.zeros(1, requires_grad=True)
print(b)
</code></pre>
<h2 id="h" class="text-2xl font-bold mt-8 mb-4">Hypothesis</h2>
<p>$ H(x) = Wx + b $</p>
<pre><code>hypothesis = x_train * W + b
print(hypothesis)
</code></pre>
<h2 id="c" class="text-2xl font-bold mt-8 mb-4">Cost</h2>
<p>$ cost(W, b) = \frac{1}{m} \sum^m_{i=1} \left( H(x^{(i)}) - y^{(i)} \right)^2 $</p>
<pre><code>print(hypothesis)
print(y_train)
print(hypothesis - y_train)
print((hypothesis - y_train) ** 2)
cost = torch.mean((hypothesis - y_train) ** 2)
print(cost)
</code></pre>
<h2 id="g" class="text-2xl font-bold mt-8 mb-4">Gradient Descent</h2>
<pre><code>optimizer = optim.SGD([W, b], lr = 0.01)
optimizer.zero_grad()
cost.backward()
optimizer.step()
print(W)
print(b)
</code></pre>
<p>가설이 잘 작동하는지 확인하여 보자.
(Let&#x27;s check if the hypothesis is now better.)</p>
<pre><code>hypothesis = x_train * W + b
print(hypothesis)
cost = torch.mean((hypothesis - y_train) ** 2)
print(cost)
</code></pre>
<h2 id="t" class="text-2xl font-bold mt-8 mb-4">Training with Full Code</h2>
<p>In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops.</p>
<pre><code># 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])

# 모델 초기화
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer 설정
optimizer = optim.SGD([W, b], lr = 0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
        
    # H(x) 계산
    hypothesis = x_train * W + b
        
    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)
        
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
        
    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print(&#x27;Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}&#x27;.format(
                epoch, nb_epochs, W.item(), b.item(), cost.item()
        ))
</code></pre>
<h2 id="high-level-implementaion-with" class="text-2xl font-bold mt-8 mb-4">High-level implementaion with <code>nn.Module</code></h2>
<p>Remember that we had this fake data.</p>
<pre><code>x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])
</code></pre>
<h3 id="linear-regression-pytorch" class="text-xl font-bold mt-6 mb-3">이제 linear regression 모델을 만들면 되는데, 기본적으로 Pytorch의 모든 모델은 제공되는 <code>nn.Module</code>을 inherit 해서 만들게 된다.</h3>
<pre><code>class LinearRegressionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(1, 1)
        
    def forward(self, x):
        return self.linear(x)
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">모델의 <code>__init__</code>에서는 사용할 레이어들을 정의하게 된다. 여기서 우리는 linear regression 모델을 만들기 때문에, <code>nn.Linear</code>를 이용할 것이다. 그리고 <code>forward</code>에서는 이 모델이 어떻게 입력값에서 출력값을 계산하는지 알려준다.</h3>
<pre><code>model = LinearRegressionModel()
</code></pre>
<h2 id="h" class="text-2xl font-bold mt-8 mb-4">Hypothesis</h2>
<p>이제 모델을 생성해서 예측값 <em>H(x)</em> 를 구해보자</p>
<pre><code>hypothesis = model(x_train)
print(hypothesis)
</code></pre>
<h2 id="c" class="text-2xl font-bold mt-8 mb-4">Cost</h2>
<p>이제 mean squared error (MSE)로 cost를 구해보자. MSE 역시 PyTorch에서 기본적으로 제공한다.</p>
<pre><code>print(hypothesis)
print(y_train)
cost = F.mse_loss(hypothesis, y_train)
print(cost)
</code></pre>
<h2 id="g" class="text-2xl font-bold mt-8 mb-4">Gradient Descent</h2>
<p>마지막 주어진 cost를 이용해 <em>H(x)</em> 의 <em>W</em> , <em>b</em> 를 바꾸어서 cost를 줄여봅시다. 이때 PyTorch의 <code>torch.optim</code>에 있는 <code>optimizer</code>들 중 하나를 사용할 수 있다.</p>
<pre><code>optimizer = optim.SGD(model.parameters(), lr=0.01)
optimizer.zero_grad()
cost.backward()
optimizer.step()
</code></pre>
<h2 id="t" class="text-2xl font-bold mt-8 mb-4">Training with Full Code</h2>
<p>이제 Linear Regression 코드를 이해했으니, 실제로 코드를 돌려 피팅하여보자.</p>
<pre><code># 데이터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[1], [2], [3]])

# 모델 초기화
model = LinearRegressionModel()

# optimizer 설정
optimizer = optim.SGD(model.parameters(), lr=0.01)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    
    # H(x) 계산
    prediction = model(x_train)
        
    # cost 계산
    cost = F.mse_loss(prediction, y_train)
        
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
        
    # 100번 마다 로그 출력
    if epoch % 100 == 0:
        params = list(model.parameters())
        W = params[0].item()
        b = params[1].item()
        print(&#x27;Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}&#x27;.format(
            epoch, nb_epochs, W, b, cost.item()
        ))
    
</code></pre>
<p>점점 <em>H(x)</em> 의 <em>W</em> 와 <em>b</em> 를 조정해서 cost가 줄어드는 것을 볼 수 있다.</p><div class="mt-10 border-t pt-10"></div></article></div><!--$--><!--/$--><script src="/_next/static/chunks/webpack-1c8036505c2e140d.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9766,[],\"\"]\n3:I[8924,[],\"\"]\n5:I[4431,[],\"OutletBoundary\"]\n7:I[5278,[],\"AsyncMetadataOutlet\"]\n9:I[4431,[],\"ViewportBoundary\"]\nb:I[4431,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[7150,[],\"\"]\n:HL[\"/_next/static/css/51e5ba5c7de07f80.css\",\"style\"]\n:HL[\"/_next/static/css/5eacd01f773eed7f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"xTAnN8PQ3b6-LdzNENhZ7\",\"p\":\"\",\"c\":[\"\",\"2022-03-21-linear_regression\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"2022-03-21-linear_regression\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/51e5ba5c7de07f80.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-screen flex flex-col font-sans\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"slug\",\"2022-03-21-linear_regression\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5eacd01f773eed7f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"flex gap-10\",\"children\":[[\"$\",\"article\",null,{\"className\":\"flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8 not-prose border-b pb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold mb-4\",\"children\":\"[ML] Linear Regression 정리\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2022-03-21\",\"children\":\"March 21, 2022\"}],\"$undefined\"]}]]}],[[\"$\",\"p\",\"p-0\",{\"children\":\"학교 수업으로 타학과 전공인 '인공지능과 딥러닝'을 수강하게 되었다.\\n오일석 교수님의 [Machine Learning 기계학습] 을 기반으로 진행되는 강의이다.\"}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":\"Perceptron을 설명하면서 Linear regression을 직접 코딩을 통해 실습하는 시간을 가졌는데, 아래의 내용은 그 실습 내용을 정리한 것이다.\"}],\"\\n\",[\"$\",\"h1\",\"h1-0\",{\"id\":\"l\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"Linear regression\"}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":\"Author: Seungjae Lee(이승재)\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":\"모두를 위한 딥러닝을 참고하였습니다.\"}],\"\\n\",[\"$\",\"h2\",\"h2-0\",{\"id\":\"t\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Theoretical Overview\"}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":\"$$ H(x) = Wx + b $\"}],\"\\n\",[\"$\",\"p\",\"p-5\",{\"children\":\"$$ cost(W, b) = \\\\frac{1}{m} \\\\sum^m_{i=1} \\\\left( H(x^{(i)}) - y^{(i)} \\\\right)^2 $\"}],\"\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"$$H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"$$cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가\"}],\"\\n\"]}],\"\\n\",[\"$\",\"h2\",\"h2-1\",{\"id\":\"i\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Import\"}],\"\\n\",[\"$\",\"pre\",\"pre-0\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"import torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-2\",{\"id\":\"f\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"For reproducibility\"}],\"\\n\",[\"$\",\"pre\",\"pre-1\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"torch.manual_seed(1)\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-3\",{\"id\":\"d\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Data\"}],\"\\n\",[\"$\",\"p\",\"p-6\",{\"children\":\"다음 예제를 위해 예시 데이터를 사용하여보자.\\n(We will use fake data for this example.)\"}],\"\\n\",[\"$\",\"pre\",\"pre-2\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"x_train = torch.FloatTensor([[1], [2], [3]])\\ny_train = torch.FloatTensor([[1], [2], [3]])\\nprint(x_train)\\nprint(x_train.shape)\\nprint(y_train)\\nprint(y_train.shape)\\n\"}]}],\"\\n\",[\"$\",\"p\",\"p-7\",{\"children\":\"기본적으로 Pytorch는 NCHW 형태이다.\"}],\"\\n\",[\"$\",\"h2\",\"h2-4\",{\"id\":\"w\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Weight Initialization\"}],\"\\n\",[\"$\",\"pre\",\"pre-3\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"W = torch.zeros(1, requires_grad=True)\\nprint(W)\\nb = torch.zeros(1, requires_grad=True)\\nprint(b)\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-5\",{\"id\":\"h\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Hypothesis\"}],\"\\n\",[\"$\",\"p\",\"p-8\",{\"children\":\"$$ H(x) = Wx + b $\"}],\"\\n\",[\"$\",\"pre\",\"pre-4\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"hypothesis = x_train * W + b\\nprint(hypothesis)\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-6\",{\"id\":\"c\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Cost\"}],\"\\n\",[\"$\",\"p\",\"p-9\",{\"children\":\"$$ cost(W, b) = \\\\frac{1}{m} \\\\sum^m_{i=1} \\\\left( H(x^{(i)}) - y^{(i)} \\\\right)^2 $\"}],\"\\n\",[\"$\",\"pre\",\"pre-5\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"print(hypothesis)\\nprint(y_train)\\nprint(hypothesis - y_train)\\nprint((hypothesis - y_train) ** 2)\\ncost = torch.mean((hypothesis - y_train) ** 2)\\nprint(cost)\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-7\",{\"id\":\"g\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Gradient Descent\"}],\"\\n\",[\"$\",\"pre\",\"pre-6\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"optimizer = optim.SGD([W, b], lr = 0.01)\\noptimizer.zero_grad()\\ncost.backward()\\noptimizer.step()\\nprint(W)\\nprint(b)\\n\"}]}],\"\\n\",[\"$\",\"p\",\"p-10\",{\"children\":\"가설이 잘 작동하는지 확인하여 보자.\\n(Let's check if the hypothesis is now better.)\"}],\"\\n\",[\"$\",\"pre\",\"pre-7\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"hypothesis = x_train * W + b\\nprint(hypothesis)\\ncost = torch.mean((hypothesis - y_train) ** 2)\\nprint(cost)\\n\"}]}],\"\\n\",[\"$\",\"h2\",\"h2-8\",{\"id\":\"t\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Training with Full Code\"}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\"],\"$L25\"]}],\"$L26\"]}]\n"])</script><script>self.__next_f.push([1,"27:I[3089,[\"182\",\"static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js\"],\"default\"]\n28:I[4010,[\"182\",\"static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js\"],\"default\"]\nf:[\"$\",\"p\",\"p-11\",{\"children\":\"In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops.\"}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"pre\",\"pre-8\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"# 데이터\\nx_train = torch.FloatTensor([[1], [2], [3]])\\ny_train = torch.FloatTensor([[1], [2], [3]])\\n\\n# 모델 초기화\\nW = torch.zeros(1, requires_grad=True)\\nb = torch.zeros(1, requires_grad=True)\\n\\n# optimizer 설정\\noptimizer = optim.SGD([W, b], lr = 0.01)\\n\\nnb_epochs = 1000\\nfor epoch in range(nb_epochs + 1):\\n        \\n    # H(x) 계산\\n    hypothesis = x_train * W + b\\n        \\n    # cost 계산\\n    cost = torch.mean((hypothesis - y_train) ** 2)\\n        \\n    # cost로 H(x) 개선\\n    optimizer.zero_grad()\\n    cost.backward()\\n    optimizer.step()\\n        \\n    # 100번마다 로그 출력\\n    if epoch % 100 == 0:\\n        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\\n                epoch, nb_epochs, W.item(), b.item(), cost.item()\\n        ))\\n\"}]}]\n"])</script><script>self.__next_f.push([1,"11:[\"$\",\"h2\",\"h2-9\",{\"id\":\"high-level-implementaion-with\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":[\"High-level implementaion with \",[\"$\",\"code\",\"code-0\",{\"children\":\"nn.Module\"}]]}]\n12:[\"$\",\"p\",\"p-12\",{\"children\":\"Remember that we had this fake data.\"}]\n13:[\"$\",\"pre\",\"pre-9\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"x_train = torch.FloatTensor([[1], [2], [3]])\\ny_train = torch.FloatTensor([[1], [2], [3]])\\n\"}]}]\n14:[\"$\",\"h3\",\"h3-0\",{\"id\":\"linear-regression-pytorch\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":[\"이제 linear regression 모델을 만들면 되는데, 기본적으로 Pytorch의 모든 모델은 제공되는 \",[\"$\",\"code\",\"code-0\",{\"children\":\"nn.Module\"}],\"을 inherit 해서 만들게 된다.\"]}]\n15:[\"$\",\"pre\",\"pre-10\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"class LinearRegressionModel(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.linear = nn.Linear(1, 1)\\n        \\n    def forward(self, x):\\n        return self.linear(x)\\n\"}]}]\n16:[\"$\",\"h3\",\"h3-1\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":[\"모델의 \",[\"$\",\"code\",\"code-0\",{\"children\":\"__init__\"}],\"에서는 사용할 레이어들을 정의하게 된다. 여기서 우리는 linear regression 모델을 만들기 때문에, \",[\"$\",\"code\",\"code-1\",{\"children\":\"nn.Linear\"}],\"를 이용할 것이다. 그리고 \",[\"$\",\"code\",\"code-2\",{\"children\":\"forward\"}],\"에서는 이 모델이 어떻게 입력값에서 출력값을 계산하는지 알려준다.\"]}]\n17:[\"$\",\"pre\",\"pre-11\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"model = LinearRegressionModel()\\n\"}]}]\n18:[\"$\",\"h2\",\"h2-10\",{\"id\":\"h\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Hypothesis\"}]\n19:[\"$\",\"p\",\"p-13\",{\"children\":[\"이제 모델을 생성해서 예측값 \",[\"$\",\"em\",\"em-0\",{\"children\":\"H(x)\"}],\" 를 구해보자\"]}]\n1a:[\"$\",\"pre\",\"pre-12\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"hypothesis = model(x_train)\\nprint(hypothesis)\\n\"}]}]\n1b:[\"$\",\"h2\",\"h2-11\",{\"id\":\"c\",\"className\":\"text-2xl font-b"])</script><script>self.__next_f.push([1,"old mt-8 mb-4\",\"children\":\"Cost\"}]\n1c:[\"$\",\"p\",\"p-14\",{\"children\":\"이제 mean squared error (MSE)로 cost를 구해보자. MSE 역시 PyTorch에서 기본적으로 제공한다.\"}]\n1d:[\"$\",\"pre\",\"pre-13\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"print(hypothesis)\\nprint(y_train)\\ncost = F.mse_loss(hypothesis, y_train)\\nprint(cost)\\n\"}]}]\n1e:[\"$\",\"h2\",\"h2-12\",{\"id\":\"g\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Gradient Descent\"}]\n1f:[\"$\",\"p\",\"p-15\",{\"children\":[\"마지막 주어진 cost를 이용해 \",[\"$\",\"em\",\"em-0\",{\"children\":\"H(x)\"}],\" 의 \",[\"$\",\"em\",\"em-1\",{\"children\":\"W\"}],\" , \",[\"$\",\"em\",\"em-2\",{\"children\":\"b\"}],\" 를 바꾸어서 cost를 줄여봅시다. 이때 PyTorch의 \",[\"$\",\"code\",\"code-0\",{\"children\":\"torch.optim\"}],\"에 있는 \",[\"$\",\"code\",\"code-1\",{\"children\":\"optimizer\"}],\"들 중 하나를 사용할 수 있다.\"]}]\n20:[\"$\",\"pre\",\"pre-14\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"optimizer = optim.SGD(model.parameters(), lr=0.01)\\noptimizer.zero_grad()\\ncost.backward()\\noptimizer.step()\\n\"}]}]\n21:[\"$\",\"h2\",\"h2-13\",{\"id\":\"t\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Training with Full Code\"}]\n22:[\"$\",\"p\",\"p-16\",{\"children\":\"이제 Linear Regression 코드를 이해했으니, 실제로 코드를 돌려 피팅하여보자.\"}]\n"])</script><script>self.__next_f.push([1,"23:[\"$\",\"pre\",\"pre-15\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"# 데이터\\nx_train = torch.FloatTensor([[1], [2], [3]])\\ny_train = torch.FloatTensor([[1], [2], [3]])\\n\\n# 모델 초기화\\nmodel = LinearRegressionModel()\\n\\n# optimizer 설정\\noptimizer = optim.SGD(model.parameters(), lr=0.01)\\n\\nnb_epochs = 1000\\nfor epoch in range(nb_epochs + 1):\\n    \\n    # H(x) 계산\\n    prediction = model(x_train)\\n        \\n    # cost 계산\\n    cost = F.mse_loss(prediction, y_train)\\n        \\n    # cost로 H(x) 개선\\n    optimizer.zero_grad()\\n    cost.backward()\\n    optimizer.step()\\n        \\n    # 100번 마다 로그 출력\\n    if epoch % 100 == 0:\\n        params = list(model.parameters())\\n        W = params[0].item()\\n        b = params[1].item()\\n        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\\n            epoch, nb_epochs, W, b, cost.item()\\n        ))\\n    \\n\"}]}]\n"])</script><script>self.__next_f.push([1,"24:[\"$\",\"p\",\"p-17\",{\"children\":[\"점점 \",[\"$\",\"em\",\"em-0\",{\"children\":\"H(x)\"}],\" 의 \",[\"$\",\"em\",\"em-1\",{\"children\":\"W\"}],\" 와 \",[\"$\",\"em\",\"em-2\",{\"children\":\"b\"}],\" 를 조정해서 cost가 줄어드는 것을 볼 수 있다.\"]}]\n25:[\"$\",\"$L27\",null,{}]\n29:T170f,"])</script><script>self.__next_f.push([1,"\n학교 수업으로 타학과 전공인 '인공지능과 딥러닝'을 수강하게 되었다.\n오일석 교수님의 [Machine Learning 기계학습] 을 기반으로 진행되는 강의이다.\n\nPerceptron을 설명하면서 Linear regression을 직접 코딩을 통해 실습하는 시간을 가졌는데, 아래의 내용은 그 실습 내용을 정리한 것이다.\n\n\n# Linear regression\nAuthor: Seungjae Lee(이승재)\n\n모두를 위한 딥러닝을 참고하였습니다.\n\n## Theoretical Overview\n$ H(x) = Wx + b $\n\n$ cost(W, b) = \\\\frac{1}{m} \\\\sum^m_{i=1} \\\\left( H(x^{(i)}) - y^{(i)} \\\\right)^2 $\n\n- $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\n- $cost(W, b)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가\n\n## Import\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n\n## For reproducibility\n    torch.manual_seed(1)\n\n## Data\n다음 예제를 위해 예시 데이터를 사용하여보자.\n(We will use fake data for this example.) \n\n    x_train = torch.FloatTensor([[1], [2], [3]])\n    y_train = torch.FloatTensor([[1], [2], [3]])\n    print(x_train)\n    print(x_train.shape)\n    print(y_train)\n    print(y_train.shape)\n기본적으로 Pytorch는 NCHW 형태이다.\n\n## Weight Initialization\n    W = torch.zeros(1, requires_grad=True)\n    print(W)\n    b = torch.zeros(1, requires_grad=True)\n    print(b)\n\n## Hypothesis\n$ H(x) = Wx + b $\n\n    hypothesis = x_train * W + b\n    print(hypothesis)\n\n## Cost\n$ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1} \\left( H(x^{(i)}) - y^{(i)} \\right)^2 $\n\n    print(hypothesis)\n    print(y_train)\n    print(hypothesis - y_train)\n    print((hypothesis - y_train) ** 2)\n    cost = torch.mean((hypothesis - y_train) ** 2)\n    print(cost)\n\n## Gradient Descent\n    optimizer = optim.SGD([W, b], lr = 0.01)\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n    print(W)\n    print(b)\n\n가설이 잘 작동하는지 확인하여 보자.\n(Let's check if the hypothesis is now better.)\n\n    hypothesis = x_train * W + b\n    print(hypothesis)\n    cost = torch.mean((hypothesis - y_train) ** 2)\n    print(cost)\n\n## Training with Full Code\nIn reality, we will be training on the dataset for multiple epochs. This can be done simply with loops.\n\n    # 데이터\n    x_train = torch.FloatTensor([[1], [2], [3]])\n    y_train = torch.FloatTensor([[1], [2], [3]])\n\n    # 모델 초기화\n    W = torch.zeros(1, requires_grad=True)\n    b = torch.zeros(1, requires_grad=True)\n\n    # optimizer 설정\n    optimizer = optim.SGD([W, b], lr = 0.01)\n\n    nb_epochs = 1000\n    for epoch in range(nb_epochs + 1):\n            \n        # H(x) 계산\n        hypothesis = x_train * W + b\n            \n        # cost 계산\n        cost = torch.mean((hypothesis - y_train) ** 2)\n            \n        # cost로 H(x) 개선\n        optimizer.zero_grad()\n        cost.backward()\n        optimizer.step()\n            \n        # 100번마다 로그 출력\n        if epoch % 100 == 0:\n            print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n                    epoch, nb_epochs, W.item(), b.item(), cost.item()\n            ))\n\n## High-level implementaion with `nn.Module`\nRemember that we had this fake data.\n\n    x_train = torch.FloatTensor([[1], [2], [3]])\n    y_train = torch.FloatTensor([[1], [2], [3]])\n\n### 이제 linear regression 모델을 만들면 되는데, 기본적으로 Pytorch의 모든 모델은 제공되는 `nn.Module`을 inherit 해서 만들게 된다.\n\n    class LinearRegressionModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = nn.Linear(1, 1)\n            \n        def forward(self, x):\n            return self.linear(x)\n\n### 모델의 `__init__`에서는 사용할 레이어들을 정의하게 된다. 여기서 우리는 linear regression 모델을 만들기 때문에, `nn.Linear`를 이용할 것이다. 그리고 `forward`에서는 이 모델이 어떻게 입력값에서 출력값을 계산하는지 알려준다.\n\n    model = LinearRegressionModel()\n\n## Hypothesis\n이제 모델을 생성해서 예측값 *H(x)* 를 구해보자\n\n    hypothesis = model(x_train)\n    print(hypothesis)\n\n## Cost\n이제 mean squared error (MSE)로 cost를 구해보자. MSE 역시 PyTorch에서 기본적으로 제공한다.\n\n    print(hypothesis)\n    print(y_train)\n    cost = F.mse_loss(hypothesis, y_train)\n    print(cost)\n\n## Gradient Descent\n마지막 주어진 cost를 이용해 *H(x)* 의 *W* , *b* 를 바꾸어서 cost를 줄여봅시다. 이때 PyTorch의 `torch.optim`에 있는 `optimizer`들 중 하나를 사용할 수 있다.\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n## Training with Full Code\n이제 Linear Regression 코드를 이해했으니, 실제로 코드를 돌려 피팅하여보자.\n\n    # 데이터\n    x_train = torch.FloatTensor([[1], [2], [3]])\n    y_train = torch.FloatTensor([[1], [2], [3]])\n\n    # 모델 초기화\n    model = LinearRegressionModel()\n\n    # optimizer 설정\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    nb_epochs = 1000\n    for epoch in range(nb_epochs + 1):\n        \n        # H(x) 계산\n        prediction = model(x_train)\n            \n        # cost 계산\n        cost = F.mse_loss(prediction, y_train)\n            \n        # cost로 H(x) 개선\n        optimizer.zero_grad()\n        cost.backward()\n        optimizer.step()\n            \n        # 100번 마다 로그 출력\n        if epoch % 100 == 0:\n            params = list(model.parameters())\n            W = params[0].item()\n            b = params[1].item()\n            print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n                epoch, nb_epochs, W, b, cost.item()\n            ))\n        \n점점 *H(x)* 의 *W* 와 *b* 를 조정해서 cost가 줄어드는 것을 볼 수 있다.\n"])</script><script>self.__next_f.push([1,"26:[\"$\",\"$L28\",null,{\"content\":\"$29\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Sehoon's Workspace\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Welcome to my page!\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"d:\"$8:metadata\"\n"])</script></body></html>