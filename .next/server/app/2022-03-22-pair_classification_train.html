<!DOCTYPE html><!--xTAnN8PQ3b6_LdzNENhZ7--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/51e5ba5c7de07f80.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/5eacd01f773eed7f.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-1c8036505c2e140d.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-c0bfd06c366bd1e0.js" async=""></script><script src="/_next/static/chunks/main-app-dcd8b77c1bc52c83.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js" async=""></script><title>Sehoon&#x27;s Workspace</title><meta name="description" content="Welcome to my page!"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-screen flex flex-col font-sans"><div hidden=""><!--$--><!--/$--></div><div class="flex gap-10"><article class="flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none"><header class="mb-8 not-prose border-b pb-8"><h1 class="text-4xl font-bold mb-4">[NLP] 문장 쌍 분류 모델 학습하기</h1><div class="flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400"><time dateTime="Tue Mar 22 2022 09:00:00 GMT+0900 (Korean Standard Time)">March 22, 2022</time></div></header><p>자연어처리의 예제를 학습하여 보자.
다음은 이전 글에서 설명하였던 문장 쌍 분류 모델을 구현한 것이다.</p>
<p>본 파일은 이기창님의 &#x27;Do it! 자연어 처리&#x27;에 기초하여 작성되었다. :)</p>
<h1 id="" class="text-3xl font-bold mt-8 mb-4">문장 쌍 분류 모델 학습하기</h1>
<p>전제와 가설을 검증하는 자연어 추론 모델 만들기</p>
<h2 id="1" class="text-2xl font-bold mt-8 mb-4">1. 각종 설정하기</h2>
<h3 id="t" class="text-xl font-bold mt-6 mb-3">TPU 관련 패키지 설치</h3>
<p>코랩 노트북 초기화 과정에서 하드웨어 가속기로 TPU를 선택했다면 다음 코드를 실행하고, GPU를 선택했다면 실행하지 않는다.</p>
<h4>code 3-0</h4>
<pre><code class="hljs language-python">!pip install cloud-tpu-client==<span class="hljs-number">0.10</span> https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-<span class="hljs-number">1.9</span>-cp37-cp37m-linux_x86_64.whl
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">의존성 패키지 설치</h3>
<p>code 3-1을 실행해 TPU 이외의 의존성 있는 패키지를 설치한다.</p>
<h4>code 3-1</h4>
<pre><code class="hljs language-python">!pip install ratsnlp
</code></pre>
<pre><code>Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)
Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)
Requirement already satisfied: torch&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)
Requirement already satisfied: Korpora&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)
Requirement already satisfied: flask&gt;=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)
Requirement already satisfied: flask-cors&gt;=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)
Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)
Requirement already satisfied: flask-ngrok&gt;=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)
Requirement already satisfied: numpy&gt;=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (1.21.5)
Requirement already satisfied: fsspec[http]&gt;=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2022.2.0)
Requirement already satisfied: tqdm&gt;=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (4.62.3)
Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.3.0)
Requirement already satisfied: torchmetrics&gt;=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.7.2)
Requirement already satisfied: future&gt;=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (0.18.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (21.3)
Requirement already satisfied: tensorboard!=2.5.0,&gt;=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (2.8.0)
Requirement already satisfied: PyYAML&lt;=5.4.1,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-&gt;ratsnlp) (5.4.1)
Requirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.10.3)
Requirement already satisfied: huggingface-hub&gt;=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.4.0)
Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (0.0.47)
Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (4.11.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2019.12.20)
Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (3.6.0)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-&gt;ratsnlp) (2.23.0)
Requirement already satisfied: click&lt;8.0,&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (7.1.2)
Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.1.0)
Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (1.0.1)
Requirement already satisfied: Jinja2&lt;3.0,&gt;=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask&gt;=1.1.4-&gt;ratsnlp) (2.11.3)
Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors&gt;=3.0.10-&gt;ratsnlp) (1.15.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.8.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&gt;=0.0.12-&gt;transformers==4.10.0-&gt;ratsnlp) (3.10.0.2)
Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2&lt;3.0,&gt;=2.10.1-&gt;flask&gt;=1.1.4-&gt;ratsnlp) (2.0.1)
Requirement already satisfied: xlrd&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (2.0.1)
Requirement already satisfied: dataclasses&gt;=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora&gt;=0.2.0-&gt;ratsnlp) (0.6)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.0.7)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (2021.10.8)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.10.0-&gt;ratsnlp) (3.0.4)
Requirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.35.0)
Requirement already satisfied: protobuf&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.17.3)
Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (57.4.0)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.6)
Requirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.0.0)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.8.1)
Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.37.1)
Requirement already satisfied: grpcio&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.44.0)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.3.6)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.6.1)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.2.4)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.8)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.1)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.10.0-&gt;ratsnlp) (3.7.0)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.4.8)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard!=2.5.0,&gt;=2.2.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (3.2.0)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (6.0.2)
Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (0.13.0)
Requirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (2.0.12)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.2.0)
Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (21.4.0)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.3.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (1.7.2)
Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;fsspec[http]&gt;=2021.4.0-&gt;pytorch-lightning==1.3.4-&gt;ratsnlp) (4.0.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.10.0-&gt;ratsnlp) (1.1.0)
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">구글 드라이브와 연결</h3>
<p>코랩 노트북은 일정시간 사용하지 않으면 당시까지의 모든 결과물이 날아갈 수 있다. 모델 체크포인트 등을 저장해 주기 위해 자신의 구글 드라이브를 코랩 노트북과 연결한다.</p>
<h4>code 3-2</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> google.colab <span class="hljs-keyword">import</span> drive
drive.mount(<span class="hljs-string">&#x27;/gdrive&#x27;</span>, force_remount=<span class="hljs-literal">True</span>)
</code></pre>
<pre><code>Mounted at /gdrive
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">모델 환경 설정</h3>
<p>kcbert-base모델을 인공지능 기업 업스테이지가 공개한 KLUE-NLI데이터* 로 파인튜닝</p>
<blockquote>
<p>*<em>klue-benchmark.com/tasks/68/data/description</em></p>
</blockquote>
<h4>code 3-3</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> ratsnlp.nlpbook.classification <span class="hljs-keyword">import</span> ClassificationTrainArguments
args = ClassificationTrainArguments(
    pretrained_model_name=<span class="hljs-string">&quot;beomi/kcbert-base&quot;</span>,
    downstream_task_name=<span class="hljs-string">&quot;pair-classification&quot;</span>,
    downstream_corpus_name=<span class="hljs-string">&quot;klue-nli&quot;</span>,
    downstream_model_dir=<span class="hljs-string">&quot;/gdrive/My Drive/nlpbook/checkpoint-paircls&quot;</span>,
    batch_size=<span class="hljs-number">32</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-number">4</span>,
    learning_rate=<span class="hljs-number">5e-5</span>,
    max_seq_length=<span class="hljs-number">64</span>,
    epochs=<span class="hljs-number">5</span>,
    tpu_cores=<span class="hljs-number">0</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-number">8</span>,
    seed=<span class="hljs-number">7</span>,
)
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">랜덤 시드 고정</h3>
<p>랜덤 시드를 설정</p>
<p>code 3-4는 <code>args</code>에 지정된 시드로 고정하는 역할을 한다.</p>
<h4>code 3-4</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> ratsnlp <span class="hljs-keyword">import</span> nlpbook
nlpbook.set_seed(args)
</code></pre>
<pre><code>set seed: 7
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">로거 설정</h3>
<p>각종 로그를 출력하는 로거를 설정</p>
<h4>code 3-5</h4>
<pre><code class="hljs language-python">nlpbook.set_logger(args)
</code></pre>
<pre><code>INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name=&#x27;beomi/kcbert-base&#x27;, downstream_task_name=&#x27;pair-classification&#x27;, downstream_corpus_name=&#x27;klue-nli&#x27;, downstream_corpus_root_dir=&#x27;/content/Korpora&#x27;, downstream_model_dir=&#x27;/gdrive/My Drive/nlpbook/checkpoint-paircls&#x27;, max_seq_length=64, save_top_k=1, monitor=&#x27;min val_loss&#x27;, seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0)
</code></pre>
<h2 id="2" class="text-2xl font-bold mt-8 mb-4">2. 말뭉치 내려받기</h2>
<h3 id="" class="text-xl font-bold mt-6 mb-3">말뭉치 내려받기</h3>
<p>KLUE-NLI 데이터를 내려받는다. <code>corpus_name</code>에 해당하는 말뭉치(<code>klue_nli</code>)를 <code>downstream_corpus_root_dir</code>아래(<code>/root/Korpora</code>)에 저장해둔다.</p>
<h4>code 3-6</h4>
<pre><code class="hljs language-python">nlpbook.download_downstream_dataset(args)
</code></pre>
<pre><code>Downloading: 100%|██████████| 12.3M/12.3M [00:00&lt;00:00, 42.3MB/s]
Downloading: 100%|██████████| 1.47M/1.47M [00:00&lt;00:00, 35.6MB/s]
</code></pre>
<h2 id="3" class="text-2xl font-bold mt-8 mb-4">3. 토크나이저 준비하기</h2>
<h3 id="" class="text-xl font-bold mt-6 mb-3">토크나이저 준비</h3>
<p>code 3-7을 실행해 <code>pretrained_model_name</code>에 해당하는 모델(<strong>kcbert-base</strong>)이 사용하는 토크나이저를 선언한다.</p>
<h4>code 3-7</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=<span class="hljs-literal">False</span>,
)
</code></pre>
<pre><code>Downloading:   0%|          | 0.00/250k [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/49.0 [00:00&lt;?, ?B/s]



Downloading:   0%|          | 0.00/619 [00:00&lt;?, ?B/s]
</code></pre>
<h2 id="4" class="text-2xl font-bold mt-8 mb-4">4. 데이터 전처리하기</h2>
<h3 id="" class="text-xl font-bold mt-6 mb-3">학습 데이터셋 구축</h3>
<p>code 3-8을 수행하면 학습 데이터셋 을 만들 수 있다. <strong>KlueNLICorpus</strong> 클래스는 JSON 파일 형식의 KLUE-NLI 데이터를 문장(전제 + 가설)과 레이블(참, 거짓, 중립)로 읽어들인다. <code>KlueNLICorpus</code>는 <code>ClassificationDataset</code>이 요구하면 이 문장과 레이블을 <code>ClassificationDataset</code>에 제공한다.</p>
<h4>code 3-8</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> ratsnlp.nlpbook.paircls <span class="hljs-keyword">import</span> KlueNLICorpus
<span class="hljs-keyword">from</span> ratsnlp.nlpbook.classification <span class="hljs-keyword">import</span> ClassificationDataset
corpus = KlueNLICorpus()
train_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode=<span class="hljs-string">&quot;train&quot;</span>,
)
</code></pre>
<pre><code>INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli
INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json
INFO:ratsnlp:tokenize sentences, it could take a lot of time...
INFO:ratsnlp:tokenize sentences [took 15.747 s]
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: contradiction
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.
INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: contradiction
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.
INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:Saving features into cached file, it could take a lot of time...
INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.934 s]
</code></pre>
<h3 id="c" class="text-xl font-bold mt-6 mb-3">ClassificationDataset 클래스가 하는 역할</h3>
<p>이 클래스는 <strong>KlueNLICorpus</strong>와 code 3-7에서 선언해 둔 <strong>토크나이저</strong>를 품고 있다.</p>
<p><strong>ClassificationDataset</strong>은 제공받은 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습할 수 있는 형태(<strong>ClassificationFeature</strong>)로 가공한다.
다시 말해, 전제와 가설 2개 문장을 각각 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수로 바꿔주는 역할을 한다.</p>
<p>(<strong>entailment: 0, contradiction: 1, neutral: 2</strong>)</p>
<p>KlueNLICorpus와 classificationDataset의 역할과 자세한 구현 내용은 아래의 링크를 참고하자!
(현재는 교재링크를 올려두지만, 추후 본인의 깃허브에 구현 예정)</p>
<ul>
<li>ratsgo.github.io/nlpbook/docs/pair_cls/detail</li>
</ul>
<h3 id="" class="text-xl font-bold mt-6 mb-3">학습 데이터 로더 구축</h3>
<p>code 3-9를 실행하면 학습할 때 쓰이는 데이터 로더를 만들 수 있다. 학습용 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배크 크기(<em>code 3-3 에서 정의한</em> <code>args</code><em>의</em> <code>batch_size</code>)만큼의 인스턴스들을 비복원(<code>replacement=False</code>)랜덤 추출(<code>RandomSampler</code>)한 뒤 이를 배치 형태로 가공(<code>nlpbook.data_collator</code>)해 모델에 공급하는 역할을 수행한다.</p>
<h4>code 3-9</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, RandomSampler
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=RandomSampler(train_dataset, replacement=<span class="hljs-literal">False</span>),
    collate_fn=nlpbook.data_collator,
    drop_last=<span class="hljs-literal">False</span>,
    num_workers=args.cpu_workers,
)
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">평가용 데이터 로더 구축</h3>
<p>code 3-10을 실행하면 평가용 데이터 로더를 구축할 수 있다. 평가용 데이터 로더는 배치 크기(code 3-3에서 정의한 <code>args</code>의 <code>batch_size</code>)만큼의 인스턴스를 순서대로 추출(<strong>Sequential Sampler</strong>)한 후 이를 배치 형태로 가공(<code>nlpbook.data_collator</code>)해 모델에 공급한다.</p>
<h4>code 3-10</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> SequentialSampler
val_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode=<span class="hljs-string">&quot;test&quot;</span>,
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=SequentialSampler(val_dataset),
    collate_fn=nlpbook.data_collator,
    drop_last=<span class="hljs-literal">False</span>,
    num_workers=args.cpu_workers,
)
</code></pre>
<pre><code>INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.116 s]
</code></pre>
<h2 id="5" class="text-2xl font-bold mt-8 mb-4">5. 모델 불러오기</h2>
<h3 id="" class="text-xl font-bold mt-6 mb-3">모델 초기화</h3>
<p>code 3-11을 수행해 모델을 초기화 한다. 프리트레인을 마친 BERT로 <code>kcbert-base</code>를 사용한다. code 3-3에서 <code>pretrained_model_name</code>을 <code>beomi/kcber-base</code>로 지정했기 때문이다. 물론 허깅페이스 모델 허브에 등록된 모델이라면 다른 모델 역시 사용할 수 있다.</p>
<p><code>BertForSequenceClassification</code>은 프리트레인을 마친 BERT모델 위에 문서 분류용 태스크 모듈을 덧붙인 형태의 모델 클래스이다. 이 클래스는 <strong>문서 분류 모델</strong>에서 사용한 것과 동일하다.</p>
<h4>code 3-11</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, BertForSequenceClassification
pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=corpus.num_labels,
)
model = BertForSequenceClassification.from_pretrained(
    args.pretrained_model_name,
    config=pretrained_model_config,
)
</code></pre>
<pre><code>Downloading:   0%|          | 0.00/438M [00:00&lt;?, ?B/s]


Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: [&#x27;cls.predictions.decoder.bias&#x27;, &#x27;cls.predictions.transform.LayerNorm.bias&#x27;, &#x27;cls.predictions.decoder.weight&#x27;, &#x27;cls.seq_relationship.weight&#x27;, &#x27;cls.predictions.transform.dense.bias&#x27;, &#x27;cls.predictions.bias&#x27;, &#x27;cls.seq_relationship.bias&#x27;, &#x27;cls.predictions.transform.LayerNorm.weight&#x27;, &#x27;cls.predictions.transform.dense.weight&#x27;]
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: [&#x27;classifier.bias&#x27;, &#x27;classifier.weight&#x27;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
<h2 id="6" class="text-2xl font-bold mt-8 mb-4">6. 모델 학습시키기</h2>
<p>code 3-12를 실행하면 문장 쌍 분류용 태스크를 정의할 수 있다. 모델은 code 3-11에서 준비한 모델 클래스를 <code>ClassificationTask</code>에 포함한다. <code>ClassificationTask</code> 클래스에는 옵티마이저, 러닝 레이트 스케줄러가 정의 되 있는데, 옵티마이저로는 아담(<code>Adam</code>), 러닝 레이트 스케줄러로는 <code>ExponentialLR</code>을 사용한다.</p>
<h3 id="" class="text-xl font-bold mt-6 mb-3">태스크 정의</h3>
<h4>code 3-12</h4>
<pre><code class="hljs language-python"><span class="hljs-keyword">from</span> ratsnlp.nlpbook.classification <span class="hljs-keyword">import</span> ClassificationTask
task = ClassificationTask(model, args)
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">트레이너 정의</h3>
<p>code 3-13을 실행하면 트레이너를 정의할 수 있다. 이 트레이너는 <strong>파이토치 라이트닝 라이브러리</strong>의 도움을 받아 <strong>GPU/TPU 설정</strong>, <strong>로그 및 체크포인트</strong> 등 귀찮은 설정들을 알아서 해준다.</p>
<h4>code 3-13</h4>
<pre><code class="hljs language-python">trainer = nlpbook.get_trainer(args)
</code></pre>
<pre><code>GPU available: True, used: True
TPU available: False, using: 0 TPU cores
</code></pre>
<h3 id="" class="text-xl font-bold mt-6 mb-3">학습 개시</h3>
<p>code 3-14와 같이 트레이너의 <code>fit()</code>함수를 호출하면 학습을 시작한다.</p>
<h4>code 3-14</h4>
<pre><code class="hljs language-python">trainer.fit(
    task,
    train_dataloader=train_dataloader,
    val_dataloaders=val_dataloader,
)
</code></pre>
<pre><code>LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                          | Params
--------------------------------------------------------
0 | model | BertForSequenceClassification | 108 M 
--------------------------------------------------------
108 M     Trainable params
0         Non-trainable params
108 M     Total params
435.683   Total estimated model params size (MB)



Training: 0it [00:00, ?it/s]



Validating: 0it [00:00, ?it/s]



Validating: 0it [00:00, ?it/s]



Validating: 0it [00:00, ?it/s]



Validating: 0it [00:00, ?it/s]



Validating: 0it [00:00, ?it/s]
</code></pre>
<h1 id="object-object" class="text-3xl font-bold mt-8 mb-4"><strong>문장 쌍 분류</strong>는 문서 분류 과제와 태스크 모듈 구조 등에서 본질적으로 다르지 않다. 입력문서가 1개냐(문서분류), 2개냐(문장 쌍 분류)의 차이가 있을 뿐이다.</h1><div class="mt-10 border-t pt-10"></div></article></div><!--$--><!--/$--><script src="/_next/static/chunks/webpack-1c8036505c2e140d.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[9766,[],\"\"]\n3:I[8924,[],\"\"]\n5:I[4431,[],\"OutletBoundary\"]\n7:I[5278,[],\"AsyncMetadataOutlet\"]\n9:I[4431,[],\"ViewportBoundary\"]\nb:I[4431,[],\"MetadataBoundary\"]\nc:\"$Sreact.suspense\"\ne:I[7150,[],\"\"]\n:HL[\"/_next/static/css/51e5ba5c7de07f80.css\",\"style\"]\n:HL[\"/_next/static/css/5eacd01f773eed7f.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"xTAnN8PQ3b6-LdzNENhZ7\",\"p\":\"\",\"c\":[\"\",\"2022-03-22-pair_classification_train\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"2022-03-22-pair_classification_train\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/51e5ba5c7de07f80.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-screen flex flex-col font-sans\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"slug\",\"2022-03-22-pair_classification_train\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5eacd01f773eed7f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$L6\",[\"$\",\"$L7\",null,{\"promise\":\"$@8\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L9\",null,{\"children\":\"$La\"}],null],[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$c\",null,{\"fallback\":null,\"children\":\"$Ld\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:T2695,"])</script><script>self.__next_f.push([1,"Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)\nRequirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)\nRequirement already satisfied: torch\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)\nRequirement already satisfied: Korpora\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)\nRequirement already satisfied: flask\u003e=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)\nRequirement already satisfied: flask-cors\u003e=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)\nRequirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)\nRequirement already satisfied: flask-ngrok\u003e=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)\nRequirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (1.21.5)\nRequirement already satisfied: fsspec[http]\u003e=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (2022.2.0)\nRequirement already satisfied: tqdm\u003e=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (4.62.3)\nRequirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.3.0)\nRequirement already satisfied: torchmetrics\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.7.2)\nRequirement already satisfied: future\u003e=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.18.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (21.3)\nRequirement already satisfied: tensorboard!=2.5.0,\u003e=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (2.8.0)\nRequirement already satisfied: PyYAML\u003c=5.4.1,\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (5.4.1)\nRequirement already satisfied: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.10.3)\nRequirement already satisfied: huggingface-hub\u003e=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.4.0)\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.0.47)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (4.11.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (2019.12.20)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (3.6.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (2.23.0)\nRequirement already satisfied: click\u003c8.0,\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (7.1.2)\nRequirement already satisfied: itsdangerous\u003c2.0,\u003e=0.24 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (1.1.0)\nRequirement already satisfied: Werkzeug\u003c2.0,\u003e=0.15 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (1.0.1)\nRequirement already satisfied: Jinja2\u003c3.0,\u003e=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (2.11.3)\nRequirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors\u003e=3.0.10-\u003eratsnlp) (1.15.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.8.1)\nRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.12-\u003etransformers==4.10.0-\u003eratsnlp) (3.10.0.2)\nRequirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2\u003c3.0,\u003e=2.10.1-\u003eflask\u003e=1.1.4-\u003eratsnlp) (2.0.1)\nRequirement already satisfied: xlrd\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora\u003e=0.2.0-\u003eratsnlp) (2.0.1)\nRequirement already satisfied: dataclasses\u003e=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora\u003e=0.2.0-\u003eratsnlp) (0.6)\nRequirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.0.7)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (1.24.3)\nRequirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (2.10)\nRequirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (2021.10.8)\nRequirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (3.0.4)\nRequirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.35.0)\nRequirement already satisfied: protobuf\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.17.3)\nRequirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (57.4.0)\nRequirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.4.6)\nRequirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.0.0)\nRequirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.8.1)\nRequirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.37.1)\nRequirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.44.0)\nRequirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.3.6)\nRequirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.6.1)\nRequirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.2.8)\nRequirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.8)\nRequirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.3.1)\nRequirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.10.0-\u003eratsnlp) (3.7.0)\nRequirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.4.8)\nRequirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.2.0)\nRequirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (6.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.13.0)\nRequirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (2.0.12)\nRequirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.2.0)\nRequirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (21.4.0)\nRequirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.3.0)\nRequirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.7.2)\nRequirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.0.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.10.0-\u003eratsnlp) (1.1.0)\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"flex gap-10\",\"children\":[[\"$\",\"article\",null,{\"className\":\"flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8 not-prose border-b pb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold mb-4\",\"children\":\"[NLP] 문장 쌍 분류 모델 학습하기\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"$D2022-03-22T00:00:00.000Z\",\"children\":\"March 22, 2022\"}],\"$undefined\"]}]]}],[[\"$\",\"p\",\"p-0\",{\"children\":\"자연어처리의 예제를 학습하여 보자.\\n다음은 이전 글에서 설명하였던 문장 쌍 분류 모델을 구현한 것이다.\"}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":\"본 파일은 이기창님의 'Do it! 자연어 처리'에 기초하여 작성되었다. :)\"}],\"\\n\",[\"$\",\"h1\",\"h1-0\",{\"id\":\"\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"문장 쌍 분류 모델 학습하기\"}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":\"전제와 가설을 검증하는 자연어 추론 모델 만들기\"}],\"\\n\",[\"$\",\"h2\",\"h2-0\",{\"id\":\"1\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"1. 각종 설정하기\"}],\"\\n\",[\"$\",\"h3\",\"h3-0\",{\"id\":\"t\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"TPU 관련 패키지 설치\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":\"코랩 노트북 초기화 과정에서 하드웨어 가속기로 TPU를 선택했다면 다음 코드를 실행하고, GPU를 선택했다면 실행하지 않는다.\"}],\"\\n\",[\"$\",\"h4\",\"h4-0\",{\"children\":\"code 3-0\"}],\"\\n\",[\"$\",\"pre\",\"pre-0\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[\"!pip install cloud-tpu-client==\",[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-number\",\"children\":\"0.10\"}],\" https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-\",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-number\",\"children\":\"1.9\"}],\"-cp37-cp37m-linux_x86_64.whl\\n\"]}]}],\"\\n\",[\"$\",\"h3\",\"h3-1\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"의존성 패키지 설치\"}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":\"code 3-1을 실행해 TPU 이외의 의존성 있는 패키지를 설치한다.\"}],\"\\n\",[\"$\",\"h4\",\"h4-1\",{\"children\":\"code 3-1\"}],\"\\n\",[\"$\",\"pre\",\"pre-1\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":\"!pip install ratsnlp\\n\"}]}],\"\\n\",[\"$\",\"pre\",\"pre-2\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"$f\"}]}],\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\",\"\\n\",\"$L57\",\"\\n\",\"$L58\",\"\\n\",\"$L59\",\"\\n\",\"$L5a\",\"\\n\",\"$L5b\",\"\\n\",\"$L5c\"],\"$L5d\"]}],\"$L5e\"]}]\n"])</script><script>self.__next_f.push([1,"61:I[3089,[\"182\",\"static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js\"],\"default\"]\n62:I[4010,[\"182\",\"static/chunks/app/%5Bslug%5D/page-ad31c54747687caf.js\"],\"default\"]\n10:[\"$\",\"h3\",\"h3-2\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"구글 드라이브와 연결\"}]\n11:[\"$\",\"p\",\"p-5\",{\"children\":\"코랩 노트북은 일정시간 사용하지 않으면 당시까지의 모든 결과물이 날아갈 수 있다. 모델 체크포인트 등을 저장해 주기 위해 자신의 구글 드라이브를 코랩 노트북과 연결한다.\"}]\n12:[\"$\",\"h4\",\"h4-2\",{\"children\":\"code 3-2\"}]\n13:[\"$\",\"pre\",\"pre-3\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" google.colab \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" drive\\ndrive.mount(\",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-string\",\"children\":\"'/gdrive'\"}],\", force_remount=\",[\"$\",\"span\",\"span-3\",{\"className\":\"hljs-literal\",\"children\":\"True\"}],\")\\n\"]}]}]\n14:[\"$\",\"pre\",\"pre-4\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"Mounted at /gdrive\\n\"}]}]\n15:[\"$\",\"h3\",\"h3-3\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"모델 환경 설정\"}]\n16:[\"$\",\"p\",\"p-6\",{\"children\":\"kcbert-base모델을 인공지능 기업 업스테이지가 공개한 KLUE-NLI데이터* 로 파인튜닝\"}]\n17:[\"$\",\"blockquote\",\"blockquote-0\",{\"children\":[\"\\n\",[\"$\",\"p\",\"p-0\",{\"children\":[\"*\",[\"$\",\"em\",\"em-0\",{\"children\":\"klue-benchmark.com/tasks/68/data/description\"}]]}],\"\\n\"]}]\n18:[\"$\",\"h4\",\"h4-3\",{\"children\":\"code 3-3\"}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"pre\",\"pre-5\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" torch\\n\",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" ratsnlp.nlpbook.classification \",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" ClassificationTrainArguments\\nargs = ClassificationTrainArguments(\\n    pretrained_model_name=\",[\"$\",\"span\",\"span-3\",{\"className\":\"hljs-string\",\"children\":\"\\\"beomi/kcbert-base\\\"\"}],\",\\n    downstream_task_name=\",[\"$\",\"span\",\"span-4\",{\"className\":\"hljs-string\",\"children\":\"\\\"pair-classification\\\"\"}],\",\\n    downstream_corpus_name=\",[\"$\",\"span\",\"span-5\",{\"className\":\"hljs-string\",\"children\":\"\\\"klue-nli\\\"\"}],\",\\n    downstream_model_dir=\",[\"$\",\"span\",\"span-6\",{\"className\":\"hljs-string\",\"children\":\"\\\"/gdrive/My Drive/nlpbook/checkpoint-paircls\\\"\"}],\",\\n    batch_size=\",[\"$\",\"span\",\"span-7\",{\"className\":\"hljs-number\",\"children\":\"32\"}],\" \",[\"$\",\"span\",\"span-8\",{\"className\":\"hljs-keyword\",\"children\":\"if\"}],\" torch.cuda.is_available() \",[\"$\",\"span\",\"span-9\",{\"className\":\"hljs-keyword\",\"children\":\"else\"}],\" \",[\"$\",\"span\",\"span-10\",{\"className\":\"hljs-number\",\"children\":\"4\"}],\",\\n    learning_rate=\",[\"$\",\"span\",\"span-11\",{\"className\":\"hljs-number\",\"children\":\"5e-5\"}],\",\\n    max_seq_length=\",[\"$\",\"span\",\"span-12\",{\"className\":\"hljs-number\",\"children\":\"64\"}],\",\\n    epochs=\",[\"$\",\"span\",\"span-13\",{\"className\":\"hljs-number\",\"children\":\"5\"}],\",\\n    tpu_cores=\",[\"$\",\"span\",\"span-14\",{\"className\":\"hljs-number\",\"children\":\"0\"}],\" \",[\"$\",\"span\",\"span-15\",{\"className\":\"hljs-keyword\",\"children\":\"if\"}],\" torch.cuda.is_available() \",[\"$\",\"span\",\"span-16\",{\"className\":\"hljs-keyword\",\"children\":\"else\"}],\" \",[\"$\",\"span\",\"span-17\",{\"className\":\"hljs-number\",\"children\":\"8\"}],\",\\n    seed=\",[\"$\",\"span\",\"span-18\",{\"className\":\"hljs-number\",\"children\":\"7\"}],\",\\n)\\n\"]}]}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"h3\",\"h3-4\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"랜덤 시드 고정\"}]\n1b:[\"$\",\"p\",\"p-7\",{\"children\":\"랜덤 시드를 설정\"}]\n1c:[\"$\",\"p\",\"p-8\",{\"children\":[\"code 3-4는 \",[\"$\",\"code\",\"code-0\",{\"children\":\"args\"}],\"에 지정된 시드로 고정하는 역할을 한다.\"]}]\n1d:[\"$\",\"h4\",\"h4-4\",{\"children\":\"code 3-4\"}]\n1e:[\"$\",\"pre\",\"pre-6\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" ratsnlp \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" nlpbook\\nnlpbook.set_seed(args)\\n\"]}]}]\n1f:[\"$\",\"pre\",\"pre-7\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"set seed: 7\\n\"}]}]\n20:[\"$\",\"h3\",\"h3-5\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"로거 설정\"}]\n21:[\"$\",\"p\",\"p-9\",{\"children\":\"각종 로그를 출력하는 로거를 설정\"}]\n22:[\"$\",\"h4\",\"h4-5\",{\"children\":\"code 3-5\"}]\n23:[\"$\",\"pre\",\"pre-8\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":\"nlpbook.set_logger(args)\\n\"}]}]\n24:[\"$\",\"pre\",\"pre-9\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/My Drive/nlpbook/checkpoint-paircls', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0)\\n\"}]}]\n25:[\"$\",\"h2\",\"h2-1\",{\"id\":\"2\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"2. 말뭉치 내려받기\"}]\n26:[\"$\",\"h3\",\"h3-6\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"말뭉치 내려받기\"}]\n27:[\"$\",\"p\",\"p-10\",{\"children\":[\"KLUE-NLI 데이터를 내려받는다. \",[\"$\",\"code\",\"code-0\",{\"children\":\"corpus_name\"}],\""])</script><script>self.__next_f.push([1,"에 해당하는 말뭉치(\",[\"$\",\"code\",\"code-1\",{\"children\":\"klue_nli\"}],\")를 \",[\"$\",\"code\",\"code-2\",{\"children\":\"downstream_corpus_root_dir\"}],\"아래(\",[\"$\",\"code\",\"code-3\",{\"children\":\"/root/Korpora\"}],\")에 저장해둔다.\"]}]\n28:[\"$\",\"h4\",\"h4-6\",{\"children\":\"code 3-6\"}]\n29:[\"$\",\"pre\",\"pre-10\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":\"nlpbook.download_downstream_dataset(args)\\n\"}]}]\n2a:[\"$\",\"pre\",\"pre-11\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"Downloading: 100%|██████████| 12.3M/12.3M [00:00\u003c00:00, 42.3MB/s]\\nDownloading: 100%|██████████| 1.47M/1.47M [00:00\u003c00:00, 35.6MB/s]\\n\"}]}]\n2b:[\"$\",\"h2\",\"h2-2\",{\"id\":\"3\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"3. 토크나이저 준비하기\"}]\n2c:[\"$\",\"h3\",\"h3-7\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"토크나이저 준비\"}]\n2d:[\"$\",\"p\",\"p-11\",{\"children\":[\"code 3-7을 실행해 \",[\"$\",\"code\",\"code-0\",{\"children\":\"pretrained_model_name\"}],\"에 해당하는 모델(\",[\"$\",\"strong\",\"strong-0\",{\"children\":\"kcbert-base\"}],\")이 사용하는 토크나이저를 선언한다.\"]}]\n2e:[\"$\",\"h4\",\"h4-7\",{\"children\":\"code 3-7\"}]\n2f:[\"$\",\"pre\",\"pre-12\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" transformers \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" BertTokenizer\\ntokenizer = BertTokenizer.from_pretrained(\\n    args.pretrained_model_name,\\n    do_lower_case=\",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-literal\",\"children\":\"False\"}],\",\\n)\\n\"]}]}]\n30:[\"$\",\"pre\",\"pre-13\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"Downloading:   0%|          | 0.00/250k [00:00\u003c?, ?B/s]\\n\\n\\n\\nDownloading:   0%|          | 0.00/49.0 [00:00\u003c?, ?B/s]\\n\\n\\n\\nDownloading:   0%|          | 0.00/619 [00:00\u003c?, ?B/s]\\n\"}]}]\n31:[\"$\",\"h2\",\"h2-3\",{\"id\":\"4\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"4. 데이터 전처리하기\"}]\n32:[\"$\""])</script><script>self.__next_f.push([1,",\"h3\",\"h3-8\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"학습 데이터셋 구축\"}]\n33:[\"$\",\"p\",\"p-12\",{\"children\":[\"code 3-8을 수행하면 학습 데이터셋 을 만들 수 있다. \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"KlueNLICorpus\"}],\" 클래스는 JSON 파일 형식의 KLUE-NLI 데이터를 문장(전제 + 가설)과 레이블(참, 거짓, 중립)로 읽어들인다. \",[\"$\",\"code\",\"code-0\",{\"children\":\"KlueNLICorpus\"}],\"는 \",[\"$\",\"code\",\"code-1\",{\"children\":\"ClassificationDataset\"}],\"이 요구하면 이 문장과 레이블을 \",[\"$\",\"code\",\"code-2\",{\"children\":\"ClassificationDataset\"}],\"에 제공한다.\"]}]\n34:[\"$\",\"h4\",\"h4-8\",{\"children\":\"code 3-8\"}]\n"])</script><script>self.__next_f.push([1,"35:[\"$\",\"pre\",\"pre-14\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" ratsnlp.nlpbook.paircls \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" KlueNLICorpus\\n\",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" ratsnlp.nlpbook.classification \",[\"$\",\"span\",\"span-3\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" ClassificationDataset\\ncorpus = KlueNLICorpus()\\ntrain_dataset = ClassificationDataset(\\n    args=args,\\n    corpus=corpus,\\n    tokenizer=tokenizer,\\n    mode=\",[\"$\",\"span\",\"span-4\",{\"className\":\"hljs-string\",\"children\":\"\\\"train\\\"\"}],\",\\n)\\n\"]}]}]\n"])</script><script>self.__next_f.push([1,"5f:T1c34,"])</script><script>self.__next_f.push([1,"INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli\nINFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json\nINFO:ratsnlp:tokenize sentences, it could take a lot of time...\nINFO:ratsnlp:tokenize sentences [took 15.747 s]\nINFO:ratsnlp:*** Example ***\nINFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\nINFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nINFO:ratsnlp:label: contradiction\nINFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\nINFO:ratsnlp:*** Example ***\nINFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.\nINFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nINFO:ratsnlp:label: neutral\nINFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\nINFO:ratsnlp:*** Example ***\nINFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.\nINFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nINFO:ratsnlp:label: neutral\nINFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\nINFO:ratsnlp:*** Example ***\nINFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.\nINFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nINFO:ratsnlp:label: contradiction\nINFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\nINFO:ratsnlp:*** Example ***\nINFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.\nINFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nINFO:ratsnlp:label: neutral\nINFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\nINFO:ratsnlp:Saving features into cached file, it could take a lot of time...\nINFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.934 s]\n"])</script><script>self.__next_f.push([1,"36:[\"$\",\"pre\",\"pre-15\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"$5f\"}]}]\n37:[\"$\",\"h3\",\"h3-9\",{\"id\":\"c\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"ClassificationDataset 클래스가 하는 역할\"}]\n38:[\"$\",\"p\",\"p-13\",{\"children\":[\"이 클래스는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"KlueNLICorpus\"}],\"와 code 3-7에서 선언해 둔 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"토크나이저\"}],\"를 품고 있다.\"]}]\n39:[\"$\",\"p\",\"p-14\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"ClassificationDataset\"}],\"은 제공받은 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습할 수 있는 형태(\",[\"$\",\"strong\",\"strong-1\",{\"children\":\"ClassificationFeature\"}],\")로 가공한다.\\n다시 말해, 전제와 가설 2개 문장을 각각 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수로 바꿔주는 역할을 한다.\"]}]\n3a:[\"$\",\"p\",\"p-15\",{\"children\":[\"(\",[\"$\",\"strong\",\"strong-0\",{\"children\":\"entailment: 0, contradiction: 1, neutral: 2\"}],\")\"]}]\n3b:[\"$\",\"p\",\"p-16\",{\"children\":\"KlueNLICorpus와 classificationDataset의 역할과 자세한 구현 내용은 아래의 링크를 참고하자!\\n(현재는 교재링크를 올려두지만, 추후 본인의 깃허브에 구현 예정)\"}]\n3c:[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"ratsgo.github.io/nlpbook/docs/pair_cls/detail\"}],\"\\n\"]}]\n3d:[\"$\",\"h3\",\"h3-10\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"학습 데이터 로더 구축\"}]\n3e:[\"$\",\"p\",\"p-17\",{\"children\":[\"code 3-9를 실행하면 학습할 때 쓰이는 데이터 로더를 만들 수 있다. 학습용 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배크 크기(\",[\"$\",\"em\",\"em-0\",{\"children\":\"code 3-3 에서 정의한\"}],\" \",[\"$\",\"code\",\"code-0\",{\"children\":\"args\"}],[\"$\",\"em\",\"em-1\",{\"children\":\"의\"}],\" \",[\"$\",\"code\",\"code-1\",{\"children\":\"batch_size\"}],\")만큼의 인스턴스들을 비복원(\",[\"$\",\"code\",\"code-2\",{\"children\":\"replacement=False\"}],"])</script><script>self.__next_f.push([1,"\")랜덤 추출(\",[\"$\",\"code\",\"code-3\",{\"children\":\"RandomSampler\"}],\")한 뒤 이를 배치 형태로 가공(\",[\"$\",\"code\",\"code-4\",{\"children\":\"nlpbook.data_collator\"}],\")해 모델에 공급하는 역할을 수행한다.\"]}]\n3f:[\"$\",\"h4\",\"h4-9\",{\"children\":\"code 3-9\"}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"pre\",\"pre-16\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" torch.utils.data \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" DataLoader, RandomSampler\\ntrain_dataloader = DataLoader(\\n    train_dataset,\\n    batch_size=args.batch_size,\\n    sampler=RandomSampler(train_dataset, replacement=\",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-literal\",\"children\":\"False\"}],\"),\\n    collate_fn=nlpbook.data_collator,\\n    drop_last=\",[\"$\",\"span\",\"span-3\",{\"className\":\"hljs-literal\",\"children\":\"False\"}],\",\\n    num_workers=args.cpu_workers,\\n)\\n\"]}]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"h3\",\"h3-11\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"평가용 데이터 로더 구축\"}]\n42:[\"$\",\"p\",\"p-18\",{\"children\":[\"code 3-10을 실행하면 평가용 데이터 로더를 구축할 수 있다. 평가용 데이터 로더는 배치 크기(code 3-3에서 정의한 \",[\"$\",\"code\",\"code-0\",{\"children\":\"args\"}],\"의 \",[\"$\",\"code\",\"code-1\",{\"children\":\"batch_size\"}],\")만큼의 인스턴스를 순서대로 추출(\",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Sequential Sampler\"}],\")한 후 이를 배치 형태로 가공(\",[\"$\",\"code\",\"code-2\",{\"children\":\"nlpbook.data_collator\"}],\")해 모델에 공급한다.\"]}]\n43:[\"$\",\"h4\",\"h4-10\",{\"children\":\"code 3-10\"}]\n"])</script><script>self.__next_f.push([1,"44:[\"$\",\"pre\",\"pre-17\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" torch.utils.data \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" SequentialSampler\\nval_dataset = ClassificationDataset(\\n    args=args,\\n    corpus=corpus,\\n    tokenizer=tokenizer,\\n    mode=\",[\"$\",\"span\",\"span-2\",{\"className\":\"hljs-string\",\"children\":\"\\\"test\\\"\"}],\",\\n)\\nval_dataloader = DataLoader(\\n    val_dataset,\\n    batch_size=args.batch_size,\\n    sampler=SequentialSampler(val_dataset),\\n    collate_fn=nlpbook.data_collator,\\n    drop_last=\",[\"$\",\"span\",\"span-3\",{\"className\":\"hljs-literal\",\"children\":\"False\"}],\",\\n    num_workers=args.cpu_workers,\\n)\\n\"]}]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"pre\",\"pre-18\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.116 s]\\n\"}]}]\n46:[\"$\",\"h2\",\"h2-4\",{\"id\":\"5\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"5. 모델 불러오기\"}]\n47:[\"$\",\"h3\",\"h3-12\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"모델 초기화\"}]\n48:[\"$\",\"p\",\"p-19\",{\"children\":[\"code 3-11을 수행해 모델을 초기화 한다. 프리트레인을 마친 BERT로 \",[\"$\",\"code\",\"code-0\",{\"children\":\"kcbert-base\"}],\"를 사용한다. code 3-3에서 \",[\"$\",\"code\",\"code-1\",{\"children\":\"pretrained_model_name\"}],\"을 \",[\"$\",\"code\",\"code-2\",{\"children\":\"beomi/kcber-base\"}],\"로 지정했기 때문이다. 물론 허깅페이스 모델 허브에 등록된 모델이라면 다른 모델 역시 사용할 수 있다.\"]}]\n49:[\"$\",\"p\",\"p-20\",{\"children\":[[\"$\",\"code\",\"code-0\",{\"children\":\"BertForSequenceClassification\"}],\"은 프리트레인을 마친 BERT모델 위에 문서 분류용 태스크 모듈을 덧붙인 형태의 모델 클래스이다. 이 클래스는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"문서 분류 모델\"}],\"에서 사용한 것과 동일하다.\"]}]\n4a:[\"$\",\"h4\",\"h4-11\",{\"children\":\"code 3-11\"}]\n4b:[\"$\",\"pre\",\"pre-19\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" transformers \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" BertConfig, BertForSequenceClassification\\npretrained_model_config = BertConfig.from_pretrained(\\n    args.pretrained_model_name,\\n    num_labels=corpus.num_labels,\\n)\\nmodel = BertForSequenceClassification.from_pretrained(\\n    args.pretrained_model_name,\\n    config=pretrained_model_config,\\n)\\n\"]}]}]\n60:T508,"])</script><script>self.__next_f.push([1,"Downloading:   0%|          | 0.00/438M [00:00\u003c?, ?B/s]\n\n\nSome weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"])</script><script>self.__next_f.push([1,"4c:[\"$\",\"pre\",\"pre-20\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"$60\"}]}]\n4d:[\"$\",\"h2\",\"h2-5\",{\"id\":\"6\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"6. 모델 학습시키기\"}]\n4e:[\"$\",\"p\",\"p-21\",{\"children\":[\"code 3-12를 실행하면 문장 쌍 분류용 태스크를 정의할 수 있다. 모델은 code 3-11에서 준비한 모델 클래스를 \",[\"$\",\"code\",\"code-0\",{\"children\":\"ClassificationTask\"}],\"에 포함한다. \",[\"$\",\"code\",\"code-1\",{\"children\":\"ClassificationTask\"}],\" 클래스에는 옵티마이저, 러닝 레이트 스케줄러가 정의 되 있는데, 옵티마이저로는 아담(\",[\"$\",\"code\",\"code-2\",{\"children\":\"Adam\"}],\"), 러닝 레이트 스케줄러로는 \",[\"$\",\"code\",\"code-3\",{\"children\":\"ExponentialLR\"}],\"을 사용한다.\"]}]\n4f:[\"$\",\"h3\",\"h3-13\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"태스크 정의\"}]\n50:[\"$\",\"h4\",\"h4-12\",{\"children\":\"code 3-12\"}]\n51:[\"$\",\"pre\",\"pre-21\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":[[\"$\",\"span\",\"span-0\",{\"className\":\"hljs-keyword\",\"children\":\"from\"}],\" ratsnlp.nlpbook.classification \",[\"$\",\"span\",\"span-1\",{\"className\":\"hljs-keyword\",\"children\":\"import\"}],\" ClassificationTask\\ntask = ClassificationTask(model, args)\\n\"]}]}]\n52:[\"$\",\"h3\",\"h3-14\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"트레이너 정의\"}]\n53:[\"$\",\"p\",\"p-22\",{\"children\":[\"code 3-13을 실행하면 트레이너를 정의할 수 있다. 이 트레이너는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"파이토치 라이트닝 라이브러리\"}],\"의 도움을 받아 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"GPU/TPU 설정\"}],\", \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"로그 및 체크포인트\"}],\" 등 귀찮은 설정들을 알아서 해준다.\"]}]\n54:[\"$\",\"h4\",\"h4-13\",{\"children\":\"code 3-13\"}]\n55:[\"$\",\"pre\",\"pre-22\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":\"trainer = nlpbook.get_trainer(args)\\n\"}]}]\n56:[\"$\",\"pre\",\"pre-23\",{\"children\":[\"$\",\"code\",\"code-0\",{\"chil"])</script><script>self.__next_f.push([1,"dren\":\"GPU available: True, used: True\\nTPU available: False, using: 0 TPU cores\\n\"}]}]\n57:[\"$\",\"h3\",\"h3-15\",{\"id\":\"\",\"className\":\"text-xl font-bold mt-6 mb-3\",\"children\":\"학습 개시\"}]\n58:[\"$\",\"p\",\"p-23\",{\"children\":[\"code 3-14와 같이 트레이너의 \",[\"$\",\"code\",\"code-0\",{\"children\":\"fit()\"}],\"함수를 호출하면 학습을 시작한다.\"]}]\n59:[\"$\",\"h4\",\"h4-14\",{\"children\":\"code 3-14\"}]\n5a:[\"$\",\"pre\",\"pre-24\",{\"children\":[\"$\",\"code\",\"code-0\",{\"className\":\"hljs language-python\",\"children\":\"trainer.fit(\\n    task,\\n    train_dataloader=train_dataloader,\\n    val_dataloaders=val_dataloader,\\n)\\n\"}]}]\n"])</script><script>self.__next_f.push([1,"5b:[\"$\",\"pre\",\"pre-25\",{\"children\":[\"$\",\"code\",\"code-0\",{\"children\":\"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\\n\\n  | Name  | Type                          | Params\\n--------------------------------------------------------\\n0 | model | BertForSequenceClassification | 108 M \\n--------------------------------------------------------\\n108 M     Trainable params\\n0         Non-trainable params\\n108 M     Total params\\n435.683   Total estimated model params size (MB)\\n\\n\\n\\nTraining: 0it [00:00, ?it/s]\\n\\n\\n\\nValidating: 0it [00:00, ?it/s]\\n\\n\\n\\nValidating: 0it [00:00, ?it/s]\\n\\n\\n\\nValidating: 0it [00:00, ?it/s]\\n\\n\\n\\nValidating: 0it [00:00, ?it/s]\\n\\n\\n\\nValidating: 0it [00:00, ?it/s]\\n\"}]}]\n"])</script><script>self.__next_f.push([1,"5c:[\"$\",\"h1\",\"h1-1\",{\"id\":\"object-object\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"문장 쌍 분류\"}],\"는 문서 분류 과제와 태스크 모듈 구조 등에서 본질적으로 다르지 않다. 입력문서가 1개냐(문서분류), 2개냐(문장 쌍 분류)의 차이가 있을 뿐이다.\"]}]\n5d:[\"$\",\"$L61\",null,{}]\n63:T71d2,"])</script><script>self.__next_f.push([1,"\n자연어처리의 예제를 학습하여 보자.\n다음은 이전 글에서 설명하였던 문장 쌍 분류 모델을 구현한 것이다.\n\n본 파일은 이기창님의 'Do it! 자연어 처리'에 기초하여 작성되었다. :)\n\n# 문장 쌍 분류 모델 학습하기\n\n전제와 가설을 검증하는 자연어 추론 모델 만들기\n\n## 1. 각종 설정하기\n\n### TPU 관련 패키지 설치\n\n코랩 노트북 초기화 과정에서 하드웨어 가속기로 TPU를 선택했다면 다음 코드를 실행하고, GPU를 선택했다면 실행하지 않는다.\n\n#### code 3-0\n\n\n```python\n!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n```\n\n### 의존성 패키지 설치\n\ncode 3-1을 실행해 TPU 이외의 의존성 있는 패키지를 설치한다.\n\n#### code 3-1\n\n\n```python\n!pip install ratsnlp\n```\n\n    Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)\n    Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)\n    Requirement already satisfied: torch\u003e=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)\n    Requirement already satisfied: Korpora\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)\n    Requirement already satisfied: flask\u003e=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)\n    Requirement already satisfied: flask-cors\u003e=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)\n    Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)\n    Requirement already satisfied: flask-ngrok\u003e=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)\n    Requirement already satisfied: numpy\u003e=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (1.21.5)\n    Requirement already satisfied: fsspec[http]\u003e=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (2022.2.0)\n    Requirement already satisfied: tqdm\u003e=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (4.62.3)\n    Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.3.0)\n    Requirement already satisfied: torchmetrics\u003e=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.7.2)\n    Requirement already satisfied: future\u003e=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (0.18.2)\n    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (21.3)\n    Requirement already satisfied: tensorboard!=2.5.0,\u003e=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (2.8.0)\n    Requirement already satisfied: PyYAML\u003c=5.4.1,\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4-\u003eratsnlp) (5.4.1)\n    Requirement already satisfied: tokenizers\u003c0.11,\u003e=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.10.3)\n    Requirement already satisfied: huggingface-hub\u003e=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.4.0)\n    Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (0.0.47)\n    Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (4.11.1)\n    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (2019.12.20)\n    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (3.6.0)\n    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0-\u003eratsnlp) (2.23.0)\n    Requirement already satisfied: click\u003c8.0,\u003e=5.1 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (7.1.2)\n    Requirement already satisfied: itsdangerous\u003c2.0,\u003e=0.24 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (1.1.0)\n    Requirement already satisfied: Werkzeug\u003c2.0,\u003e=0.15 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (1.0.1)\n    Requirement already satisfied: Jinja2\u003c3.0,\u003e=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask\u003e=1.1.4-\u003eratsnlp) (2.11.3)\n    Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors\u003e=3.0.10-\u003eratsnlp) (1.15.0)\n    Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.8.1)\n    Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003e=0.0.12-\u003etransformers==4.10.0-\u003eratsnlp) (3.10.0.2)\n    Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2\u003c3.0,\u003e=2.10.1-\u003eflask\u003e=1.1.4-\u003eratsnlp) (2.0.1)\n    Requirement already satisfied: xlrd\u003e=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora\u003e=0.2.0-\u003eratsnlp) (2.0.1)\n    Requirement already satisfied: dataclasses\u003e=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora\u003e=0.2.0-\u003eratsnlp) (0.6)\n    Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.0.7)\n    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (1.24.3)\n    Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (2.10)\n    Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (2021.10.8)\n    Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers==4.10.0-\u003eratsnlp) (3.0.4)\n    Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.35.0)\n    Requirement already satisfied: protobuf\u003e=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.17.3)\n    Requirement already satisfied: setuptools\u003e=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (57.4.0)\n    Requirement already satisfied: google-auth-oauthlib\u003c0.5,\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.4.6)\n    Requirement already satisfied: absl-py\u003e=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.0.0)\n    Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.8.1)\n    Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.37.1)\n    Requirement already satisfied: grpcio\u003e=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.44.0)\n    Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.3.6)\n    Requirement already satisfied: tensorboard-data-server\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.6.1)\n    Requirement already satisfied: cachetools\u003c5.0,\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.2.4)\n    Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.2.8)\n    Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.8)\n    Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.3.1)\n    Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers==4.10.0-\u003eratsnlp) (3.7.0)\n    Requirement already satisfied: pyasn1\u003c0.5.0,\u003e=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.4.8)\n    Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c0.5,\u003e=0.4.1-\u003etensorboard!=2.5.0,\u003e=2.2.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (3.2.0)\n    Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (6.0.2)\n    Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (0.13.0)\n    Requirement already satisfied: charset-normalizer\u003c3.0,\u003e=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (2.0.12)\n    Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.2.0)\n    Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (21.4.0)\n    Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.3.0)\n    Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (1.7.2)\n    Requirement already satisfied: async-timeout\u003c5.0,\u003e=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-\u003efsspec[http]\u003e=2021.4.0-\u003epytorch-lightning==1.3.4-\u003eratsnlp) (4.0.2)\n    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003etransformers==4.10.0-\u003eratsnlp) (1.1.0)\n    \n\n### 구글 드라이브와 연결\n\n코랩 노트북은 일정시간 사용하지 않으면 당시까지의 모든 결과물이 날아갈 수 있다. 모델 체크포인트 등을 저장해 주기 위해 자신의 구글 드라이브를 코랩 노트북과 연결한다.\n\n#### code 3-2\n\n\n```python\nfrom google.colab import drive\ndrive.mount('/gdrive', force_remount=True)\n```\n\n    Mounted at /gdrive\n    \n\n### 모델 환경 설정\n\nkcbert-base모델을 인공지능 기업 업스테이지가 공개한 KLUE-NLI데이터* 로 파인튜닝\n\n\n\u003e **klue-benchmark.com/tasks/68/data/description*\n\n\n\n#### code 3-3\n\n\n```python\nimport torch\nfrom ratsnlp.nlpbook.classification import ClassificationTrainArguments\nargs = ClassificationTrainArguments(\n    pretrained_model_name=\"beomi/kcbert-base\",\n    downstream_task_name=\"pair-classification\",\n    downstream_corpus_name=\"klue-nli\",\n    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-paircls\",\n    batch_size=32 if torch.cuda.is_available() else 4,\n    learning_rate=5e-5,\n    max_seq_length=64,\n    epochs=5,\n    tpu_cores=0 if torch.cuda.is_available() else 8,\n    seed=7,\n)\n```\n\n### 랜덤 시드 고정\n\n랜덤 시드를 설정\n\ncode 3-4는 `args`에 지정된 시드로 고정하는 역할을 한다.\n\n#### code 3-4\n\n\n```python\nfrom ratsnlp import nlpbook\nnlpbook.set_seed(args)\n```\n\n    set seed: 7\n    \n\n### 로거 설정\n\n각종 로그를 출력하는 로거를 설정\n\n#### code 3-5\n\n\n```python\nnlpbook.set_logger(args)\n```\n\n    INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/My Drive/nlpbook/checkpoint-paircls', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0)\n    \n\n## 2. 말뭉치 내려받기\n\n### 말뭉치 내려받기\n\nKLUE-NLI 데이터를 내려받는다. `corpus_name`에 해당하는 말뭉치(`klue_nli`)를 `downstream_corpus_root_dir`아래(`/root/Korpora`)에 저장해둔다.\n\n#### code 3-6\n\n\n```python\nnlpbook.download_downstream_dataset(args)\n```\n\n    Downloading: 100%|██████████| 12.3M/12.3M [00:00\u003c00:00, 42.3MB/s]\n    Downloading: 100%|██████████| 1.47M/1.47M [00:00\u003c00:00, 35.6MB/s]\n    \n\n## 3. 토크나이저 준비하기\n\n### 토크나이저 준비\n\ncode 3-7을 실행해 `pretrained_model_name`에 해당하는 모델(**kcbert-base**)이 사용하는 토크나이저를 선언한다.\n\n#### code 3-7\n\n\n```python\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\n    args.pretrained_model_name,\n    do_lower_case=False,\n)\n```\n\n\n    Downloading:   0%|          | 0.00/250k [00:00\u003c?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/49.0 [00:00\u003c?, ?B/s]\n\n\n\n    Downloading:   0%|          | 0.00/619 [00:00\u003c?, ?B/s]\n\n\n## 4. 데이터 전처리하기\n\n### 학습 데이터셋 구축\n\ncode 3-8을 수행하면 학습 데이터셋 을 만들 수 있다. **KlueNLICorpus** 클래스는 JSON 파일 형식의 KLUE-NLI 데이터를 문장(전제 + 가설)과 레이블(참, 거짓, 중립)로 읽어들인다. `KlueNLICorpus`는 `ClassificationDataset`이 요구하면 이 문장과 레이블을 `ClassificationDataset`에 제공한다.\n\n\n\n#### code 3-8\n\n\n```python\nfrom ratsnlp.nlpbook.paircls import KlueNLICorpus\nfrom ratsnlp.nlpbook.classification import ClassificationDataset\ncorpus = KlueNLICorpus()\ntrain_dataset = ClassificationDataset(\n    args=args,\n    corpus=corpus,\n    tokenizer=tokenizer,\n    mode=\"train\",\n)\n```\n\n    INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli\n    INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json\n    INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n    INFO:ratsnlp:tokenize sentences [took 15.747 s]\n    INFO:ratsnlp:*** Example ***\n    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n    INFO:ratsnlp:label: contradiction\n    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n    INFO:ratsnlp:*** Example ***\n    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.\n    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n    INFO:ratsnlp:label: neutral\n    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n    INFO:ratsnlp:*** Example ***\n    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.\n    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n    INFO:ratsnlp:label: neutral\n    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n    INFO:ratsnlp:*** Example ***\n    INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.\n    INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n    INFO:ratsnlp:label: contradiction\n    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n    INFO:ratsnlp:*** Example ***\n    INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.\n    INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n    INFO:ratsnlp:label: neutral\n    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n    INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n    INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.934 s]\n    \n\n### ClassificationDataset 클래스가 하는 역할\n\n이 클래스는 **KlueNLICorpus**와 code 3-7에서 선언해 둔 **토크나이저**를 품고 있다.  \n\n**ClassificationDataset**은 제공받은 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습할 수 있는 형태(**ClassificationFeature**)로 가공한다.\n다시 말해, 전제와 가설 2개 문장을 각각 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수로 바꿔주는 역할을 한다.\n\n(**entailment: 0, contradiction: 1, neutral: 2**)\n\n\nKlueNLICorpus와 classificationDataset의 역할과 자세한 구현 내용은 아래의 링크를 참고하자!\n(현재는 교재링크를 올려두지만, 추후 본인의 깃허브에 구현 예정)\n\n- ratsgo.github.io/nlpbook/docs/pair_cls/detail \n\n### 학습 데이터 로더 구축\n\ncode 3-9를 실행하면 학습할 때 쓰이는 데이터 로더를 만들 수 있다. 학습용 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배크 크기(*code 3-3 에서 정의한* `args`*의* `batch_size`)만큼의 인스턴스들을 비복원(`replacement=False`)랜덤 추출(`RandomSampler`)한 뒤 이를 배치 형태로 가공(`nlpbook.data_collator`)해 모델에 공급하는 역할을 수행한다. \n\n#### code 3-9\n\n\n```python\nfrom torch.utils.data import DataLoader, RandomSampler\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=args.batch_size,\n    sampler=RandomSampler(train_dataset, replacement=False),\n    collate_fn=nlpbook.data_collator,\n    drop_last=False,\n    num_workers=args.cpu_workers,\n)\n```\n\n### 평가용 데이터 로더 구축\n\ncode 3-10을 실행하면 평가용 데이터 로더를 구축할 수 있다. 평가용 데이터 로더는 배치 크기(code 3-3에서 정의한 `args`의 `batch_size`)만큼의 인스턴스를 순서대로 추출(**Sequential Sampler**)한 후 이를 배치 형태로 가공(`nlpbook.data_collator`)해 모델에 공급한다.\n\n#### code 3-10\n\n\n```python\nfrom torch.utils.data import SequentialSampler\nval_dataset = ClassificationDataset(\n    args=args,\n    corpus=corpus,\n    tokenizer=tokenizer,\n    mode=\"test\",\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=args.batch_size,\n    sampler=SequentialSampler(val_dataset),\n    collate_fn=nlpbook.data_collator,\n    drop_last=False,\n    num_workers=args.cpu_workers,\n)\n```\n\n    INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.116 s]\n    \n\n## 5. 모델 불러오기\n\n### 모델 초기화\n\ncode 3-11을 수행해 모델을 초기화 한다. 프리트레인을 마친 BERT로 `kcbert-base`를 사용한다. code 3-3에서 `pretrained_model_name`을 `beomi/kcber-base`로 지정했기 때문이다. 물론 허깅페이스 모델 허브에 등록된 모델이라면 다른 모델 역시 사용할 수 있다.\n\n`BertForSequenceClassification`은 프리트레인을 마친 BERT모델 위에 문서 분류용 태스크 모듈을 덧붙인 형태의 모델 클래스이다. 이 클래스는 **문서 분류 모델**에서 사용한 것과 동일하다.\n\n#### code 3-11\n\n\n```python\nfrom transformers import BertConfig, BertForSequenceClassification\npretrained_model_config = BertConfig.from_pretrained(\n    args.pretrained_model_name,\n    num_labels=corpus.num_labels,\n)\nmodel = BertForSequenceClassification.from_pretrained(\n    args.pretrained_model_name,\n    config=pretrained_model_config,\n)\n```\n\n\n    Downloading:   0%|          | 0.00/438M [00:00\u003c?, ?B/s]\n\n\n    Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n    \n\n## 6. 모델 학습시키기\n\ncode 3-12를 실행하면 문장 쌍 분류용 태스크를 정의할 수 있다. 모델은 code 3-11에서 준비한 모델 클래스를 `ClassificationTask`에 포함한다. `ClassificationTask` 클래스에는 옵티마이저, 러닝 레이트 스케줄러가 정의 되 있는데, 옵티마이저로는 아담(`Adam`), 러닝 레이트 스케줄러로는 `ExponentialLR`을 사용한다.\n\n### 태스크 정의\n\n#### code 3-12\n\n\n```python\nfrom ratsnlp.nlpbook.classification import ClassificationTask\ntask = ClassificationTask(model, args)\n```\n\n### 트레이너 정의\n\ncode 3-13을 실행하면 트레이너를 정의할 수 있다. 이 트레이너는 **파이토치 라이트닝 라이브러리**의 도움을 받아 **GPU/TPU 설정**, **로그 및 체크포인트** 등 귀찮은 설정들을 알아서 해준다.\n\n#### code 3-13\n\n\n```python\ntrainer = nlpbook.get_trainer(args)\n```\n\n    GPU available: True, used: True\n    TPU available: False, using: 0 TPU cores\n    \n\n### 학습 개시\n\ncode 3-14와 같이 트레이너의 `fit()`함수를 호출하면 학습을 시작한다.\n\n#### code 3-14\n\n\n```python\ntrainer.fit(\n    task,\n    train_dataloader=train_dataloader,\n    val_dataloaders=val_dataloader,\n)\n```\n\n    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n    \n      | Name  | Type                          | Params\n    --------------------------------------------------------\n    0 | model | BertForSequenceClassification | 108 M \n    --------------------------------------------------------\n    108 M     Trainable params\n    0         Non-trainable params\n    108 M     Total params\n    435.683   Total estimated model params size (MB)\n    \n\n\n    Training: 0it [00:00, ?it/s]\n\n\n\n    Validating: 0it [00:00, ?it/s]\n\n\n\n    Validating: 0it [00:00, ?it/s]\n\n\n\n    Validating: 0it [00:00, ?it/s]\n\n\n\n    Validating: 0it [00:00, ?it/s]\n\n\n\n    Validating: 0it [00:00, ?it/s]\n\n\n# **문장 쌍 분류**는 문서 분류 과제와 태스크 모듈 구조 등에서 본질적으로 다르지 않다. 입력문서가 1개냐(문서분류), 2개냐(문장 쌍 분류)의 차이가 있을 뿐이다.\n\n"])</script><script>self.__next_f.push([1,"5e:[\"$\",\"$L62\",null,{\"content\":\"$63\"}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,"8:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Sehoon's Workspace\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Welcome to my page!\"}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"d:\"$8:metadata\"\n"])</script></body></html>