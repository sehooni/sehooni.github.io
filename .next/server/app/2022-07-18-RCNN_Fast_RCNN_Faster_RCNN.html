<!DOCTYPE html><!--8lJiHtAmlyU3nNFMbG8_k--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG" as="image"/><link rel="stylesheet" href="/_next/static/chunks/2f40a2027cd59172.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/chunks/b9ef641e76e3a351.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/475fa0f5a019bf38.js"/><script src="/_next/static/chunks/aa5e9022907a8769.js" async=""></script><script src="/_next/static/chunks/42fbd80a90fec4a2.js" async=""></script><script src="/_next/static/chunks/5cdb1f5564fc8217.js" async=""></script><script src="/_next/static/chunks/turbopack-f5bb12e1c2d48879.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/865c404e1d9a0c65.js" async=""></script><script src="/_next/static/chunks/6b8d09032578b975.js" async=""></script><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG" as="image"/><link rel="preload" href="https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG" as="image"/><title>Sehoon&#x27;s Workspace</title><meta name="description" content="Welcome to my page!"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="min-h-screen flex flex-col font-sans"><div hidden=""><!--$--><!--/$--></div><div class="flex gap-10"><article class="flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none"><header class="mb-8 not-prose border-b pb-8"><h1 class="text-4xl font-bold mb-4">[PaperReview] RCNN, Fast R-CNN, Faster R-CNN</h1><div class="flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400"><time dateTime="2022-07-18">July 18, 2022</time></div></header><p>2021년 머신러닝 스터디에서 진행하였던 <a href="https://docs.google.com/presentation/d/1VOyGj7IH4VlivV45MAWQFb-j1qn9RHmu/edit?usp=sharing&amp;ouid=103416735755875236001&amp;rtpof=true&amp;sd=true">&#x27;R-CNN, Fast R-CNN, Faster R-CNN&#x27; 논문 리뷰</a>를 재구성한 포스팅임을 미리 알려드립니다.</p>
<h1 id="i" class="text-3xl font-bold mt-8 mb-4">introduction</h1>
<p>위 논문은 CV(Computer Vision), 즉 이미치 처리에 있어 기반이 되는 논문입니다.</p>
<p>오늘은 R-CNN, Fast R-CNN, Faster R-CNN에 대해 이야기해보고자 합니다.
우선 이 기법들이 무엇인가에 대해 알아볼 필요가 있습니다. 위 3가지 기술들은 Object Detection, 즉 사물을 인식하는 방법에 들어가는 하나의 기법입니다.
그렇다면 <strong>Object Detection</strong>은 무엇일까요?</p>
<h1 id="o" class="text-3xl font-bold mt-8 mb-4">Object Detection (사물을 인식하는 방법)</h1>
<p>이미지 내에서 사물을 인식하는 방법에는 다양한 유형이 존재합니다. 여러 물체에 대해 어떤 물체인지 분류하는 것을 <strong>Classification</strong>이라 합니다.
또한 그 물체가 어디 있는지 박스를 통해 위치 정보를 나타내는 것을 <strong>Localization</strong>이라 합니다.</p>
<p><strong>Object Detection</strong>이란, 다수의 사물이 존재하는 상황에서 각 사물의 위치와 클래스를 찾는 작업을 말합니다.
<img src="https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG" alt="2"/></p>
<h1 id="1" class="text-3xl font-bold mt-8 mb-4">1-Stage Detector vs 2-Stage Detector</h1>
<p>Deep Learning을 이용한 object detection은 크게 1-stage detector와 2-stage detector로 나눌 수 있습니다. 아래 그림을 통해 설명을 이어 진행하겠습니다.</p>
<p>가운데 수평 화살표를 기준으로 위 쪽에 위치한 논문들이 2-stage detector 논문들이고, 아래 쪽에 위치한 논문들이 1-stage detector 논문들 입니다. 이번 시간에는 위 쪽에 위치한 2-stage detector 논문들에 대해 공부해볼 예정입니다.
그렇다면 여기서 계속 나오는 말, stage detector란 무엇이고, 1, 2 stage detector의 차이는 무엇일까요??
<img src="https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG" alt="3"/></p>
<p>Object Detection 문제는 앞에서 이야기하였듯이 물체의 위치를 찾는 localization 문제와, 물체를 식별하는 classification 문제를 합한 것입니다.
1-stage detector는 이 두 문제를 동시에 행하는 방법이고, 2-stage detector는 이 두 문제를 순차적으로 행하는 방법입니다.
따라서 1-stage detector가 비교적 빠르지만 정확도가 낮고, 반대로 2-stage detector가 비교적 느리지만 정확도가 높습니다.</p>
<p>오늘 우리가 중점적으로 다룰 R-CNN, Fast R-CNN, Faster R-CNN은 2-stage detector의 대표적인 기법입니다. (참고로 1-stage에는 YOLO 계열과 SSD 계열이 포함됩니다.)
<img src="https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG" alt="4"/></p>
<h1 id="r" class="text-3xl font-bold mt-8 mb-4">R-CNN: Regions with CNN features</h1>
<p>Object Detection 분야에서 최초로 딥러닝(CNN)을 적용시킨 것이 R-CNN입니다. 논문에서 소개하는 R-CNN의 구조는 다음과 같습니다.</p>
<ol>
<li>Selective search를 이용해 2,000개의 <strong>ROI(Region of Interest)를 추출</strong>.</li>
<li>각 ROI에 대하여 wraping을 수행하여 동일한 크기의 입력 이미지로 변경.</li>
<li>Warped image를 CNN에 넣어서(forward) 이미지 <u>feature를 추출</u>.</li>
<li>해당 <strong>feature</strong>를 SVM에 넣어 클래스(class) 분류 결과를 얻음.
<ul>
<li>이때 각 클래스에 독립적으로 훈련된 이진(binary) SVM을 사용.</li>
</ul>
</li>
<li>해당 <strong>feature</strong>를 regressor에 넣어 위치(bounding box)를 예측.</li>
</ol>
<p>R-CNN은 2-stage detector로서 전체 task를 두 가지 단계로 나누어 진행합니다.
첫번째 단계는 <strong>Region Proposal</strong>로 <u>물체의 위치를 찾는 일</u>이고, 두번째 단계는 <strong>Region Classification</strong>으로 <u>물체를 분류하는 일</u>입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG" alt="5"/>
<img src="https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG" alt="6"/></p>
<p>이 논문에서는 위 두 task를 행하기 위해 구조를 아래의 3가지 모듈로 나누어 놓았습니다.</p>
<ul>
<li>카테고리와 무관하게 물체의 영역을 찾는 모듈인 <strong>Region Proposal</strong></li>
<li>각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 <strong>Large Convolutional Neural Network인 CNN</strong></li>
<li>Classification을 위한 선형 지도학습 모델인 <strong>Support Vector Machine</strong>인 <strong>SVM</strong></li>
</ul>
<p>CNN의 경우, 이전 논문인 <a href="https://sehooni.github.io/paperreview/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks/">ImageNet Classification with Deep Convolutional Neural Networks</a>에서 이야기 했기에 생략하고,
Region proposal과 SVM에 대해 알아보겠습니다.</p>
<h2 id="r" class="text-2xl font-bold mt-8 mb-4">Region Proposal (영역 찾기)</h2>
<p>R-CNN의 구조를 조금 더 자세히 살펴보면 다음과 같습니다. R-CNN은 <strong>Region Proposal 단계</strong>에서 <strong>Selective Search</strong>라는 알고리즘을 이용하였습니다.
Selective Search 알고리즘은 Segmentation 분야에 많이 쓰이는 알고리즘이며, 객체오 주변 간의 색감(Color), 질감(Texture) 차이, 다른 물체에 둘러 싸여 있는지(Enclosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘 입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG" alt="8"/>
오른쪽 그림과 같이 <strong>Bounding box</strong>들을 Random하게 많이 생성을 하고, 이들을 조금씩 Merge해 나가면서 물체를 인식해 나가는 방식으로 되어있습니다.
이 알고리즘에 대해서는 <strong>&quot;물체의 위치를 파악하기 위한 알고리즘이구나&quot;</strong> 정도로 이해하면 될 것 같습니다. 이와 관련한 논문도 <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">Selective Search</a>논문이 있습니다.</p>
<p>R-CNN에서는 <strong>Selective Search</strong> 알고리즘을 통해 한 이미지에서 2,000개의 Region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈(224*224)로 압축하여 통일시키는 작업(Wraping)을 거칩니다.</p>
<h2 id="s" class="text-2xl font-bold mt-8 mb-4">SVM (Support Vector Machine)</h2>
<p>CNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, <strong>Linear SVM</strong>을 이용하여 classification을 진행합니다.(Category-Specific Linear SVMs)
여기서 의문점이 드는 부분들이 있습니다. 분명 CNN에서 Classifier로 softmax를 사용한 것 같은데 R-CNN에서는 왜 SVM을 사용하였을까? 라는 의문점 말입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG" alt="9"/></p>
<p>이 논문에서는 CNN을 fine-tunning할 때, 이미지의 positive/negative examples와 SVM을 학습할 때의 이미지의 positive/negative examples를 따로 정의했습니다.
CNN fine-tunning에서는 IoU가 0.5가 넘으면 positive로 두었고, 이외에는 &quot;background&quot;로 labeled해 두었습니다.
<img src="https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG" alt="10"/></p>
<p>여기서 IoU란, Intersection ove Union의 줄임말로, 두 바운딩 박스가 겹치는 비율을 의미합니다.
성능 평가를 예시로 들자면, mAP@0.5는 정답과 예측의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미를 말하며, NMS 계산을 예시로 들자면, 같은 클래스(class)끼리 IoU가 50% 이상일 때 낮은 confidence의 box를 제거한다는 의미입니다.
이때, mAP는 mean Average Precision을 의미합니다. 이러한 정보는 Computer Vision 분야에서 성능평가 지표로 사용됩니다.
<img src="https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG" alt="11"/></p>
<p>반면에 SVM을 학습할 때의 ground-truth boxes만 positive example로 두고, IoU가 0.3 미만은 모두 negative, 나머지는 전부 무시하게 됩니다.
SVM을 CNN fine-tunning과 같은 값을 두고 학습했을 때 훨씬 성능이 안 좋게 나왔다고 합니다.
시기상으로 fine-tunning 학습 데이터가 많지 않았기 때문에 IoU가 0.5~1 사이의 positive 영역들이 다소 정확하지 않았고, 때문에 바로 softmax classifier를 적용시켰을 때 성능이 좋지 않아서 위와 같이 SVM을 학습하는 과정이 필요했다고 보여집니다.</p>
<p>어찌됐는 SVM은 CNN으로부터 추출된 각각의 feature vector들의 점수를 class 별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 classifier입니다.
R-CNN 논문에서도 이에 대한 설명을 기재해 놓았습니다.
VOC2007 데이터셋 기준으로 Softmax를 사용하였을 때 mAP값이 54.2%에서 50.9%로 떨어졌다고 합니다.</p>
<h2 id="r" class="text-2xl font-bold mt-8 mb-4">R-CNN: Bounding Box Regression</h2>
<p>Selective Search로 만들어낸 Bounding Box는 아무래도 완전히 정확하지 않기 때문에 물체를 정확히 감싸도록 조정해주는 <strong>선형회귀 모델(Bounding Box Regression)</strong> 을 넣었습니다. 설명의 간편화를 위해 bounding box를 bBox로 명시하겠습니다.
<strong>bBox</strong>의 input 값은 N개의 Training Pairs로 이루어져 있습니다.<br/>
<img src="https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG" alt="12"/></p>
<p><strong>x, y, w, h</strong>는 각각 bBox의 <strong>x, y좌표(위치), width(너비), height(높이)</strong> 를 뜻 합니다.
<strong>P</strong>는 <strong>선택된 bBox</strong>이고 <strong>G</strong>는 <strong>Ground Truth(실제 값) bBox</strong>입니다.
<strong><u>선택된 P를 G에 맞추도록 transform 하는 것을 학습하는 것이 Bounding Box Regression의 목표</u></strong>입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG" alt="13"/></p>
<h2 id="r" class="text-2xl font-bold mt-8 mb-4">R-CNN의 한계점</h2>
<p>R-CNN은 이전 Object Detection 방법들에 비해 굉장히 뛰어난 성능을 보였다는 것은 분명하지만 명확한 몇몇 단점들을 가지고 있습니다.</p>
<ul>
<li>입력 이미지에 대하여 <strong>CPU 기반의 Selective Search</strong>를 진행해야 하므로 많은 시간이 소요됩니다.</li>
<li>모든 RoI를 CNN에 넣어야 하기 때문에 <strong>2,000번의 CNN 연산이 필요</strong>합니다.
<ul>
<li>학습(training)과 평가(testing) 과정에서 많은 시간이 필요합니다.</li>
</ul>
</li>
<li>전체 아키텍처에서 SVM, Regressor 모듈이 CNN과 분리되어 있습니다.
<ul>
<li>CNN은 고정되므로, SVM과 Bounding Box Regression 결과로 CNN을 업데이트할 수 없습니다.</li>
<li>다시 말해 <u>end-to-end 방식으로 학습할 수 없습니다.</u></li>
</ul>
</li>
</ul>
<p>마지막 줄을 정리하자면 <strong>Back Propagation이 안된다</strong> 고 볼 수 있습니다. R-CNN은 Multi-stage Trainig을 수행하며, <strong>SVM, Bounding Box Regression</strong>에서 학습한 결과가 <strong>CNN</strong>을 <strong>업데이트 시키지 못하는 것</strong>입니다.</p>
<p>이러한 한계가 존재하지만, R-CNN은 <strong>최초로 Object Detection에 딥러닝 방법인 CNN을 적용시켰다는 점</strong>과 <strong>이후 2-Stage detector들의 구조에 막대한 영향을 미쳤다는 점</strong>에서 의미가 큰 논문입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG" alt="14"/></p>
<p>다음으로 등장한 <strong>Fast R-CNN</strong>에 대해 알아보겠습니다.</p>
<h1 id="f" class="text-3xl font-bold mt-8 mb-4">Fast R-CNN</h1>
<p>**Fast R-CNN*도 <strong>R-CNN</strong>과 똑같이 처음에 <strong>Selective Search</strong>를 통해 Region Proposal을 뽑아내긴 합니다.
하지만 R-CNN과 다르게 <strong><u>뽑아낸 영역</u></strong><u>을 Crop하지 않고 그대로 가지고 있고, <strong>전체 이미지를 CNN Model에 집어 넣은 후</strong> CNN으로부터 나온 <strong>Feature Map</strong>에 <strong>RoI Projection</strong>을 하는 방식을 택했습니다.</u></p>
<p>즉, input image 1장으로부터 <strong>CNN Model</strong>에 들어가는 이미지는 <strong>2,000장 → 1장</strong>이 되었습니다.
<img src="https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG" alt="15"/></p>
<p>이 Projection 한 bBox들을 <strong>RoI Pooling</strong> 하는 것이 <strong>Fast R-CNN의 핵심</strong>입니다.
Projection시킨 RoI를 **FCs(Fully Connected Layer)**에 넣기 위해서는 같은 Resolution의 Feature map이 필요합니다.
하지만 Selective Search를 통해 구해졌던 RoI 영역은 각각 다른 크기를 가지고 있습니다.
따라서 이 Resolution의 크키를 맞추기 위해 <strong>RoI Pooling</strong>을 수행합니다.</p>
<p><strong>RoI Pooling</strong>은 간단히 말해서 <strong><u>크기가 다른 Feature Map</u></strong><u>의 <strong>Region</strong>마다 <strong>Stride를 다르게 Max Pooling을 진행</strong></u>하여 결과값을 맞추는 방법입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG" alt="16"/></p>
<p>마지막으로 <strong>Fixed Length Feature Vector를 FCs</strong>에 집어넣은 후 두 자식 layer인 output layer로 뻗어 나가 <strong>Classification</strong>과 <strong>bBOx Regression</strong>을 진행합니다.
이는 R-CNN과 비슷하게 진행되지만 <strong>Fast R-CNN</strong>은 <strong>Softmax</strong>를 사용하여 Classification을 진행하였다는 점에서 차이를 보입니다.</p>
<p>이후 등장한 Faster R-CNN에 대해 알아보도록 하겠습니다.</p>
<h1 id="f" class="text-3xl font-bold mt-8 mb-4">Faster R-CNN</h1>
<p>Faster R-CNN의 논문에서는, Region Proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가할 것이라고 말하고 있습니다.
따라서 Faster R-CNN은 Deep Network를 사용하여 Region Proposal를 진행하는 **RPN (Region Proposal Networks)**를 소개합니다.
<img src="https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG" alt="17"/></p>
<h2 id="f" class="text-2xl font-bold mt-8 mb-4">Faster R-CNN: Region Proposal Network(RPN)</h2>
<p><strong>RPN</strong>의 <strong>input</strong>은 image의 <strong>Feature Map</strong>이고, <strong>output</strong>은 Object proposal들의 <strong>Sample</strong>입니다.
이 Sample들을 Fast R-CNN과 동일하게 <strong>RoI Pooling</strong>을 한 후, <strong>Classification, bBox Regression</strong>을 진행합니다.
Pretrained된 CNN을 거쳐서 나온 <strong>Feature Map</strong>은 ZFNet 기준으로 256-d, VGG-16 기준으로 512-d를 갖게됩니다.(여기서 d는 차원으로 이해하였습니다.)</p>
<p>이때 ZFNet은 ILSVRC 2013에서 우승한 CNN구조이고, VGG-16은 옥스퍼드 대학의 연구팀 VGG에 의해 개발된 VGGNet의 모델 중 하나로 16개 층으로 구성된 모델이며, 2014년 이미지넷 인식대회에서 준우승한 모델을 의미합니다.
이 <strong>Feature Map</strong>을 k개의 Anchor box를 통해 영역을 정하고 <strong>Classification Layer</strong>와 <strong>bBox Regression</strong>을 거쳐서 물체가 위치한 곳을 학습하게 됩니다.
<strong><u>여기서 Classification Layer가 물체가 있는지 없는지만 확인하므로, Class의 수는 2입니다.</u></strong>
<img src="https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG" alt="18"/></p>
<h1 id="" class="text-3xl font-bold mt-8 mb-4">요약하자면...</h1>
<p>3가지 모델들을 정리해보자면 아래와 같습니다.
발전 방향의 순서대로 각 모델의 장단점에 대해 정리하였습니다. 확실히 faster R-CNN으로 갈수록 사진당 속도가 빠르게 증가하였음을 확인할 수 있으며, 동시에 정확도로 볼 수 있는 mAP의 값 또한 미비하지만 증가했음을 확인할 수 있습니다.
<img src="https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG" alt="19"/>
<img src="https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG" alt="20"/></p>
<hr/>
<p>PS. 추가 문의사항 및 질문은 환영합니다. 그를 통해 저도 더 성장할 수 있을테니까요. 긴 글 읽어주셔서 감사합니다.</p>
<h1 id="r" class="text-3xl font-bold mt-8 mb-4">Reference</h1>
<ul>
<li>논문
<ul>
<li><a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li><a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">Fast R-CNN</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></li>
</ul>
</li>
</ul><div class="mt-10 border-t pt-10"></div></article></div><!--$--><!--/$--><script src="/_next/static/chunks/475fa0f5a019bf38.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"default\"]\n3:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"default\"]\n5:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"OutletBoundary\"]\n6:\"$Sreact.suspense\"\n8:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"ViewportBoundary\"]\na:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"MetadataBoundary\"]\nc:I[68027,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/865c404e1d9a0c65.js\"],\"default\"]\n:HL[\"/_next/static/chunks/2f40a2027cd59172.css\",\"style\"]\n:HL[\"/_next/static/chunks/b9ef641e76e3a351.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"8lJiHtAmlyU3nNFMbG8_k\",\"c\":[\"\",\"2022-07-18-RCNN_Fast_RCNN_Faster_RCNN\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"2022-07-18-RCNN_Fast_RCNN_Faster_RCNN\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/2f40a2027cd59172.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-screen flex flex-col font-sans\",\"children\":[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L2\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L3\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L4\",[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/b9ef641e76e3a351.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/6b8d09032578b975.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L5\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@7\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L8\",null,{\"children\":\"$@9\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$La\",null,{\"children\":[\"$\",\"$6\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@b\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,":HL[\"https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG\",\"image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"flex gap-10\",\"children\":[[\"$\",\"article\",null,{\"className\":\"flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-8 not-prose border-b pb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold mb-4\",\"children\":\"[PaperReview] RCNN, Fast R-CNN, Faster R-CNN\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400\",\"children\":[[\"$\",\"time\",null,{\"dateTime\":\"2022-07-18\",\"children\":\"July 18, 2022\"}],\"$undefined\"]}]]}],[[\"$\",\"p\",\"p-0\",{\"children\":[\"2021년 머신러닝 스터디에서 진행하였던 \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://docs.google.com/presentation/d/1VOyGj7IH4VlivV45MAWQFb-j1qn9RHmu/edit?usp=sharing\u0026ouid=103416735755875236001\u0026rtpof=true\u0026sd=true\",\"children\":\"'R-CNN, Fast R-CNN, Faster R-CNN' 논문 리뷰\"}],\"를 재구성한 포스팅임을 미리 알려드립니다.\"]}],\"\\n\",[\"$\",\"h1\",\"h1-0\",{\"id\":\"i\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"introduction\"}],\"\\n\",[\"$\",\"p\",\"p-1\",{\"children\":\"위 논문은 CV(Computer Vision), 즉 이미치 처리에 있어 기반이 되는 논문입니다.\"}],\"\\n\",[\"$\",\"p\",\"p-2\",{\"children\":[\"오늘은 R-CNN, Fast R-CNN, Faster R-CNN에 대해 이야기해보고자 합니다.\\n우선 이 기법들이 무엇인가에 대해 알아볼 필요가 있습니다. 위 3가지 기술들은 Object Detection, 즉 사물을 인식하는 방법에 들어가는 하나의 기법입니다.\\n그렇다면 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Object Detection\"}],\"은 무엇일까요?\"]}],\"\\n\",[\"$\",\"h1\",\"h1-1\",{\"id\":\"o\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"Object Detection (사물을 인식하는 방법)\"}],\"\\n\",[\"$\",\"p\",\"p-3\",{\"children\":[\"이미지 내에서 사물을 인식하는 방법에는 다양한 유형이 존재합니다. 여러 물체에 대해 어떤 물체인지 분류하는 것을 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Classification\"}],\"이라 합니다.\\n또한 그 물체가 어디 있는지 박스를 통해 위치 정보를 나타내는 것을 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Localization\"}],\"이라 합니다.\"]}],\"\\n\",[\"$\",\"p\",\"p-4\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"Object Detection\"}],\"이란, 다수의 사물이 존재하는 상황에서 각 사물의 위치와 클래스를 찾는 작업을 말합니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG\",\"alt\":\"2\"}]]}],\"\\n\",[\"$\",\"h1\",\"h1-2\",{\"id\":\"1\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"1-Stage Detector vs 2-Stage Detector\"}],\"\\n\",[\"$\",\"p\",\"p-5\",{\"children\":\"Deep Learning을 이용한 object detection은 크게 1-stage detector와 2-stage detector로 나눌 수 있습니다. 아래 그림을 통해 설명을 이어 진행하겠습니다.\"}],\"\\n\",[\"$\",\"p\",\"p-6\",{\"children\":[\"가운데 수평 화살표를 기준으로 위 쪽에 위치한 논문들이 2-stage detector 논문들이고, 아래 쪽에 위치한 논문들이 1-stage detector 논문들 입니다. 이번 시간에는 위 쪽에 위치한 2-stage detector 논문들에 대해 공부해볼 예정입니다.\\n그렇다면 여기서 계속 나오는 말, stage detector란 무엇이고, 1, 2 stage detector의 차이는 무엇일까요??\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG\",\"alt\":\"3\"}]]}],\"\\n\",[\"$\",\"p\",\"p-7\",{\"children\":\"Object Detection 문제는 앞에서 이야기하였듯이 물체의 위치를 찾는 localization 문제와, 물체를 식별하는 classification 문제를 합한 것입니다.\\n1-stage detector는 이 두 문제를 동시에 행하는 방법이고, 2-stage detector는 이 두 문제를 순차적으로 행하는 방법입니다.\\n따라서 1-stage detector가 비교적 빠르지만 정확도가 낮고, 반대로 2-stage detector가 비교적 느리지만 정확도가 높습니다.\"}],\"\\n\",[\"$\",\"p\",\"p-8\",{\"children\":[\"오늘 우리가 중점적으로 다룰 R-CNN, Fast R-CNN, Faster R-CNN은 2-stage detector의 대표적인 기법입니다. (참고로 1-stage에는 YOLO 계열과 SSD 계열이 포함됩니다.)\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG\",\"alt\":\"4\"}]]}],\"\\n\",[\"$\",\"h1\",\"h1-3\",{\"id\":\"r\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"R-CNN: Regions with CNN features\"}],\"\\n\",[\"$\",\"p\",\"p-9\",{\"children\":\"Object Detection 분야에서 최초로 딥러닝(CNN)을 적용시킨 것이 R-CNN입니다. 논문에서 소개하는 R-CNN의 구조는 다음과 같습니다.\"}],\"\\n\",[\"$\",\"ol\",\"ol-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"Selective search를 이용해 2,000개의 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"ROI(Region of Interest)를 추출\"}],\".\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":\"각 ROI에 대하여 wraping을 수행하여 동일한 크기의 입력 이미지로 변경.\"}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"Warped image를 CNN에 넣어서(forward) 이미지 \",[\"$\",\"u\",\"u-0\",{\"children\":\"feature를 추출\"}],\".\"]}],\"\\n\",\"$Ld\",\"\\n\",\"$Le\",\"\\n\"]}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\"],\"$L3a\"]}],\"$L3b\"]}]\n"])</script><script>self.__next_f.push([1,"3c:I[24170,[\"/_next/static/chunks/6b8d09032578b975.js\"],\"default\"]\n3d:I[55132,[\"/_next/static/chunks/6b8d09032578b975.js\"],\"default\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG\",\"image\"]\n:HL[\"https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG\",\"image\"]\nd:[\"$\",\"li\",\"li-3\",{\"children\":[\"해당 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"feature\"}],\"를 SVM에 넣어 클래스(class) 분류 결과를 얻음.\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"이때 각 클래스에 독립적으로 훈련된 이진(binary) SVM을 사용.\"}],\"\\n\"]}],\"\\n\"]}]\ne:[\"$\",\"li\",\"li-4\",{\"children\":[\"해당 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"feature\"}],\"를 regressor에 넣어 위치(bounding box)를 예측.\"]}]\nf:[\"$\",\"p\",\"p-10\",{\"children\":[\"R-CNN은 2-stage detector로서 전체 task를 두 가지 단계로 나누어 진행합니다.\\n첫번째 단계는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Region Proposal\"}],\"로 \",[\"$\",\"u\",\"u-0\",{\"children\":\"물체의 위치를 찾는 일\"}],\"이고, 두번째 단계는 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Region Classification\"}],\"으로 \",[\"$\",\"u\",\"u-1\",{\"children\":\"물체를 분류하는 일\"}],\"입니다.\"]}]\n10:[\"$\",\"p\",\"p-11\",{\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG\",\"alt\":\"5\"}],\"\\n\",[\"$\",\"img\",\"img-1\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG\",\"alt\":\"6\"}]]}]\n11:[\"$\",\"p\",\"p-12\",{\"children\":\"이 논문에서는 위 두 task를 행하기 위해 구조를 아래의 3가지 모듈로 나누어 놓았습니다.\"}]\n12:[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"카테고리와 무관하게 물체의 영역을 찾는 모듈인 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Region Proposal\"}]]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Large Convolutional Neural Network인 CNN\"}]]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"Classification을 위한 선형 지도학습 모델인 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Support Vector Machine\"}],\"인 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"SVM\"}]]}],\"\\n\"]}]\n13:[\"$\",\"p\",\"p-13\",{\"children\":[\"CNN의 경우, 이전 논문인 \",[\"$\",\"a\",\"a-0\",{\"href\":\"https://sehooni.github.io/paperreview/ImageNet_Classification_with_Deep_Convolutional_Neural_"])</script><script>self.__next_f.push([1,"Networks/\",\"children\":\"ImageNet Classification with Deep Convolutional Neural Networks\"}],\"에서 이야기 했기에 생략하고,\\nRegion proposal과 SVM에 대해 알아보겠습니다.\"]}]\n14:[\"$\",\"h2\",\"h2-0\",{\"id\":\"r\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Region Proposal (영역 찾기)\"}]\n15:[\"$\",\"p\",\"p-14\",{\"children\":[\"R-CNN의 구조를 조금 더 자세히 살펴보면 다음과 같습니다. R-CNN은 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Region Proposal 단계\"}],\"에서 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Selective Search\"}],\"라는 알고리즘을 이용하였습니다.\\nSelective Search 알고리즘은 Segmentation 분야에 많이 쓰이는 알고리즘이며, 객체오 주변 간의 색감(Color), 질감(Texture) 차이, 다른 물체에 둘러 싸여 있는지(Enclosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘 입니다.\"]}]\n16:[\"$\",\"p\",\"p-15\",{\"children\":[[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG\",\"alt\":\"8\"}],\"\\n오른쪽 그림과 같이 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Bounding box\"}],\"들을 Random하게 많이 생성을 하고, 이들을 조금씩 Merge해 나가면서 물체를 인식해 나가는 방식으로 되어있습니다.\\n이 알고리즘에 대해서는 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"\\\"물체의 위치를 파악하기 위한 알고리즘이구나\\\"\"}],\" 정도로 이해하면 될 것 같습니다. 이와 관련한 논문도 \",[\"$\",\"a\",\"a-0\",{\"href\":\"http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\",\"children\":\"Selective Search\"}],\"논문이 있습니다.\"]}]\n17:[\"$\",\"p\",\"p-16\",{\"children\":[\"R-CNN에서는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Selective Search\"}],\" 알고리즘을 통해 한 이미지에서 2,000개의 Region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈(224*224)로 압축하여 통일시키는 작업(Wraping)을 거칩니다.\"]}]\n18:[\"$\",\"h2\",\"h2-1\",{\"id\":\"s\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"SVM (Support Vector Machine)\"}]\n19:[\"$\",\"p\",\"p-17\",{\"children\":[\"CNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Linear SVM\"}],\"을 이용하여 classification을 진행합니다.(Category-Specific Linear SVMs)\\n여기서 의문점이 드는 부분들이 있습니다. 분명 CNN에서 Classifier로 softmax를 사용한 것 같은데 R-CNN에서는 왜 SVM을 사용하였을까? 라는 의문점 말입니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG\",\"alt\":\"9\"}]]}]\n1a:[\"$\",\"p\",\"p-18\",{\"children\":[\"이 논문에서는 CNN을 fine-tunning할 때, 이미지의 positive/negative examples와 SVM을 학습할 때의 이미지의 positive/negative examples를 따로 정의했습니다.\\nCNN fine-tunning에서는 IoU가 0.5가 넘으면 positive로 두었고, 이외에는 \\\"background\\\"로 labeled해 두었습니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG\",\"alt\":\"10\"}]]}]\n1b:[\"$\",\"p\",\"p-19\",{\"children\":[\"여기서 IoU란, Intersection ove Union의 줄임말로, 두 바운딩 박스가 겹치는 비율을 의미합니다.\\n성능 평가를 예시로 들자면, mAP@0.5는 정답과 예측의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미를 말하며, NMS 계산을 예시로 들자면, 같은 클래스(class)끼리 IoU가 50% 이상일 때 낮은 confidence의 box를 제거한다는 의미입니다.\\n이때, mAP는 mean Average Precision을 의미합니다. 이러한 정보는 Computer Vision 분야에서 성능평가 지표로 사용됩니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG\",\"alt\":\"11\"}]]}]\n1c:[\"$\",\"p\",\"p-20\",{\"children\":\"반면에 SVM을 학습할 때의 ground-tr"])</script><script>self.__next_f.push([1,"uth boxes만 positive example로 두고, IoU가 0.3 미만은 모두 negative, 나머지는 전부 무시하게 됩니다.\\nSVM을 CNN fine-tunning과 같은 값을 두고 학습했을 때 훨씬 성능이 안 좋게 나왔다고 합니다.\\n시기상으로 fine-tunning 학습 데이터가 많지 않았기 때문에 IoU가 0.5~1 사이의 positive 영역들이 다소 정확하지 않았고, 때문에 바로 softmax classifier를 적용시켰을 때 성능이 좋지 않아서 위와 같이 SVM을 학습하는 과정이 필요했다고 보여집니다.\"}]\n1d:[\"$\",\"p\",\"p-21\",{\"children\":\"어찌됐는 SVM은 CNN으로부터 추출된 각각의 feature vector들의 점수를 class 별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 classifier입니다.\\nR-CNN 논문에서도 이에 대한 설명을 기재해 놓았습니다.\\nVOC2007 데이터셋 기준으로 Softmax를 사용하였을 때 mAP값이 54.2%에서 50.9%로 떨어졌다고 합니다.\"}]\n1e:[\"$\",\"h2\",\"h2-2\",{\"id\":\"r\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"R-CNN: Bounding Box Regression\"}]\n1f:[\"$\",\"p\",\"p-22\",{\"children\":[\"Selective Search로 만들어낸 Bounding Box는 아무래도 완전히 정확하지 않기 때문에 물체를 정확히 감싸도록 조정해주는 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"선형회귀 모델(Bounding Box Regression)\"}],\" 을 넣었습니다. 설명의 간편화를 위해 bounding box를 bBox로 명시하겠습니다.\\n\",[\"$\",\"strong\",\"strong-1\",{\"children\":\"bBox\"}],\"의 input 값은 N개의 Training Pairs로 이루어져 있습니다.\",[\"$\",\"br\",\"br-0\",{}],\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG\",\"alt\":\"12\"}]]}]\n20:[\"$\",\"p\",\"p-23\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"x, y, w, h\"}],\"는 각각 bBox의 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"x, y좌표(위치), width(너비), height(높이)\"}],\" 를 뜻 합니다.\\n\",[\"$\",\"strong\",\"strong-2\",{\"children\":\"P\"}],\"는 \",[\"$\",\"strong\",\"strong-3\",{\"children\":\"선택된 bBox\"}],\"이고 \",[\"$\",\"strong\",\"strong-4\",{\"children\":\"G\"}],\"는 \",[\"$\",\"strong\",\"strong-5\",{\"children\":\"Ground Truth(실제 값) bBox\"}],\"입니다.\\n\",[\"$\",\"strong\",\"strong-6\",{\"children\":[\"$\",\"u\",\"u-0\",{\"children\":\"선택된 P를 G에 맞추도록 transform 하는 것을 학습하는 것이 Bounding Box Regression의 목표\"}]}],\"입니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG\",\"alt\":\"13\"}]]}]\n21:[\"$\",\"h2\",\"h2-3\",{\"id\":\"r\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"R-CNN의 한계점\"}]\n22:[\"$\",\"p\",\"p-24\",{\"children\":\"R-CNN은 이전 Object Detection 방법들에 비해 굉장히 뛰어난 성능을 보였다는 것은 분명하지만 명확한 몇몇 단점들을 가지고 있습니다.\"}]\n23:[\"$\",\"ul\",\"ul-1\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"입력 이미지에 대하여 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"CPU 기반의 Selective Search\"}],\"를 진행해야 하므로 많은 시간이 소요됩니다.\"]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"모든 RoI를 CNN에 넣어야 하기 때문에 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"2,000번의 CNN 연산이 필요\"}],\"합니다.\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"학습(training)과 평가(testing) 과정에서 많은 시간이 필요합니다.\"}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",\"li-2\",{\"children\":[\"전체 아키텍처에서 SVM, Regressor 모듈이 CNN과 분리되어 있습니다.\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":\"CNN은 고정되므로, SVM과 Bounding Box Regression 결과로 CNN을 업데이트할 수 없습니다.\"}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"다시 말해 \",[\"$\",\"u\",\"u-0\",{\"children\":\"end-to-end 방식으로 학습할 수 없습니다.\"}]]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n24:[\"$\",\"p\",\"p-25\",{\"children\":[\"마지막 줄을 정리하자면 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Back Propagation이 안된다\"}],"])</script><script>self.__next_f.push([1,"\" 고 볼 수 있습니다. R-CNN은 Multi-stage Trainig을 수행하며, \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"SVM, Bounding Box Regression\"}],\"에서 학습한 결과가 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"CNN\"}],\"을 \",[\"$\",\"strong\",\"strong-3\",{\"children\":\"업데이트 시키지 못하는 것\"}],\"입니다.\"]}]\n25:[\"$\",\"p\",\"p-26\",{\"children\":[\"이러한 한계가 존재하지만, R-CNN은 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"최초로 Object Detection에 딥러닝 방법인 CNN을 적용시켰다는 점\"}],\"과 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"이후 2-Stage detector들의 구조에 막대한 영향을 미쳤다는 점\"}],\"에서 의미가 큰 논문입니다.\"]}]\n26:[\"$\",\"p\",\"p-27\",{\"children\":[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG\",\"alt\":\"14\"}]}]\n27:[\"$\",\"p\",\"p-28\",{\"children\":[\"다음으로 등장한 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Fast R-CNN\"}],\"에 대해 알아보겠습니다.\"]}]\n28:[\"$\",\"h1\",\"h1-4\",{\"id\":\"f\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"Fast R-CNN\"}]\n29:[\"$\",\"p\",\"p-29\",{\"children\":[\"**Fast R-CNN*도 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"R-CNN\"}],\"과 똑같이 처음에 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Selective Search\"}],\"를 통해 Region Proposal을 뽑아내긴 합니다.\\n하지만 R-CNN과 다르게 \",[\"$\",\"strong\",\"strong-2\",{\"children\":[\"$\",\"u\",\"u-0\",{\"children\":\"뽑아낸 영역\"}]}],[\"$\",\"u\",\"u-0\",{\"children\":[\"을 Crop하지 않고 그대로 가지고 있고, \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"전체 이미지를 CNN Model에 집어 넣은 후\"}],\" CNN으로부터 나온 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Feature Map\"}],\"에 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"RoI Projection\"}],\"을 하는 방식을 택했습니다.\"]}]]}]\n2a:[\"$\",\"p\",\"p-30\",{\"children\":[\"즉, input image 1장으로부터 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"CNN Model\"}],\"에 들어가는 이미지는 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"2,000장 → 1장\"}],\"이 되었습니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG\",\"alt\":\"15\"}]]}]\n2b:[\"$\",\"p\",\"p-31\",{\"children\":[\"이 Projection 한 bBox들을 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"RoI Pooling\"}],\" 하는 것이 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Fast R-CNN의 핵심\"}],\"입니다.\\nProjection시킨 RoI를 **FCs(Fully Connected Layer)**에 넣기 위해서는 같은 Resolution의 Feature map이 필요합니다.\\n하지만 Selective Search를 통해 구해졌던 RoI 영역은 각각 다른 크기를 가지고 있습니다.\\n따라서 이 Resolution의 크키를 맞추기 위해 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"RoI Pooling\"}],\"을 수행합니다.\"]}]\n2c:[\"$\",\"p\",\"p-32\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"RoI Pooling\"}],\"은 간단히 말해서 \",[\"$\",\"strong\",\"strong-1\",{\"children\":[\"$\",\"u\",\"u-0\",{\"children\":\"크기가 다른 Feature Map\"}]}],[\"$\",\"u\",\"u-0\",{\"children\":[\"의 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Region\"}],\"마다 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Stride를 다르게 Max Pooling을 진행\"}]]}],\"하여 결과값을 맞추는 방법입니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG\",\"alt\":\"16\"}]]}]\n2d:[\"$\",\"p\",\"p-33\",{\"children\":[\"마지막으로 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Fixed Length Feature Vector를 FCs\"}],\"에 집어넣은 후 두 자식 layer인 output layer로 뻗어 나가 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Classification\"}],\"과 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"bBOx Regression\"}],\"을 진행합니다.\\n이는 R-CNN과 비슷하게 진행되지만 \",[\"$\",\"strong\",\"strong-3\",{\"children\":\"Fast R-CNN\"}],\"은 \",[\"$\",\"strong\",\"strong-4\",{\"children\":\"Softmax\"}],\"를 사용하여 Classification을 진행하였다는 점에서 차이를 보입니다.\"]}]\n2e:[\"$\",\"p\",\"p-34\",{\"children\":\"이후 등장한 Faster R-CNN에 대해 알아보도록 하겠습니다.\"}"])</script><script>self.__next_f.push([1,"]\n2f:[\"$\",\"h1\",\"h1-5\",{\"id\":\"f\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"Faster R-CNN\"}]\n30:[\"$\",\"p\",\"p-35\",{\"children\":[\"Faster R-CNN의 논문에서는, Region Proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가할 것이라고 말하고 있습니다.\\n따라서 Faster R-CNN은 Deep Network를 사용하여 Region Proposal를 진행하는 **RPN (Region Proposal Networks)**를 소개합니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG\",\"alt\":\"17\"}]]}]\n31:[\"$\",\"h2\",\"h2-4\",{\"id\":\"f\",\"className\":\"text-2xl font-bold mt-8 mb-4\",\"children\":\"Faster R-CNN: Region Proposal Network(RPN)\"}]\n32:[\"$\",\"p\",\"p-36\",{\"children\":[[\"$\",\"strong\",\"strong-0\",{\"children\":\"RPN\"}],\"의 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"input\"}],\"은 image의 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"Feature Map\"}],\"이고, \",[\"$\",\"strong\",\"strong-3\",{\"children\":\"output\"}],\"은 Object proposal들의 \",[\"$\",\"strong\",\"strong-4\",{\"children\":\"Sample\"}],\"입니다.\\n이 Sample들을 Fast R-CNN과 동일하게 \",[\"$\",\"strong\",\"strong-5\",{\"children\":\"RoI Pooling\"}],\"을 한 후, \",[\"$\",\"strong\",\"strong-6\",{\"children\":\"Classification, bBox Regression\"}],\"을 진행합니다.\\nPretrained된 CNN을 거쳐서 나온 \",[\"$\",\"strong\",\"strong-7\",{\"children\":\"Feature Map\"}],\"은 ZFNet 기준으로 256-d, VGG-16 기준으로 512-d를 갖게됩니다.(여기서 d는 차원으로 이해하였습니다.)\"]}]\n33:[\"$\",\"p\",\"p-37\",{\"children\":[\"이때 ZFNet은 ILSVRC 2013에서 우승한 CNN구조이고, VGG-16은 옥스퍼드 대학의 연구팀 VGG에 의해 개발된 VGGNet의 모델 중 하나로 16개 층으로 구성된 모델이며, 2014년 이미지넷 인식대회에서 준우승한 모델을 의미합니다.\\n이 \",[\"$\",\"strong\",\"strong-0\",{\"children\":\"Feature Map\"}],\"을 k개의 Anchor box를 통해 영역을 정하고 \",[\"$\",\"strong\",\"strong-1\",{\"children\":\"Classification Layer\"}],\"와 \",[\"$\",\"strong\",\"strong-2\",{\"children\":\"bBox Regression\"}],\"을 거쳐서 물체가 위치한 곳을 학습하게 됩니다.\\n\",[\"$\",\"strong\",\"strong-3\",{\"children\":[\"$\",\"u\",\"u-0\",{\"children\":\"여기서 Classification Layer가 물체가 있는지 없는지만 확인하므로, Class의 수는 2입니다.\"}]}],\"\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG\",\"alt\":\"18\"}]]}]\n34:[\"$\",\"h1\",\"h1-6\",{\"id\":\"\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"요약하자면...\"}]\n35:[\"$\",\"p\",\"p-38\",{\"children\":[\"3가지 모델들을 정리해보자면 아래와 같습니다.\\n발전 방향의 순서대로 각 모델의 장단점에 대해 정리하였습니다. 확실히 faster R-CNN으로 갈수록 사진당 속도가 빠르게 증가하였음을 확인할 수 있으며, 동시에 정확도로 볼 수 있는 mAP의 값 또한 미비하지만 증가했음을 확인할 수 있습니다.\\n\",[\"$\",\"img\",\"img-0\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG\",\"alt\":\"19\"}],\"\\n\",[\"$\",\"img\",\"img-1\",{\"src\":\"https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG\",\"alt\":\"20\"}]]}]\n36:[\"$\",\"hr\",\"hr-0\",{}]\n37:[\"$\",\"p\",\"p-39\",{\"children\":\"PS. 추가 문의사항 및 질문은 환영합니다. 그를 통해 저도 더 성장할 수 있을테니까요. 긴 글 읽어주셔서 감사합니다.\"}]\n38:[\"$\",\"h1\",\"h1-7\",{\"id\":\"r\",\"className\":\"text-3xl font-bold mt-8 mb-4\",\"children\":\"Reference\"}]\n39:[\"$\",\"ul\",\"ul-2\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"논문\\n\",[\"$\",\"ul\",\"ul-0\",{\"children\":[\"\\n\",[\"$\",\"li\",\"li-0\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://arxiv.org/abs/1311.2524\",\"children\":\"Rich feature hierarchies for accurate object detection and semantic segmentation\"}]}],\"\\n\",[\"$\",\"li\",\"li-1\",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html\",\"children\":\"Fast R-CNN\"}]}],\"\\n\",[\"$\",\"li\",\"li-2\""])</script><script>self.__next_f.push([1,",{\"children\":[\"$\",\"a\",\"a-0\",{\"href\":\"https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html\",\"children\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\"}]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}]\n3a:[\"$\",\"$L3c\",null,{}]\n3e:T3e1c,"])</script><script>self.__next_f.push([1,"\n2021년 머신러닝 스터디에서 진행하였던 ['R-CNN, Fast R-CNN, Faster R-CNN' 논문 리뷰](https://docs.google.com/presentation/d/1VOyGj7IH4VlivV45MAWQFb-j1qn9RHmu/edit?usp=sharing\u0026ouid=103416735755875236001\u0026rtpof=true\u0026sd=true)를 재구성한 포스팅임을 미리 알려드립니다.\n\n# introduction\n위 논문은 CV(Computer Vision), 즉 이미치 처리에 있어 기반이 되는 논문입니다.\n\n오늘은 R-CNN, Fast R-CNN, Faster R-CNN에 대해 이야기해보고자 합니다.\n우선 이 기법들이 무엇인가에 대해 알아볼 필요가 있습니다. 위 3가지 기술들은 Object Detection, 즉 사물을 인식하는 방법에 들어가는 하나의 기법입니다.\n그렇다면 **Object Detection**은 무엇일까요?\n\n# Object Detection (사물을 인식하는 방법)\n이미지 내에서 사물을 인식하는 방법에는 다양한 유형이 존재합니다. 여러 물체에 대해 어떤 물체인지 분류하는 것을 **Classification**이라 합니다.\n또한 그 물체가 어디 있는지 박스를 통해 위치 정보를 나타내는 것을 **Localization**이라 합니다.\n\n**Object Detection**이란, 다수의 사물이 존재하는 상황에서 각 사물의 위치와 클래스를 찾는 작업을 말합니다.\n![2](https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG)\n\n# 1-Stage Detector vs 2-Stage Detector\nDeep Learning을 이용한 object detection은 크게 1-stage detector와 2-stage detector로 나눌 수 있습니다. 아래 그림을 통해 설명을 이어 진행하겠습니다.\n\n가운데 수평 화살표를 기준으로 위 쪽에 위치한 논문들이 2-stage detector 논문들이고, 아래 쪽에 위치한 논문들이 1-stage detector 논문들 입니다. 이번 시간에는 위 쪽에 위치한 2-stage detector 논문들에 대해 공부해볼 예정입니다.\n그렇다면 여기서 계속 나오는 말, stage detector란 무엇이고, 1, 2 stage detector의 차이는 무엇일까요??\n![3](https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG)\n\nObject Detection 문제는 앞에서 이야기하였듯이 물체의 위치를 찾는 localization 문제와, 물체를 식별하는 classification 문제를 합한 것입니다.\n1-stage detector는 이 두 문제를 동시에 행하는 방법이고, 2-stage detector는 이 두 문제를 순차적으로 행하는 방법입니다.\n따라서 1-stage detector가 비교적 빠르지만 정확도가 낮고, 반대로 2-stage detector가 비교적 느리지만 정확도가 높습니다.\n\n오늘 우리가 중점적으로 다룰 R-CNN, Fast R-CNN, Faster R-CNN은 2-stage detector의 대표적인 기법입니다. (참고로 1-stage에는 YOLO 계열과 SSD 계열이 포함됩니다.)\n![4](https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG)\n\n# R-CNN: Regions with CNN features\nObject Detection 분야에서 최초로 딥러닝(CNN)을 적용시킨 것이 R-CNN입니다. 논문에서 소개하는 R-CNN의 구조는 다음과 같습니다.\n1. Selective search를 이용해 2,000개의 **ROI(Region of Interest)를 추출**.\n2. 각 ROI에 대하여 wraping을 수행하여 동일한 크기의 입력 이미지로 변경.\n3. Warped image를 CNN에 넣어서(forward) 이미지 \u003cu\u003efeature를 추출\u003c/u\u003e.\n4. 해당 **feature**를 SVM에 넣어 클래스(class) 분류 결과를 얻음.\n   - 이때 각 클래스에 독립적으로 훈련된 이진(binary) SVM을 사용. \n5. 해당 **feature**를 regressor에 넣어 위치(bounding box)를 예측.\n\nR-CNN은 2-stage detector로서 전체 task를 두 가지 단계로 나누어 진행합니다.\n첫번째 단계는 **Region Proposal**로 \u003cu\u003e물체의 위치를 찾는 일\u003c/u\u003e이고, 두번째 단계는 **Region Classification**으로 \u003cu\u003e물체를 분류하는 일\u003c/u\u003e입니다.\n\n![5](https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG)\n![6](https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG)\n\n이 논문에서는 위 두 task를 행하기 위해 구조를 아래의 3가지 모듈로 나누어 놓았습니다.\n- 카테고리와 무관하게 물체의 영역을 찾는 모듈인 **Region Proposal**\n- 각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 **Large Convolutional Neural Network인 CNN**\n- Classification을 위한 선형 지도학습 모델인 **Support Vector Machine**인 **SVM**\n\nCNN의 경우, 이전 논문인 [ImageNet Classification with Deep Convolutional Neural Networks](https://sehooni.github.io/paperreview/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks/)에서 이야기 했기에 생략하고, \nRegion proposal과 SVM에 대해 알아보겠습니다.\n\n## Region Proposal (영역 찾기)\nR-CNN의 구조를 조금 더 자세히 살펴보면 다음과 같습니다. R-CNN은 **Region Proposal 단계**에서 **Selective Search**라는 알고리즘을 이용하였습니다.\nSelective Search 알고리즘은 Segmentation 분야에 많이 쓰이는 알고리즘이며, 객체오 주변 간의 색감(Color), 질감(Texture) 차이, 다른 물체에 둘러 싸여 있는지(Enclosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘 입니다.\n\n![8](https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG)\n오른쪽 그림과 같이 **Bounding box**들을 Random하게 많이 생성을 하고, 이들을 조금씩 Merge해 나가면서 물체를 인식해 나가는 방식으로 되어있습니다.\n 이 알고리즘에 대해서는 **\"물체의 위치를 파악하기 위한 알고리즘이구나\"** 정도로 이해하면 될 것 같습니다. 이와 관련한 논문도 [Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)논문이 있습니다.\n\nR-CNN에서는 **Selective Search** 알고리즘을 통해 한 이미지에서 2,000개의 Region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈(224*224)로 압축하여 통일시키는 작업(Wraping)을 거칩니다.\n\n## SVM (Support Vector Machine)\nCNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, **Linear SVM**을 이용하여 classification을 진행합니다.(Category-Specific Linear SVMs) \n여기서 의문점이 드는 부분들이 있습니다. 분명 CNN에서 Classifier로 softmax를 사용한 것 같은데 R-CNN에서는 왜 SVM을 사용하였을까? 라는 의문점 말입니다.\n![9](https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG)\n\n이 논문에서는 CNN을 fine-tunning할 때, 이미지의 positive/negative examples와 SVM을 학습할 때의 이미지의 positive/negative examples를 따로 정의했습니다. \nCNN fine-tunning에서는 IoU가 0.5가 넘으면 positive로 두었고, 이외에는 \"background\"로 labeled해 두었습니다.\n![10](https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG)\n\n여기서 IoU란, Intersection ove Union의 줄임말로, 두 바운딩 박스가 겹치는 비율을 의미합니다. \n성능 평가를 예시로 들자면, mAP@0.5는 정답과 예측의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미를 말하며, NMS 계산을 예시로 들자면, 같은 클래스(class)끼리 IoU가 50% 이상일 때 낮은 confidence의 box를 제거한다는 의미입니다. \n이때, mAP는 mean Average Precision을 의미합니다. 이러한 정보는 Computer Vision 분야에서 성능평가 지표로 사용됩니다.\n![11](https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG)\n\n반면에 SVM을 학습할 때의 ground-truth boxes만 positive example로 두고, IoU가 0.3 미만은 모두 negative, 나머지는 전부 무시하게 됩니다.\nSVM을 CNN fine-tunning과 같은 값을 두고 학습했을 때 훨씬 성능이 안 좋게 나왔다고 합니다.\n시기상으로 fine-tunning 학습 데이터가 많지 않았기 때문에 IoU가 0.5~1 사이의 positive 영역들이 다소 정확하지 않았고, 때문에 바로 softmax classifier를 적용시켰을 때 성능이 좋지 않아서 위와 같이 SVM을 학습하는 과정이 필요했다고 보여집니다.\n\n어찌됐는 SVM은 CNN으로부터 추출된 각각의 feature vector들의 점수를 class 별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 classifier입니다.\nR-CNN 논문에서도 이에 대한 설명을 기재해 놓았습니다.\nVOC2007 데이터셋 기준으로 Softmax를 사용하였을 때 mAP값이 54.2%에서 50.9%로 떨어졌다고 합니다.\n\n## R-CNN: Bounding Box Regression\nSelective Search로 만들어낸 Bounding Box는 아무래도 완전히 정확하지 않기 때문에 물체를 정확히 감싸도록 조정해주는 **선형회귀 모델(Bounding Box Regression)** 을 넣었습니다. 설명의 간편화를 위해 bounding box를 bBox로 명시하겠습니다.\n**bBox**의 input 값은 N개의 Training Pairs로 이루어져 있습니다.  \n![12](https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG)\n\n**x, y, w, h**는 각각 bBox의 **x, y좌표(위치), width(너비), height(높이)** 를 뜻 합니다.\n**P**는 **선택된 bBox**이고 **G**는 **Ground Truth(실제 값) bBox**입니다.\n**\u003cu\u003e선택된 P를 G에 맞추도록 transform 하는 것을 학습하는 것이 Bounding Box Regression의 목표**\u003c/u\u003e입니다.\n![13](https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG)\n\n## R-CNN의 한계점\nR-CNN은 이전 Object Detection 방법들에 비해 굉장히 뛰어난 성능을 보였다는 것은 분명하지만 명확한 몇몇 단점들을 가지고 있습니다.\n- 입력 이미지에 대하여 **CPU 기반의 Selective Search**를 진행해야 하므로 많은 시간이 소요됩니다.\n- 모든 RoI를 CNN에 넣어야 하기 때문에 **2,000번의 CNN 연산이 필요**합니다.\n  - 학습(training)과 평가(testing) 과정에서 많은 시간이 필요합니다.\n- 전체 아키텍처에서 SVM, Regressor 모듈이 CNN과 분리되어 있습니다.\n  - CNN은 고정되므로, SVM과 Bounding Box Regression 결과로 CNN을 업데이트할 수 없습니다.\n  - 다시 말해 \u003cu\u003eend-to-end 방식으로 학습할 수 없습니다.\u003c/u\u003e\n\n마지막 줄을 정리하자면 **Back Propagation이 안된다** 고 볼 수 있습니다. R-CNN은 Multi-stage Trainig을 수행하며, **SVM, Bounding Box Regression**에서 학습한 결과가 **CNN**을 **업데이트 시키지 못하는 것**입니다.\n\n이러한 한계가 존재하지만, R-CNN은 **최초로 Object Detection에 딥러닝 방법인 CNN을 적용시켰다는 점**과 **이후 2-Stage detector들의 구조에 막대한 영향을 미쳤다는 점**에서 의미가 큰 논문입니다.\n    \n![14](https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG)\n\n다음으로 등장한 **Fast R-CNN**에 대해 알아보겠습니다.\n\n# Fast R-CNN\n**Fast R-CNN*도 **R-CNN**과 똑같이 처음에 **Selective Search**를 통해 Region Proposal을 뽑아내긴 합니다.\n하지만 R-CNN과 다르게 **\u003cu\u003e뽑아낸 영역**을 Crop하지 않고 그대로 가지고 있고, **전체 이미지를 CNN Model에 집어 넣은 후** CNN으로부터 나온 **Feature Map**에 **RoI Projection**을 하는 방식을 택했습니다.\u003c/u\u003e\n\n즉, input image 1장으로부터 **CNN Model**에 들어가는 이미지는 **2,000장 → 1장**이 되었습니다.\n![15](https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG)\n\n이 Projection 한 bBox들을 **RoI Pooling** 하는 것이 **Fast R-CNN의 핵심**입니다.\nProjection시킨 RoI를 **FCs(Fully Connected Layer)**에 넣기 위해서는 같은 Resolution의 Feature map이 필요합니다.\n하지만 Selective Search를 통해 구해졌던 RoI 영역은 각각 다른 크기를 가지고 있습니다.\n따라서 이 Resolution의 크키를 맞추기 위해 **RoI Pooling**을 수행합니다.\n\n**RoI Pooling**은 간단히 말해서 **\u003cu\u003e크기가 다른 Feature Map**의 **Region**마다 **Stride를 다르게 Max Pooling을 진행**\u003c/u\u003e하여 결과값을 맞추는 방법입니다.\n![16](https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG)\n\n마지막으로 **Fixed Length Feature Vector를 FCs**에 집어넣은 후 두 자식 layer인 output layer로 뻗어 나가 **Classification**과 **bBOx Regression**을 진행합니다.\n이는 R-CNN과 비슷하게 진행되지만 **Fast R-CNN**은 **Softmax**를 사용하여 Classification을 진행하였다는 점에서 차이를 보입니다.\n\n이후 등장한 Faster R-CNN에 대해 알아보도록 하겠습니다.\n\n# Faster R-CNN\nFaster R-CNN의 논문에서는, Region Proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가할 것이라고 말하고 있습니다.\n따라서 Faster R-CNN은 Deep Network를 사용하여 Region Proposal를 진행하는 **RPN (Region Proposal Networks)**를 소개합니다.\n![17](https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG)\n\n## Faster R-CNN: Region Proposal Network(RPN)\n**RPN**의 **input**은 image의 **Feature Map**이고, **output**은 Object proposal들의 **Sample**입니다. \n이 Sample들을 Fast R-CNN과 동일하게 **RoI Pooling**을 한 후, **Classification, bBox Regression**을 진행합니다.\nPretrained된 CNN을 거쳐서 나온 **Feature Map**은 ZFNet 기준으로 256-d, VGG-16 기준으로 512-d를 갖게됩니다.(여기서 d는 차원으로 이해하였습니다.)\n\n이때 ZFNet은 ILSVRC 2013에서 우승한 CNN구조이고, VGG-16은 옥스퍼드 대학의 연구팀 VGG에 의해 개발된 VGGNet의 모델 중 하나로 16개 층으로 구성된 모델이며, 2014년 이미지넷 인식대회에서 준우승한 모델을 의미합니다.\n이 **Feature Map**을 k개의 Anchor box를 통해 영역을 정하고 **Classification Layer**와 **bBox Regression**을 거쳐서 물체가 위치한 곳을 학습하게 됩니다.\n**\u003cu\u003e여기서 Classification Layer가 물체가 있는지 없는지만 확인하므로, Class의 수는 2입니다.**\u003c/u\u003e\n![18](https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG)\n\n# 요약하자면...\n3가지 모델들을 정리해보자면 아래와 같습니다.\n발전 방향의 순서대로 각 모델의 장단점에 대해 정리하였습니다. 확실히 faster R-CNN으로 갈수록 사진당 속도가 빠르게 증가하였음을 확인할 수 있으며, 동시에 정확도로 볼 수 있는 mAP의 값 또한 미비하지만 증가했음을 확인할 수 있습니다.\n![19](https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG)\n![20](https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG)\n\n---\nPS. 추가 문의사항 및 질문은 환영합니다. 그를 통해 저도 더 성장할 수 있을테니까요. 긴 글 읽어주셔서 감사합니다. \n\n\n# Reference\n- 논문\n  - [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)\n  - [Fast R-CNN](https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html)\n  - [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html)"])</script><script>self.__next_f.push([1,"3b:[\"$\",\"$L3d\",null,{\"content\":\"$3e\"}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"title\",\"0\",{\"children\":\"Sehoon's Workspace\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Welcome to my page!\"}]]\n7:null\n"])</script></body></html>