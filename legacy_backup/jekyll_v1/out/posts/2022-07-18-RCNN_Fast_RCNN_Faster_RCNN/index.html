<!DOCTYPE html><!--oAIH0_g3w6LOdbw_0qDiA--><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/8fa8e9b5e3e9b2cd.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/cbd55ab9639e1e66.js"/><script src="/_next/static/chunks/1abc3924204d7dd0.js" async=""></script><script src="/_next/static/chunks/9c23f44fff36548a.js" async=""></script><script src="/_next/static/chunks/5944084dd90310d5.js" async=""></script><script src="/_next/static/chunks/turbopack-aeaf7046609aeecd.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/247eb132b7f7b574.js" async=""></script><script src="/_next/static/chunks/796e69ae18b2784c.js" async=""></script><script src="/_next/static/chunks/a8f82a9835eb887b.js" async=""></script><title>[PaperReview] RCNN, Fast R-CNN, Faster R-CNN</title><meta name="description" content="RCNN, Fast R-CNN, Faster R-CNN Paper 리뷰"/><link rel="icon" href="/favicon.ico?favicon.0b3bf435.ico" sizes="256x256" type="image/x-icon"/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased bg-white dark:bg-black text-gray-900 dark:text-white flex min-h-screen"><div hidden=""><!--$--><!--/$--></div><aside class="w-64 h-screen sticky top-0 bg-gray-50 dark:bg-gray-900 border-r border-gray-200 dark:border-gray-800 p-6 overflow-y-auto hidden lg:block"><div class="mb-8"><h1 class="text-2xl font-bold font-sans tracking-tight"><a href="/">Sehoon&#x27;s Workspace</a></h1><p class="text-sm text-gray-500 mt-2">Tech &amp; Study Blog</p></div><nav class="space-y-8"><div><h3 class="text-xs font-semibold text-gray-400 uppercase tracking-wider mb-2">Menu</h3><ul class="space-y-1"><li><a class="block py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors text-sm" href="/">Recent Posts</a></li><li><a class="block py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors text-sm" href="/about/">About</a></li></ul></div><div><h3 class="text-xs font-semibold text-gray-400 uppercase tracking-wider mb-2">Categories</h3><ul class="space-y-1"><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/proteomics/"><span>proteomics</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->9<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/nlp/"><span>NLP</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->6<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/blog/"><span>Blog</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->5<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/jetson/"><span>Jetson</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->4<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/paperreview/"><span>PaperReview</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->4<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/neural_style_transfer/"><span>Neural_Style_Transfer</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->3<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/jekyll/"><span>jekyll</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->2<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/ml/"><span>ML</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->2<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/algorithms/"><span>algorithms</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/contest/"><span>Contest</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/capstone/"><span>Capstone</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/linux/"><span>Linux</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/etc/"><span>ETC</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/datastructure/"><span>DataStructure</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li><li><a class="flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm" href="/category/dl/"><span>DL</span><span class="text-xs text-gray-400 group-hover:text-blue-500 font-mono">(<!-- -->1<!-- -->)</span></a></li></ul></div></nav><div class="mt-8 pt-8 border-t border-gray-200 dark:border-gray-800"><div class="flex space-x-4"></div></div></aside><main class="flex-1 min-w-0"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-10"><div class="relative"><article class="prose prose-lg dark:prose-invert max-w-none"><header class="mb-10 not-prose border-b border-gray-100 dark:border-gray-800 pb-8"><div class="mb-4 text-sm text-gray-500 flex items-center space-x-2"><a class="font-medium text-blue-600 hover:underline" href="/category/paperreview/">PaperReview</a><span>•</span><time></time></div><h1 class="text-4xl font-extrabold tracking-tight text-gray-900 dark:text-white mb-4">[PaperReview] RCNN, Fast R-CNN, Faster R-CNN</h1></header><div><p>2021년 머신러닝 스터디에서 진행하였던 <a href="https://docs.google.com/presentation/d/1VOyGj7IH4VlivV45MAWQFb-j1qn9RHmu/edit?usp=sharing&#x26;ouid=103416735755875236001&#x26;rtpof=true&#x26;sd=true">'R-CNN, Fast R-CNN, Faster R-CNN' 논문 리뷰</a>를 재구성한 포스팅임을 미리 알려드립니다.</p>
<h1>introduction</h1>
<p>위 논문은 CV(Computer Vision), 즉 이미치 처리에 있어 기반이 되는 논문입니다.</p>
<p>오늘은 R-CNN, Fast R-CNN, Faster R-CNN에 대해 이야기해보고자 합니다.
우선 이 기법들이 무엇인가에 대해 알아볼 필요가 있습니다. 위 3가지 기술들은 Object Detection, 즉 사물을 인식하는 방법에 들어가는 하나의 기법입니다.
그렇다면 <strong>Object Detection</strong>은 무엇일까요?</p>
<h1>Object Detection (사물을 인식하는 방법)</h1>
<p>이미지 내에서 사물을 인식하는 방법에는 다양한 유형이 존재합니다. 여러 물체에 대해 어떤 물체인지 분류하는 것을 <strong>Classification</strong>이라 합니다.
또한 그 물체가 어디 있는지 박스를 통해 위치 정보를 나타내는 것을 <strong>Localization</strong>이라 합니다.</p>
<p><strong>Object Detection</strong>이란, 다수의 사물이 존재하는 상황에서 각 사물의 위치와 클래스를 찾는 작업을 말합니다.
<img src="https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG" alt="2"></p>
<h1>1-Stage Detector vs 2-Stage Detector</h1>
<p>Deep Learning을 이용한 object detection은 크게 1-stage detector와 2-stage detector로 나눌 수 있습니다. 아래 그림을 통해 설명을 이어 진행하겠습니다.</p>
<p>가운데 수평 화살표를 기준으로 위 쪽에 위치한 논문들이 2-stage detector 논문들이고, 아래 쪽에 위치한 논문들이 1-stage detector 논문들 입니다. 이번 시간에는 위 쪽에 위치한 2-stage detector 논문들에 대해 공부해볼 예정입니다.
그렇다면 여기서 계속 나오는 말, stage detector란 무엇이고, 1, 2 stage detector의 차이는 무엇일까요??
<img src="https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG" alt="3"></p>
<p>Object Detection 문제는 앞에서 이야기하였듯이 물체의 위치를 찾는 localization 문제와, 물체를 식별하는 classification 문제를 합한 것입니다.
1-stage detector는 이 두 문제를 동시에 행하는 방법이고, 2-stage detector는 이 두 문제를 순차적으로 행하는 방법입니다.
따라서 1-stage detector가 비교적 빠르지만 정확도가 낮고, 반대로 2-stage detector가 비교적 느리지만 정확도가 높습니다.</p>
<p>오늘 우리가 중점적으로 다룰 R-CNN, Fast R-CNN, Faster R-CNN은 2-stage detector의 대표적인 기법입니다. (참고로 1-stage에는 YOLO 계열과 SSD 계열이 포함됩니다.)
<img src="https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG" alt="4"></p>
<h1>R-CNN: Regions with CNN features</h1>
<p>Object Detection 분야에서 최초로 딥러닝(CNN)을 적용시킨 것이 R-CNN입니다. 논문에서 소개하는 R-CNN의 구조는 다음과 같습니다.</p>
<ol>
<li>Selective search를 이용해 2,000개의 <strong>ROI(Region of Interest)를 추출</strong>.</li>
<li>각 ROI에 대하여 wraping을 수행하여 동일한 크기의 입력 이미지로 변경.</li>
<li>Warped image를 CNN에 넣어서(forward) 이미지 feature를 추출.</li>
<li>해당 <strong>feature</strong>를 SVM에 넣어 클래스(class) 분류 결과를 얻음.
<ul>
<li>이때 각 클래스에 독립적으로 훈련된 이진(binary) SVM을 사용.</li>
</ul>
</li>
<li>해당 <strong>feature</strong>를 regressor에 넣어 위치(bounding box)를 예측.</li>
</ol>
<p>R-CNN은 2-stage detector로서 전체 task를 두 가지 단계로 나누어 진행합니다.
첫번째 단계는 <strong>Region Proposal</strong>로 물체의 위치를 찾는 일이고, 두번째 단계는 <strong>Region Classification</strong>으로 물체를 분류하는 일입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG" alt="5">
<img src="https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG" alt="6"></p>
<p>이 논문에서는 위 두 task를 행하기 위해 구조를 아래의 3가지 모듈로 나누어 놓았습니다.</p>
<ul>
<li>카테고리와 무관하게 물체의 영역을 찾는 모듈인 <strong>Region Proposal</strong></li>
<li>각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 <strong>Large Convolutional Neural Network인 CNN</strong></li>
<li>Classification을 위한 선형 지도학습 모델인 <strong>Support Vector Machine</strong>인 <strong>SVM</strong></li>
</ul>
<p>CNN의 경우, 이전 논문인 <a href="https://sehooni.github.io/paperreview/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks/">ImageNet Classification with Deep Convolutional Neural Networks</a>에서 이야기 했기에 생략하고,
Region proposal과 SVM에 대해 알아보겠습니다.</p>
<h2>Region Proposal (영역 찾기)</h2>
<p>R-CNN의 구조를 조금 더 자세히 살펴보면 다음과 같습니다. R-CNN은 <strong>Region Proposal 단계</strong>에서 <strong>Selective Search</strong>라는 알고리즘을 이용하였습니다.
Selective Search 알고리즘은 Segmentation 분야에 많이 쓰이는 알고리즘이며, 객체오 주변 간의 색감(Color), 질감(Texture) 차이, 다른 물체에 둘러 싸여 있는지(Enclosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘 입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG" alt="8">
오른쪽 그림과 같이 <strong>Bounding box</strong>들을 Random하게 많이 생성을 하고, 이들을 조금씩 Merge해 나가면서 물체를 인식해 나가는 방식으로 되어있습니다.
이 알고리즘에 대해서는 <strong>"물체의 위치를 파악하기 위한 알고리즘이구나"</strong> 정도로 이해하면 될 것 같습니다. 이와 관련한 논문도 <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">Selective Search</a>논문이 있습니다.</p>
<p>R-CNN에서는 <strong>Selective Search</strong> 알고리즘을 통해 한 이미지에서 2,000개의 Region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈(224*224)로 압축하여 통일시키는 작업(Wraping)을 거칩니다.</p>
<h2>SVM (Support Vector Machine)</h2>
<p>CNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, <strong>Linear SVM</strong>을 이용하여 classification을 진행합니다.(Category-Specific Linear SVMs)
여기서 의문점이 드는 부분들이 있습니다. 분명 CNN에서 Classifier로 softmax를 사용한 것 같은데 R-CNN에서는 왜 SVM을 사용하였을까? 라는 의문점 말입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG" alt="9"></p>
<p>이 논문에서는 CNN을 fine-tunning할 때, 이미지의 positive/negative examples와 SVM을 학습할 때의 이미지의 positive/negative examples를 따로 정의했습니다.
CNN fine-tunning에서는 IoU가 0.5가 넘으면 positive로 두었고, 이외에는 "background"로 labeled해 두었습니다.
<img src="https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG" alt="10"></p>
<p>여기서 IoU란, Intersection ove Union의 줄임말로, 두 바운딩 박스가 겹치는 비율을 의미합니다.
성능 평가를 예시로 들자면, mAP@0.5는 정답과 예측의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미를 말하며, NMS 계산을 예시로 들자면, 같은 클래스(class)끼리 IoU가 50% 이상일 때 낮은 confidence의 box를 제거한다는 의미입니다.
이때, mAP는 mean Average Precision을 의미합니다. 이러한 정보는 Computer Vision 분야에서 성능평가 지표로 사용됩니다.
<img src="https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG" alt="11"></p>
<p>반면에 SVM을 학습할 때의 ground-truth boxes만 positive example로 두고, IoU가 0.3 미만은 모두 negative, 나머지는 전부 무시하게 됩니다.
SVM을 CNN fine-tunning과 같은 값을 두고 학습했을 때 훨씬 성능이 안 좋게 나왔다고 합니다.
시기상으로 fine-tunning 학습 데이터가 많지 않았기 때문에 IoU가 0.5~1 사이의 positive 영역들이 다소 정확하지 않았고, 때문에 바로 softmax classifier를 적용시켰을 때 성능이 좋지 않아서 위와 같이 SVM을 학습하는 과정이 필요했다고 보여집니다.</p>
<p>어찌됐는 SVM은 CNN으로부터 추출된 각각의 feature vector들의 점수를 class 별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 classifier입니다.
R-CNN 논문에서도 이에 대한 설명을 기재해 놓았습니다.
VOC2007 데이터셋 기준으로 Softmax를 사용하였을 때 mAP값이 54.2%에서 50.9%로 떨어졌다고 합니다.</p>
<h2>R-CNN: Bounding Box Regression</h2>
<p>Selective Search로 만들어낸 Bounding Box는 아무래도 완전히 정확하지 않기 때문에 물체를 정확히 감싸도록 조정해주는 <strong>선형회귀 모델(Bounding Box Regression)</strong> 을 넣었습니다. 설명의 간편화를 위해 bounding box를 bBox로 명시하겠습니다.
<strong>bBox</strong>의 input 값은 N개의 Training Pairs로 이루어져 있습니다.<br>
<img src="https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG" alt="12"></p>
<p><strong>x, y, w, h</strong>는 각각 bBox의 <strong>x, y좌표(위치), width(너비), height(높이)</strong> 를 뜻 합니다.
<strong>P</strong>는 <strong>선택된 bBox</strong>이고 <strong>G</strong>는 <strong>Ground Truth(실제 값) bBox</strong>입니다.
<strong>선택된 P를 G에 맞추도록 transform 하는 것을 학습하는 것이 Bounding Box Regression의 목표</strong>입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG" alt="13"></p>
<h2>R-CNN의 한계점</h2>
<p>R-CNN은 이전 Object Detection 방법들에 비해 굉장히 뛰어난 성능을 보였다는 것은 분명하지만 명확한 몇몇 단점들을 가지고 있습니다.</p>
<ul>
<li>입력 이미지에 대하여 <strong>CPU 기반의 Selective Search</strong>를 진행해야 하므로 많은 시간이 소요됩니다.</li>
<li>모든 RoI를 CNN에 넣어야 하기 때문에 <strong>2,000번의 CNN 연산이 필요</strong>합니다.
<ul>
<li>학습(training)과 평가(testing) 과정에서 많은 시간이 필요합니다.</li>
</ul>
</li>
<li>전체 아키텍처에서 SVM, Regressor 모듈이 CNN과 분리되어 있습니다.
<ul>
<li>CNN은 고정되므로, SVM과 Bounding Box Regression 결과로 CNN을 업데이트할 수 없습니다.</li>
<li>다시 말해 end-to-end 방식으로 학습할 수 없습니다.</li>
</ul>
</li>
</ul>
<p>마지막 줄을 정리하자면 <strong>Back Propagation이 안된다</strong> 고 볼 수 있습니다. R-CNN은 Multi-stage Trainig을 수행하며, <strong>SVM, Bounding Box Regression</strong>에서 학습한 결과가 <strong>CNN</strong>을 <strong>업데이트 시키지 못하는 것</strong>입니다.</p>
<p>이러한 한계가 존재하지만, R-CNN은 <strong>최초로 Object Detection에 딥러닝 방법인 CNN을 적용시켰다는 점</strong>과 <strong>이후 2-Stage detector들의 구조에 막대한 영향을 미쳤다는 점</strong>에서 의미가 큰 논문입니다.</p>
<p><img src="https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG" alt="14"></p>
<p>다음으로 등장한 <strong>Fast R-CNN</strong>에 대해 알아보겠습니다.</p>
<h1>Fast R-CNN</h1>
<p>**Fast R-CNN*도 <strong>R-CNN</strong>과 똑같이 처음에 <strong>Selective Search</strong>를 통해 Region Proposal을 뽑아내긴 합니다.
하지만 R-CNN과 다르게 <strong>뽑아낸 영역</strong>을 Crop하지 않고 그대로 가지고 있고, <strong>전체 이미지를 CNN Model에 집어 넣은 후</strong> CNN으로부터 나온 <strong>Feature Map</strong>에 <strong>RoI Projection</strong>을 하는 방식을 택했습니다.</p>
<p>즉, input image 1장으로부터 <strong>CNN Model</strong>에 들어가는 이미지는 <strong>2,000장 → 1장</strong>이 되었습니다.
<img src="https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG" alt="15"></p>
<p>이 Projection 한 bBox들을 <strong>RoI Pooling</strong> 하는 것이 <strong>Fast R-CNN의 핵심</strong>입니다.
Projection시킨 RoI를 **FCs(Fully Connected Layer)**에 넣기 위해서는 같은 Resolution의 Feature map이 필요합니다.
하지만 Selective Search를 통해 구해졌던 RoI 영역은 각각 다른 크기를 가지고 있습니다.
따라서 이 Resolution의 크키를 맞추기 위해 <strong>RoI Pooling</strong>을 수행합니다.</p>
<p><strong>RoI Pooling</strong>은 간단히 말해서 <strong>크기가 다른 Feature Map</strong>의 <strong>Region</strong>마다 <strong>Stride를 다르게 Max Pooling을 진행</strong>하여 결과값을 맞추는 방법입니다.
<img src="https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG" alt="16"></p>
<p>마지막으로 <strong>Fixed Length Feature Vector를 FCs</strong>에 집어넣은 후 두 자식 layer인 output layer로 뻗어 나가 <strong>Classification</strong>과 <strong>bBOx Regression</strong>을 진행합니다.
이는 R-CNN과 비슷하게 진행되지만 <strong>Fast R-CNN</strong>은 <strong>Softmax</strong>를 사용하여 Classification을 진행하였다는 점에서 차이를 보입니다.</p>
<p>이후 등장한 Faster R-CNN에 대해 알아보도록 하겠습니다.</p>
<h1>Faster R-CNN</h1>
<p>Faster R-CNN의 논문에서는, Region Proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가할 것이라고 말하고 있습니다.
따라서 Faster R-CNN은 Deep Network를 사용하여 Region Proposal를 진행하는 **RPN (Region Proposal Networks)**를 소개합니다.
<img src="https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG" alt="17"></p>
<h2>Faster R-CNN: Region Proposal Network(RPN)</h2>
<p><strong>RPN</strong>의 <strong>input</strong>은 image의 <strong>Feature Map</strong>이고, <strong>output</strong>은 Object proposal들의 <strong>Sample</strong>입니다.
이 Sample들을 Fast R-CNN과 동일하게 <strong>RoI Pooling</strong>을 한 후, <strong>Classification, bBox Regression</strong>을 진행합니다.
Pretrained된 CNN을 거쳐서 나온 <strong>Feature Map</strong>은 ZFNet 기준으로 256-d, VGG-16 기준으로 512-d를 갖게됩니다.(여기서 d는 차원으로 이해하였습니다.)</p>
<p>이때 ZFNet은 ILSVRC 2013에서 우승한 CNN구조이고, VGG-16은 옥스퍼드 대학의 연구팀 VGG에 의해 개발된 VGGNet의 모델 중 하나로 16개 층으로 구성된 모델이며, 2014년 이미지넷 인식대회에서 준우승한 모델을 의미합니다.
이 <strong>Feature Map</strong>을 k개의 Anchor box를 통해 영역을 정하고 <strong>Classification Layer</strong>와 <strong>bBox Regression</strong>을 거쳐서 물체가 위치한 곳을 학습하게 됩니다.
<strong>여기서 Classification Layer가 물체가 있는지 없는지만 확인하므로, Class의 수는 2입니다.</strong>
<img src="https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG" alt="18"></p>
<h1>요약하자면...</h1>
<p>3가지 모델들을 정리해보자면 아래와 같습니다.
발전 방향의 순서대로 각 모델의 장단점에 대해 정리하였습니다. 확실히 faster R-CNN으로 갈수록 사진당 속도가 빠르게 증가하였음을 확인할 수 있으며, 동시에 정확도로 볼 수 있는 mAP의 값 또한 미비하지만 증가했음을 확인할 수 있습니다.
<img src="https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG" alt="19">
<img src="https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG" alt="20"></p>
<hr>
<p>PS. 추가 문의사항 및 질문은 환영합니다. 그를 통해 저도 더 성장할 수 있을테니까요. 긴 글 읽어주셔서 감사합니다.</p>
<h1>Reference</h1>
<ul>
<li>논문
<ul>
<li><a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li><a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">Fast R-CNN</a></li>
<li><a href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></li>
</ul>
</li>
</ul>
</div></article></div><!--$--><!--/$--></div></main><script src="/_next/static/chunks/cbd55ab9639e1e66.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n3:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n4:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"default\"]\n6:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"OutletBoundary\"]\n7:\"$Sreact.suspense\"\n9:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"ViewportBoundary\"]\nb:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"MetadataBoundary\"]\nd:I[68027,[],\"default\"]\n:HL[\"/_next/static/chunks/8fa8e9b5e3e9b2cd.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"oAIH0-g3w6LOdbw_0qDiA\",\"c\":[\"\",\"posts\",\"2022-07-18-RCNN_Fast_RCNN_Faster_RCNN\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"posts\",{\"children\":[[\"id\",\"2022-07-18-RCNN_Fast_RCNN_Faster_RCNN\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/8fa8e9b5e3e9b2cd.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/796e69ae18b2784c.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased bg-white dark:bg-black text-gray-900 dark:text-white flex min-h-screen\",\"children\":[\"$L2\",[\"$\",\"main\",null,{\"className\":\"flex-1 min-w-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-10\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/a8f82a9835eb887b.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L6\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@8\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L9\",null,{\"children\":\"$@a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lb\",null,{\"children\":[\"$\",\"$7\",null,{\"name\":\"Next.Metadata\",\"children\":\"$@c\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$d\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"e:I[22016,[\"/_next/static/chunks/796e69ae18b2784c.js\",\"/_next/static/chunks/a8f82a9835eb887b.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"2:[\"$\",\"aside\",null,{\"className\":\"w-64 h-screen sticky top-0 bg-gray-50 dark:bg-gray-900 border-r border-gray-200 dark:border-gray-800 p-6 overflow-y-auto hidden lg:block\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-2xl font-bold font-sans tracking-tight\",\"children\":[\"$\",\"$Le\",null,{\"href\":\"/\",\"children\":\"Sehoon's Workspace\"}]}],[\"$\",\"p\",null,{\"className\":\"text-sm text-gray-500 mt-2\",\"children\":\"Tech \u0026 Study Blog\"}]]}],[\"$\",\"nav\",null,{\"className\":\"space-y-8\",\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xs font-semibold text-gray-400 uppercase tracking-wider mb-2\",\"children\":\"Menu\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/\",\"className\":\"block py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors text-sm\",\"children\":\"Recent Posts\"}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/about\",\"className\":\"block py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors text-sm\",\"children\":\"About\"}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"h3\",null,{\"className\":\"text-xs font-semibold text-gray-400 uppercase tracking-wider mb-2\",\"children\":\"Categories\"}],[\"$\",\"ul\",null,{\"className\":\"space-y-1\",\"children\":[[\"$\",\"li\",\"proteomics\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/proteomics\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"proteomics\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",9,\")\"]}]]}]}],[\"$\",\"li\",\"NLP\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/nlp\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"NLP\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",6,\")\"]}]]}]}],[\"$\",\"li\",\"Blog\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/blog\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Blog\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",5,\")\"]}]]}]}],[\"$\",\"li\",\"Jetson\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/jetson\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Jetson\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",4,\")\"]}]]}]}],[\"$\",\"li\",\"PaperReview\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/paperreview\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"PaperReview\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",4,\")\"]}]]}]}],[\"$\",\"li\",\"Neural_Style_Transfer\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/neural_style_transfer\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Neural_Style_Transfer\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",3,\")\"]}]]}]}],[\"$\",\"li\",\"jekyll\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/jekyll\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"jekyll\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",2,\")\"]}]]}]}],[\"$\",\"li\",\"ML\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/ml\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[\"$Lf\",\"$L10\"]}]}],\"$L11\",\"$L12\",\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\"]}]]}]]}],\"$L18\"]}]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"span\",null,{\"children\":\"ML\"}]\n10:[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",2,\")\"]}]\n11:[\"$\",\"li\",\"algorithms\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/algorithms\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"algorithms\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n12:[\"$\",\"li\",\"Contest\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/contest\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Contest\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n13:[\"$\",\"li\",\"Capstone\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/capstone\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Capstone\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n14:[\"$\",\"li\",\"Linux\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/linux\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"Linux\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n15:[\"$\",\"li\",\"ETC\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/etc\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"ETC\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n16:[\"$\",\"li\",\"DataStructure\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/datastructure\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"DataStructure\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n17:[\"$\",\"li\",\"DL\",{\"children\":[\"$\",\"$Le\",null,{\"href\":\"/category/dl\",\"className\":\"flex items-center justify-between py-1.5 text-gray-700 dark:text-gray-300 hover:text-blue-600 transition-colors group text-sm\",\"children\":[[\"$\",\"span\",null,{\"children\":\"DL\"}],[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-400 group-hover:text-blue-500 font-mono\",\"children\":[\"(\",1,\")\"]}]]}]}]\n18:[\"$\",\"div\",null,{\"className\":\"mt-8 pt-8 border-t border-gray-200 dark:border-gray-800\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex space-x-4\"}]}]\n"])</script><script>self.__next_f.push([1,"19:T44ba,"])</script><script>self.__next_f.push([1,"\u003cp\u003e2021년 머신러닝 스터디에서 진행하였던 \u003ca href=\"https://docs.google.com/presentation/d/1VOyGj7IH4VlivV45MAWQFb-j1qn9RHmu/edit?usp=sharing\u0026#x26;ouid=103416735755875236001\u0026#x26;rtpof=true\u0026#x26;sd=true\"\u003e'R-CNN, Fast R-CNN, Faster R-CNN' 논문 리뷰\u003c/a\u003e를 재구성한 포스팅임을 미리 알려드립니다.\u003c/p\u003e\n\u003ch1\u003eintroduction\u003c/h1\u003e\n\u003cp\u003e위 논문은 CV(Computer Vision), 즉 이미치 처리에 있어 기반이 되는 논문입니다.\u003c/p\u003e\n\u003cp\u003e오늘은 R-CNN, Fast R-CNN, Faster R-CNN에 대해 이야기해보고자 합니다.\n우선 이 기법들이 무엇인가에 대해 알아볼 필요가 있습니다. 위 3가지 기술들은 Object Detection, 즉 사물을 인식하는 방법에 들어가는 하나의 기법입니다.\n그렇다면 \u003cstrong\u003eObject Detection\u003c/strong\u003e은 무엇일까요?\u003c/p\u003e\n\u003ch1\u003eObject Detection (사물을 인식하는 방법)\u003c/h1\u003e\n\u003cp\u003e이미지 내에서 사물을 인식하는 방법에는 다양한 유형이 존재합니다. 여러 물체에 대해 어떤 물체인지 분류하는 것을 \u003cstrong\u003eClassification\u003c/strong\u003e이라 합니다.\n또한 그 물체가 어디 있는지 박스를 통해 위치 정보를 나타내는 것을 \u003cstrong\u003eLocalization\u003c/strong\u003e이라 합니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eObject Detection\u003c/strong\u003e이란, 다수의 사물이 존재하는 상황에서 각 사물의 위치와 클래스를 찾는 작업을 말합니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/179476396-5f98f40f-c2f6-4b27-95be-78af55c06752.PNG\" alt=\"2\"\u003e\u003c/p\u003e\n\u003ch1\u003e1-Stage Detector vs 2-Stage Detector\u003c/h1\u003e\n\u003cp\u003eDeep Learning을 이용한 object detection은 크게 1-stage detector와 2-stage detector로 나눌 수 있습니다. 아래 그림을 통해 설명을 이어 진행하겠습니다.\u003c/p\u003e\n\u003cp\u003e가운데 수평 화살표를 기준으로 위 쪽에 위치한 논문들이 2-stage detector 논문들이고, 아래 쪽에 위치한 논문들이 1-stage detector 논문들 입니다. 이번 시간에는 위 쪽에 위치한 2-stage detector 논문들에 대해 공부해볼 예정입니다.\n그렇다면 여기서 계속 나오는 말, stage detector란 무엇이고, 1, 2 stage detector의 차이는 무엇일까요??\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/179479840-0670f524-9d21-458d-ab52-bd8257a136ea.PNG\" alt=\"3\"\u003e\u003c/p\u003e\n\u003cp\u003eObject Detection 문제는 앞에서 이야기하였듯이 물체의 위치를 찾는 localization 문제와, 물체를 식별하는 classification 문제를 합한 것입니다.\n1-stage detector는 이 두 문제를 동시에 행하는 방법이고, 2-stage detector는 이 두 문제를 순차적으로 행하는 방법입니다.\n따라서 1-stage detector가 비교적 빠르지만 정확도가 낮고, 반대로 2-stage detector가 비교적 느리지만 정확도가 높습니다.\u003c/p\u003e\n\u003cp\u003e오늘 우리가 중점적으로 다룰 R-CNN, Fast R-CNN, Faster R-CNN은 2-stage detector의 대표적인 기법입니다. (참고로 1-stage에는 YOLO 계열과 SSD 계열이 포함됩니다.)\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/179480874-347c4f7f-e29f-4e90-9f8d-ee996b56934f.PNG\" alt=\"4\"\u003e\u003c/p\u003e\n\u003ch1\u003eR-CNN: Regions with CNN features\u003c/h1\u003e\n\u003cp\u003eObject Detection 분야에서 최초로 딥러닝(CNN)을 적용시킨 것이 R-CNN입니다. 논문에서 소개하는 R-CNN의 구조는 다음과 같습니다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSelective search를 이용해 2,000개의 \u003cstrong\u003eROI(Region of Interest)를 추출\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e각 ROI에 대하여 wraping을 수행하여 동일한 크기의 입력 이미지로 변경.\u003c/li\u003e\n\u003cli\u003eWarped image를 CNN에 넣어서(forward) 이미지 feature를 추출.\u003c/li\u003e\n\u003cli\u003e해당 \u003cstrong\u003efeature\u003c/strong\u003e를 SVM에 넣어 클래스(class) 분류 결과를 얻음.\n\u003cul\u003e\n\u003cli\u003e이때 각 클래스에 독립적으로 훈련된 이진(binary) SVM을 사용.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e해당 \u003cstrong\u003efeature\u003c/strong\u003e를 regressor에 넣어 위치(bounding box)를 예측.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eR-CNN은 2-stage detector로서 전체 task를 두 가지 단계로 나누어 진행합니다.\n첫번째 단계는 \u003cstrong\u003eRegion Proposal\u003c/strong\u003e로 물체의 위치를 찾는 일이고, 두번째 단계는 \u003cstrong\u003eRegion Classification\u003c/strong\u003e으로 물체를 분류하는 일입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://user-images.githubusercontent.com/84653623/179483511-9fd51908-e38f-402d-8b7e-ceb1b5489851.PNG\" alt=\"5\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387235-141c14f3-4a71-4ac5-aa8d-cf8b1e558c08.PNG\" alt=\"6\"\u003e\u003c/p\u003e\n\u003cp\u003e이 논문에서는 위 두 task를 행하기 위해 구조를 아래의 3가지 모듈로 나누어 놓았습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e카테고리와 무관하게 물체의 영역을 찾는 모듈인 \u003cstrong\u003eRegion Proposal\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e각각의 영역으로부터 고정된 크기의 Feature Vector를 뽑아내는 \u003cstrong\u003eLarge Convolutional Neural Network인 CNN\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eClassification을 위한 선형 지도학습 모델인 \u003cstrong\u003eSupport Vector Machine\u003c/strong\u003e인 \u003cstrong\u003eSVM\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCNN의 경우, 이전 논문인 \u003ca href=\"https://sehooni.github.io/paperreview/ImageNet_Classification_with_Deep_Convolutional_Neural_Networks/\"\u003eImageNet Classification with Deep Convolutional Neural Networks\u003c/a\u003e에서 이야기 했기에 생략하고,\nRegion proposal과 SVM에 대해 알아보겠습니다.\u003c/p\u003e\n\u003ch2\u003eRegion Proposal (영역 찾기)\u003c/h2\u003e\n\u003cp\u003eR-CNN의 구조를 조금 더 자세히 살펴보면 다음과 같습니다. R-CNN은 \u003cstrong\u003eRegion Proposal 단계\u003c/strong\u003e에서 \u003cstrong\u003eSelective Search\u003c/strong\u003e라는 알고리즘을 이용하였습니다.\nSelective Search 알고리즘은 Segmentation 분야에 많이 쓰이는 알고리즘이며, 객체오 주변 간의 색감(Color), 질감(Texture) 차이, 다른 물체에 둘러 싸여 있는지(Enclosed) 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악할 수 있도록 하는 알고리즘 입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387319-05a0cf3c-c598-4053-903b-bf43757b6aa2.PNG\" alt=\"8\"\u003e\n오른쪽 그림과 같이 \u003cstrong\u003eBounding box\u003c/strong\u003e들을 Random하게 많이 생성을 하고, 이들을 조금씩 Merge해 나가면서 물체를 인식해 나가는 방식으로 되어있습니다.\n이 알고리즘에 대해서는 \u003cstrong\u003e\"물체의 위치를 파악하기 위한 알고리즘이구나\"\u003c/strong\u003e 정도로 이해하면 될 것 같습니다. 이와 관련한 논문도 \u003ca href=\"http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\"\u003eSelective Search\u003c/a\u003e논문이 있습니다.\u003c/p\u003e\n\u003cp\u003eR-CNN에서는 \u003cstrong\u003eSelective Search\u003c/strong\u003e 알고리즘을 통해 한 이미지에서 2,000개의 Region을 뽑아내고, 이들을 모두 CNN에 넣기 위해 같은 사이즈(224*224)로 압축하여 통일시키는 작업(Wraping)을 거칩니다.\u003c/p\u003e\n\u003ch2\u003eSVM (Support Vector Machine)\u003c/h2\u003e\n\u003cp\u003eCNN 모델로부터 Feature가 추출이 되고 Training Label이 적용되고 나면, \u003cstrong\u003eLinear SVM\u003c/strong\u003e을 이용하여 classification을 진행합니다.(Category-Specific Linear SVMs)\n여기서 의문점이 드는 부분들이 있습니다. 분명 CNN에서 Classifier로 softmax를 사용한 것 같은데 R-CNN에서는 왜 SVM을 사용하였을까? 라는 의문점 말입니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387322-810065ad-72fa-4385-8b93-1be6dfeec1c2.PNG\" alt=\"9\"\u003e\u003c/p\u003e\n\u003cp\u003e이 논문에서는 CNN을 fine-tunning할 때, 이미지의 positive/negative examples와 SVM을 학습할 때의 이미지의 positive/negative examples를 따로 정의했습니다.\nCNN fine-tunning에서는 IoU가 0.5가 넘으면 positive로 두었고, 이외에는 \"background\"로 labeled해 두었습니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387333-dbd08e1a-0059-477a-871d-3803171845a1.PNG\" alt=\"10\"\u003e\u003c/p\u003e\n\u003cp\u003e여기서 IoU란, Intersection ove Union의 줄임말로, 두 바운딩 박스가 겹치는 비율을 의미합니다.\n성능 평가를 예시로 들자면, mAP@0.5는 정답과 예측의 IoU가 50% 이상일 때 정답으로 판정하겠다는 의미를 말하며, NMS 계산을 예시로 들자면, 같은 클래스(class)끼리 IoU가 50% 이상일 때 낮은 confidence의 box를 제거한다는 의미입니다.\n이때, mAP는 mean Average Precision을 의미합니다. 이러한 정보는 Computer Vision 분야에서 성능평가 지표로 사용됩니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387340-20af13f3-52e9-4d5f-a0aa-d76c5b856d60.PNG\" alt=\"11\"\u003e\u003c/p\u003e\n\u003cp\u003e반면에 SVM을 학습할 때의 ground-truth boxes만 positive example로 두고, IoU가 0.3 미만은 모두 negative, 나머지는 전부 무시하게 됩니다.\nSVM을 CNN fine-tunning과 같은 값을 두고 학습했을 때 훨씬 성능이 안 좋게 나왔다고 합니다.\n시기상으로 fine-tunning 학습 데이터가 많지 않았기 때문에 IoU가 0.5~1 사이의 positive 영역들이 다소 정확하지 않았고, 때문에 바로 softmax classifier를 적용시켰을 때 성능이 좋지 않아서 위와 같이 SVM을 학습하는 과정이 필요했다고 보여집니다.\u003c/p\u003e\n\u003cp\u003e어찌됐는 SVM은 CNN으로부터 추출된 각각의 feature vector들의 점수를 class 별로 매기고, 객체인지 아닌지, 객체라면 어떤 객체인지 등을 판별하는 classifier입니다.\nR-CNN 논문에서도 이에 대한 설명을 기재해 놓았습니다.\nVOC2007 데이터셋 기준으로 Softmax를 사용하였을 때 mAP값이 54.2%에서 50.9%로 떨어졌다고 합니다.\u003c/p\u003e\n\u003ch2\u003eR-CNN: Bounding Box Regression\u003c/h2\u003e\n\u003cp\u003eSelective Search로 만들어낸 Bounding Box는 아무래도 완전히 정확하지 않기 때문에 물체를 정확히 감싸도록 조정해주는 \u003cstrong\u003e선형회귀 모델(Bounding Box Regression)\u003c/strong\u003e 을 넣었습니다. 설명의 간편화를 위해 bounding box를 bBox로 명시하겠습니다.\n\u003cstrong\u003ebBox\u003c/strong\u003e의 input 값은 N개의 Training Pairs로 이루어져 있습니다.\u003cbr\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387347-5761eb95-821a-4477-8d18-7e3c58af01fc.PNG\" alt=\"12\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ex, y, w, h\u003c/strong\u003e는 각각 bBox의 \u003cstrong\u003ex, y좌표(위치), width(너비), height(높이)\u003c/strong\u003e 를 뜻 합니다.\n\u003cstrong\u003eP\u003c/strong\u003e는 \u003cstrong\u003e선택된 bBox\u003c/strong\u003e이고 \u003cstrong\u003eG\u003c/strong\u003e는 \u003cstrong\u003eGround Truth(실제 값) bBox\u003c/strong\u003e입니다.\n\u003cstrong\u003e선택된 P를 G에 맞추도록 transform 하는 것을 학습하는 것이 Bounding Box Regression의 목표\u003c/strong\u003e입니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387361-08aa2812-44be-4ee2-a7d9-61d999450baf.PNG\" alt=\"13\"\u003e\u003c/p\u003e\n\u003ch2\u003eR-CNN의 한계점\u003c/h2\u003e\n\u003cp\u003eR-CNN은 이전 Object Detection 방법들에 비해 굉장히 뛰어난 성능을 보였다는 것은 분명하지만 명확한 몇몇 단점들을 가지고 있습니다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e입력 이미지에 대하여 \u003cstrong\u003eCPU 기반의 Selective Search\u003c/strong\u003e를 진행해야 하므로 많은 시간이 소요됩니다.\u003c/li\u003e\n\u003cli\u003e모든 RoI를 CNN에 넣어야 하기 때문에 \u003cstrong\u003e2,000번의 CNN 연산이 필요\u003c/strong\u003e합니다.\n\u003cul\u003e\n\u003cli\u003e학습(training)과 평가(testing) 과정에서 많은 시간이 필요합니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e전체 아키텍처에서 SVM, Regressor 모듈이 CNN과 분리되어 있습니다.\n\u003cul\u003e\n\u003cli\u003eCNN은 고정되므로, SVM과 Bounding Box Regression 결과로 CNN을 업데이트할 수 없습니다.\u003c/li\u003e\n\u003cli\u003e다시 말해 end-to-end 방식으로 학습할 수 없습니다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e마지막 줄을 정리하자면 \u003cstrong\u003eBack Propagation이 안된다\u003c/strong\u003e 고 볼 수 있습니다. R-CNN은 Multi-stage Trainig을 수행하며, \u003cstrong\u003eSVM, Bounding Box Regression\u003c/strong\u003e에서 학습한 결과가 \u003cstrong\u003eCNN\u003c/strong\u003e을 \u003cstrong\u003e업데이트 시키지 못하는 것\u003c/strong\u003e입니다.\u003c/p\u003e\n\u003cp\u003e이러한 한계가 존재하지만, R-CNN은 \u003cstrong\u003e최초로 Object Detection에 딥러닝 방법인 CNN을 적용시켰다는 점\u003c/strong\u003e과 \u003cstrong\u003e이후 2-Stage detector들의 구조에 막대한 영향을 미쳤다는 점\u003c/strong\u003e에서 의미가 큰 논문입니다.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387377-92f7e2e0-7e8f-410d-b79e-181fd84f43f1.PNG\" alt=\"14\"\u003e\u003c/p\u003e\n\u003cp\u003e다음으로 등장한 \u003cstrong\u003eFast R-CNN\u003c/strong\u003e에 대해 알아보겠습니다.\u003c/p\u003e\n\u003ch1\u003eFast R-CNN\u003c/h1\u003e\n\u003cp\u003e**Fast R-CNN*도 \u003cstrong\u003eR-CNN\u003c/strong\u003e과 똑같이 처음에 \u003cstrong\u003eSelective Search\u003c/strong\u003e를 통해 Region Proposal을 뽑아내긴 합니다.\n하지만 R-CNN과 다르게 \u003cstrong\u003e뽑아낸 영역\u003c/strong\u003e을 Crop하지 않고 그대로 가지고 있고, \u003cstrong\u003e전체 이미지를 CNN Model에 집어 넣은 후\u003c/strong\u003e CNN으로부터 나온 \u003cstrong\u003eFeature Map\u003c/strong\u003e에 \u003cstrong\u003eRoI Projection\u003c/strong\u003e을 하는 방식을 택했습니다.\u003c/p\u003e\n\u003cp\u003e즉, input image 1장으로부터 \u003cstrong\u003eCNN Model\u003c/strong\u003e에 들어가는 이미지는 \u003cstrong\u003e2,000장 → 1장\u003c/strong\u003e이 되었습니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387388-30643d6c-9a69-496c-9373-414c454f876b.PNG\" alt=\"15\"\u003e\u003c/p\u003e\n\u003cp\u003e이 Projection 한 bBox들을 \u003cstrong\u003eRoI Pooling\u003c/strong\u003e 하는 것이 \u003cstrong\u003eFast R-CNN의 핵심\u003c/strong\u003e입니다.\nProjection시킨 RoI를 **FCs(Fully Connected Layer)**에 넣기 위해서는 같은 Resolution의 Feature map이 필요합니다.\n하지만 Selective Search를 통해 구해졌던 RoI 영역은 각각 다른 크기를 가지고 있습니다.\n따라서 이 Resolution의 크키를 맞추기 위해 \u003cstrong\u003eRoI Pooling\u003c/strong\u003e을 수행합니다.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRoI Pooling\u003c/strong\u003e은 간단히 말해서 \u003cstrong\u003e크기가 다른 Feature Map\u003c/strong\u003e의 \u003cstrong\u003eRegion\u003c/strong\u003e마다 \u003cstrong\u003eStride를 다르게 Max Pooling을 진행\u003c/strong\u003e하여 결과값을 맞추는 방법입니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387395-c5fe9e4d-9fc5-4933-a403-78e7a653290f.PNG\" alt=\"16\"\u003e\u003c/p\u003e\n\u003cp\u003e마지막으로 \u003cstrong\u003eFixed Length Feature Vector를 FCs\u003c/strong\u003e에 집어넣은 후 두 자식 layer인 output layer로 뻗어 나가 \u003cstrong\u003eClassification\u003c/strong\u003e과 \u003cstrong\u003ebBOx Regression\u003c/strong\u003e을 진행합니다.\n이는 R-CNN과 비슷하게 진행되지만 \u003cstrong\u003eFast R-CNN\u003c/strong\u003e은 \u003cstrong\u003eSoftmax\u003c/strong\u003e를 사용하여 Classification을 진행하였다는 점에서 차이를 보입니다.\u003c/p\u003e\n\u003cp\u003e이후 등장한 Faster R-CNN에 대해 알아보도록 하겠습니다.\u003c/p\u003e\n\u003ch1\u003eFaster R-CNN\u003c/h1\u003e\n\u003cp\u003eFaster R-CNN의 논문에서는, Region Proposal 방법을 GPU를 통한 학습으로 진행하면 확실히 성능이 증가할 것이라고 말하고 있습니다.\n따라서 Faster R-CNN은 Deep Network를 사용하여 Region Proposal를 진행하는 **RPN (Region Proposal Networks)**를 소개합니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387402-98b2e298-e31c-426c-b926-717d949fdc2e.PNG\" alt=\"17\"\u003e\u003c/p\u003e\n\u003ch2\u003eFaster R-CNN: Region Proposal Network(RPN)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eRPN\u003c/strong\u003e의 \u003cstrong\u003einput\u003c/strong\u003e은 image의 \u003cstrong\u003eFeature Map\u003c/strong\u003e이고, \u003cstrong\u003eoutput\u003c/strong\u003e은 Object proposal들의 \u003cstrong\u003eSample\u003c/strong\u003e입니다.\n이 Sample들을 Fast R-CNN과 동일하게 \u003cstrong\u003eRoI Pooling\u003c/strong\u003e을 한 후, \u003cstrong\u003eClassification, bBox Regression\u003c/strong\u003e을 진행합니다.\nPretrained된 CNN을 거쳐서 나온 \u003cstrong\u003eFeature Map\u003c/strong\u003e은 ZFNet 기준으로 256-d, VGG-16 기준으로 512-d를 갖게됩니다.(여기서 d는 차원으로 이해하였습니다.)\u003c/p\u003e\n\u003cp\u003e이때 ZFNet은 ILSVRC 2013에서 우승한 CNN구조이고, VGG-16은 옥스퍼드 대학의 연구팀 VGG에 의해 개발된 VGGNet의 모델 중 하나로 16개 층으로 구성된 모델이며, 2014년 이미지넷 인식대회에서 준우승한 모델을 의미합니다.\n이 \u003cstrong\u003eFeature Map\u003c/strong\u003e을 k개의 Anchor box를 통해 영역을 정하고 \u003cstrong\u003eClassification Layer\u003c/strong\u003e와 \u003cstrong\u003ebBox Regression\u003c/strong\u003e을 거쳐서 물체가 위치한 곳을 학습하게 됩니다.\n\u003cstrong\u003e여기서 Classification Layer가 물체가 있는지 없는지만 확인하므로, Class의 수는 2입니다.\u003c/strong\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387417-111f9431-a58d-4e34-b0fc-868a4494c29e.PNG\" alt=\"18\"\u003e\u003c/p\u003e\n\u003ch1\u003e요약하자면...\u003c/h1\u003e\n\u003cp\u003e3가지 모델들을 정리해보자면 아래와 같습니다.\n발전 방향의 순서대로 각 모델의 장단점에 대해 정리하였습니다. 확실히 faster R-CNN으로 갈수록 사진당 속도가 빠르게 증가하였음을 확인할 수 있으며, 동시에 정확도로 볼 수 있는 mAP의 값 또한 미비하지만 증가했음을 확인할 수 있습니다.\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208604296-cb2fb558-8064-4358-8cd6-f797f6f389d5.PNG\" alt=\"19\"\u003e\n\u003cimg src=\"https://user-images.githubusercontent.com/84653623/208387438-e7b201d4-2153-488c-b1a5-e4e862f3871d.PNG\" alt=\"20\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003ePS. 추가 문의사항 및 질문은 환영합니다. 그를 통해 저도 더 성장할 수 있을테니까요. 긴 글 읽어주셔서 감사합니다.\u003c/p\u003e\n\u003ch1\u003eReference\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e논문\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1311.2524\"\u003eRich feature hierarchies for accurate object detection and semantic segmentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html\"\u003eFast R-CNN\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html\"\u003eFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"])</script><script>self.__next_f.push([1,"5:[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"article\",null,{\"className\":\"prose prose-lg dark:prose-invert max-w-none\",\"children\":[[\"$\",\"header\",null,{\"className\":\"mb-10 not-prose border-b border-gray-100 dark:border-gray-800 pb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-4 text-sm text-gray-500 flex items-center space-x-2\",\"children\":[[[\"$\",\"$Le\",null,{\"href\":\"/category/paperreview\",\"className\":\"font-medium text-blue-600 hover:underline\",\"children\":\"PaperReview\"}],[\"$\",\"span\",null,{\"children\":\"•\"}]],[\"$\",\"time\",null,{\"children\":\"$undefined\"}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl font-extrabold tracking-tight text-gray-900 dark:text-white mb-4\",\"children\":\"[PaperReview] RCNN, Fast R-CNN, Faster R-CNN\"}]]}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$19\"}}]]}],\"$L1a\"]}]\n"])</script><script>self.__next_f.push([1,"1b:I[50718,[\"/_next/static/chunks/796e69ae18b2784c.js\",\"/_next/static/chunks/a8f82a9835eb887b.js\"],\"default\"]\n1a:[\"$\",\"$L1b\",null,{}]\n"])</script><script>self.__next_f.push([1,"a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"1c:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/247eb132b7f7b574.js\"],\"IconMark\"]\nc:[[\"$\",\"title\",\"0\",{\"children\":\"[PaperReview] RCNN, Fast R-CNN, Faster R-CNN\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"RCNN, Fast R-CNN, Faster R-CNN Paper 리뷰\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico?favicon.0b3bf435.ico\",\"sizes\":\"256x256\",\"type\":\"image/x-icon\"}],[\"$\",\"$L1c\",\"3\",{}]]\n8:null\n"])</script></body></html>