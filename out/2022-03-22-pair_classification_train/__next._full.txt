1:"$Sreact.fragment"
2:I[39756,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"default"]
3:I[37457,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"default"]
5:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"OutletBoundary"]
6:"$Sreact.suspense"
8:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"ViewportBoundary"]
a:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"MetadataBoundary"]
c:I[68027,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/865c404e1d9a0c65.js"],"default"]
:HL["/_next/static/chunks/2f40a2027cd59172.css","style"]
:HL["/_next/static/chunks/b9ef641e76e3a351.css","style"]
0:{"P":null,"b":"8lJiHtAmlyU3nNFMbG8_k","c":["","2022-03-22-pair_classification_train"],"q":"","i":false,"f":[[["",{"children":[["slug","2022-03-22-pair_classification_train","d"],{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/2f40a2027cd59172.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"className":"min-h-screen flex flex-col font-sans","children":["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}],{"children":[["$","$1","c",{"children":[null,["$","$L2",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L3",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["$","$1","c",{"children":["$L4",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/b9ef641e76e3a351.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/6b8d09032578b975.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"children":["$","$6",null,{"name":"Next.MetadataOutlet","children":"$@7"}]}]]}],{},null,false,false]},null,false,false]},null,false,false],["$","$1","h",{"children":[null,["$","$L8",null,{"children":"$@9"}],["$","div",null,{"hidden":true,"children":["$","$La",null,{"children":["$","$6",null,{"name":"Next.Metadata","children":"$@b"}]}]}],null]}],false]],"m":"$undefined","G":["$c",[]],"S":true}
d:T2695,Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)
Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)
Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)
Requirement already satisfied: Korpora>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)
Requirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)
Requirement already satisfied: flask-cors>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)
Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)
Requirement already satisfied: flask-ngrok>=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)
Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (1.21.5)
Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2022.2.0)
Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (4.62.3)
Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.3.0)
Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.7.2)
Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.18.2)
Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (21.3)
Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.8.0)
Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (5.4.1)
Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.10.3)
Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.4.0)
Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.0.47)
Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (4.11.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2019.12.20)
Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (3.6.0)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2.23.0)
Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)
Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)
Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)
Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)
Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)
Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (3.8.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->ratsnlp) (3.10.0.2)
Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)
Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (2.0.1)
Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (0.6)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.4->ratsnlp) (3.0.7)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2021.10.8)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)
Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.35.0)
Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.17.3)
Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (57.4.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.6)
Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.0.0)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.8.1)
Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.37.1)
Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.44.0)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.3.6)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.6.1)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.2.4)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.2.8)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.8)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.1)
Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0->ratsnlp) (3.7.0)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.8)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.2.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (6.0.2)
Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (0.13.0)
Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (2.0.12)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.2.0)
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (21.4.0)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.0)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.7.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (4.0.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.1.0)
4:["$","div",null,{"className":"flex gap-10","children":[["$","article",null,{"className":"flex-1 min-w-0 prose prose-slate dark:prose-invert max-w-none","children":[["$","header",null,{"className":"mb-8 not-prose border-b pb-8","children":[["$","h1",null,{"className":"text-4xl font-bold mb-4","children":"[NLP] 문장 쌍 분류 모델 학습하기"}],["$","div",null,{"className":"flex items-center gap-4 text-sm text-gray-500 dark:text-gray-400","children":[["$","time",null,{"dateTime":"$D2022-03-22T00:00:00.000Z","children":"March 22, 2022"}],"$undefined"]}]]}],[["$","p","p-0",{"children":"자연어처리의 예제를 학습하여 보자.\n다음은 이전 글에서 설명하였던 문장 쌍 분류 모델을 구현한 것이다."}],"\n",["$","p","p-1",{"children":"본 파일은 이기창님의 'Do it! 자연어 처리'에 기초하여 작성되었다. :)"}],"\n",["$","h1","h1-0",{"id":"","className":"text-3xl font-bold mt-8 mb-4","children":"문장 쌍 분류 모델 학습하기"}],"\n",["$","p","p-2",{"children":"전제와 가설을 검증하는 자연어 추론 모델 만들기"}],"\n",["$","h2","h2-0",{"id":"1","className":"text-2xl font-bold mt-8 mb-4","children":"1. 각종 설정하기"}],"\n",["$","h3","h3-0",{"id":"t","className":"text-xl font-bold mt-6 mb-3","children":"TPU 관련 패키지 설치"}],"\n",["$","p","p-3",{"children":"코랩 노트북 초기화 과정에서 하드웨어 가속기로 TPU를 선택했다면 다음 코드를 실행하고, GPU를 선택했다면 실행하지 않는다."}],"\n",["$","h4","h4-0",{"children":"code 3-0"}],"\n",["$","pre","pre-0",{"children":["$","code","code-0",{"className":"hljs language-python","children":["!pip install cloud-tpu-client==",["$","span","span-0",{"className":"hljs-number","children":"0.10"}]," https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-",["$","span","span-1",{"className":"hljs-number","children":"1.9"}],"-cp37-cp37m-linux_x86_64.whl\n"]}]}],"\n",["$","h3","h3-1",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"의존성 패키지 설치"}],"\n",["$","p","p-4",{"children":"code 3-1을 실행해 TPU 이외의 의존성 있는 패키지를 설치한다."}],"\n",["$","h4","h4-1",{"children":"code 3-1"}],"\n",["$","pre","pre-1",{"children":["$","code","code-0",{"className":"hljs language-python","children":"!pip install ratsnlp\n"}]}],"\n",["$","pre","pre-2",{"children":["$","code","code-0",{"children":"$d"}]}],"\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e","\n","$L3f","\n","$L40","\n","$L41","\n","$L42","\n","$L43","\n","$L44","\n","$L45","\n","$L46","\n","$L47","\n","$L48","\n","$L49","\n","$L4a","\n","$L4b","\n","$L4c","\n","$L4d","\n","$L4e","\n","$L4f","\n","$L50","\n","$L51","\n","$L52","\n","$L53","\n","$L54","\n","$L55","\n","$L56","\n","$L57","\n","$L58","\n","$L59","\n","$L5a"],"$L5b"]}],"$L5c"]}]
5f:I[24170,["/_next/static/chunks/6b8d09032578b975.js"],"default"]
60:I[55132,["/_next/static/chunks/6b8d09032578b975.js"],"default"]
e:["$","h3","h3-2",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"구글 드라이브와 연결"}]
f:["$","p","p-5",{"children":"코랩 노트북은 일정시간 사용하지 않으면 당시까지의 모든 결과물이 날아갈 수 있다. 모델 체크포인트 등을 저장해 주기 위해 자신의 구글 드라이브를 코랩 노트북과 연결한다."}]
10:["$","h4","h4-2",{"children":"code 3-2"}]
11:["$","pre","pre-3",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," google.colab ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," drive\ndrive.mount(",["$","span","span-2",{"className":"hljs-string","children":"'/gdrive'"}],", force_remount=",["$","span","span-3",{"className":"hljs-literal","children":"True"}],")\n"]}]}]
12:["$","pre","pre-4",{"children":["$","code","code-0",{"children":"Mounted at /gdrive\n"}]}]
13:["$","h3","h3-3",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"모델 환경 설정"}]
14:["$","p","p-6",{"children":"kcbert-base모델을 인공지능 기업 업스테이지가 공개한 KLUE-NLI데이터* 로 파인튜닝"}]
15:["$","blockquote","blockquote-0",{"children":["\n",["$","p","p-0",{"children":["*",["$","em","em-0",{"children":"klue-benchmark.com/tasks/68/data/description"}]]}],"\n"]}]
16:["$","h4","h4-3",{"children":"code 3-3"}]
17:["$","pre","pre-5",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"import"}]," torch\n",["$","span","span-1",{"className":"hljs-keyword","children":"from"}]," ratsnlp.nlpbook.classification ",["$","span","span-2",{"className":"hljs-keyword","children":"import"}]," ClassificationTrainArguments\nargs = ClassificationTrainArguments(\n    pretrained_model_name=",["$","span","span-3",{"className":"hljs-string","children":"\"beomi/kcbert-base\""}],",\n    downstream_task_name=",["$","span","span-4",{"className":"hljs-string","children":"\"pair-classification\""}],",\n    downstream_corpus_name=",["$","span","span-5",{"className":"hljs-string","children":"\"klue-nli\""}],",\n    downstream_model_dir=",["$","span","span-6",{"className":"hljs-string","children":"\"/gdrive/My Drive/nlpbook/checkpoint-paircls\""}],",\n    batch_size=",["$","span","span-7",{"className":"hljs-number","children":"32"}]," ",["$","span","span-8",{"className":"hljs-keyword","children":"if"}]," torch.cuda.is_available() ",["$","span","span-9",{"className":"hljs-keyword","children":"else"}]," ",["$","span","span-10",{"className":"hljs-number","children":"4"}],",\n    learning_rate=",["$","span","span-11",{"className":"hljs-number","children":"5e-5"}],",\n    max_seq_length=",["$","span","span-12",{"className":"hljs-number","children":"64"}],",\n    epochs=",["$","span","span-13",{"className":"hljs-number","children":"5"}],",\n    tpu_cores=",["$","span","span-14",{"className":"hljs-number","children":"0"}]," ",["$","span","span-15",{"className":"hljs-keyword","children":"if"}]," torch.cuda.is_available() ",["$","span","span-16",{"className":"hljs-keyword","children":"else"}]," ",["$","span","span-17",{"className":"hljs-number","children":"8"}],",\n    seed=",["$","span","span-18",{"className":"hljs-number","children":"7"}],",\n)\n"]}]}]
18:["$","h3","h3-4",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"랜덤 시드 고정"}]
19:["$","p","p-7",{"children":"랜덤 시드를 설정"}]
1a:["$","p","p-8",{"children":["code 3-4는 ",["$","code","code-0",{"children":"args"}],"에 지정된 시드로 고정하는 역할을 한다."]}]
1b:["$","h4","h4-4",{"children":"code 3-4"}]
1c:["$","pre","pre-6",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," ratsnlp ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," nlpbook\nnlpbook.set_seed(args)\n"]}]}]
1d:["$","pre","pre-7",{"children":["$","code","code-0",{"children":"set seed: 7\n"}]}]
1e:["$","h3","h3-5",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"로거 설정"}]
1f:["$","p","p-9",{"children":"각종 로그를 출력하는 로거를 설정"}]
20:["$","h4","h4-5",{"children":"code 3-5"}]
21:["$","pre","pre-8",{"children":["$","code","code-0",{"className":"hljs language-python","children":"nlpbook.set_logger(args)\n"}]}]
22:["$","pre","pre-9",{"children":["$","code","code-0",{"children":"INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/My Drive/nlpbook/checkpoint-paircls', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0)\n"}]}]
23:["$","h2","h2-1",{"id":"2","className":"text-2xl font-bold mt-8 mb-4","children":"2. 말뭉치 내려받기"}]
24:["$","h3","h3-6",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"말뭉치 내려받기"}]
25:["$","p","p-10",{"children":["KLUE-NLI 데이터를 내려받는다. ",["$","code","code-0",{"children":"corpus_name"}],"에 해당하는 말뭉치(",["$","code","code-1",{"children":"klue_nli"}],")를 ",["$","code","code-2",{"children":"downstream_corpus_root_dir"}],"아래(",["$","code","code-3",{"children":"/root/Korpora"}],")에 저장해둔다."]}]
26:["$","h4","h4-6",{"children":"code 3-6"}]
27:["$","pre","pre-10",{"children":["$","code","code-0",{"className":"hljs language-python","children":"nlpbook.download_downstream_dataset(args)\n"}]}]
28:["$","pre","pre-11",{"children":["$","code","code-0",{"children":"Downloading: 100%|██████████| 12.3M/12.3M [00:00<00:00, 42.3MB/s]\nDownloading: 100%|██████████| 1.47M/1.47M [00:00<00:00, 35.6MB/s]\n"}]}]
29:["$","h2","h2-2",{"id":"3","className":"text-2xl font-bold mt-8 mb-4","children":"3. 토크나이저 준비하기"}]
2a:["$","h3","h3-7",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"토크나이저 준비"}]
2b:["$","p","p-11",{"children":["code 3-7을 실행해 ",["$","code","code-0",{"children":"pretrained_model_name"}],"에 해당하는 모델(",["$","strong","strong-0",{"children":"kcbert-base"}],")이 사용하는 토크나이저를 선언한다."]}]
2c:["$","h4","h4-7",{"children":"code 3-7"}]
2d:["$","pre","pre-12",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," transformers ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\n    args.pretrained_model_name,\n    do_lower_case=",["$","span","span-2",{"className":"hljs-literal","children":"False"}],",\n)\n"]}]}]
2e:["$","pre","pre-13",{"children":["$","code","code-0",{"children":"Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]\n\n\n\nDownloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]\n\n\n\nDownloading:   0%|          | 0.00/619 [00:00<?, ?B/s]\n"}]}]
2f:["$","h2","h2-3",{"id":"4","className":"text-2xl font-bold mt-8 mb-4","children":"4. 데이터 전처리하기"}]
30:["$","h3","h3-8",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"학습 데이터셋 구축"}]
31:["$","p","p-12",{"children":["code 3-8을 수행하면 학습 데이터셋 을 만들 수 있다. ",["$","strong","strong-0",{"children":"KlueNLICorpus"}]," 클래스는 JSON 파일 형식의 KLUE-NLI 데이터를 문장(전제 + 가설)과 레이블(참, 거짓, 중립)로 읽어들인다. ",["$","code","code-0",{"children":"KlueNLICorpus"}],"는 ",["$","code","code-1",{"children":"ClassificationDataset"}],"이 요구하면 이 문장과 레이블을 ",["$","code","code-2",{"children":"ClassificationDataset"}],"에 제공한다."]}]
32:["$","h4","h4-8",{"children":"code 3-8"}]
33:["$","pre","pre-14",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," ratsnlp.nlpbook.paircls ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," KlueNLICorpus\n",["$","span","span-2",{"className":"hljs-keyword","children":"from"}]," ratsnlp.nlpbook.classification ",["$","span","span-3",{"className":"hljs-keyword","children":"import"}]," ClassificationDataset\ncorpus = KlueNLICorpus()\ntrain_dataset = ClassificationDataset(\n    args=args,\n    corpus=corpus,\n    tokenizer=tokenizer,\n    mode=",["$","span","span-4",{"className":"hljs-string","children":"\"train\""}],",\n)\n"]}]}]
5d:T1c34,INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli
INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json
INFO:ratsnlp:tokenize sentences, it could take a lot of time...
INFO:ratsnlp:tokenize sentences [took 15.747 s]
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: contradiction
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.
INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.
INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: contradiction
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
INFO:ratsnlp:*** Example ***
INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.
INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
INFO:ratsnlp:label: neutral
INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
INFO:ratsnlp:Saving features into cached file, it could take a lot of time...
INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.934 s]
34:["$","pre","pre-15",{"children":["$","code","code-0",{"children":"$5d"}]}]
35:["$","h3","h3-9",{"id":"c","className":"text-xl font-bold mt-6 mb-3","children":"ClassificationDataset 클래스가 하는 역할"}]
36:["$","p","p-13",{"children":["이 클래스는 ",["$","strong","strong-0",{"children":"KlueNLICorpus"}],"와 code 3-7에서 선언해 둔 ",["$","strong","strong-1",{"children":"토크나이저"}],"를 품고 있다."]}]
37:["$","p","p-14",{"children":[["$","strong","strong-0",{"children":"ClassificationDataset"}],"은 제공받은 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습할 수 있는 형태(",["$","strong","strong-1",{"children":"ClassificationFeature"}],")로 가공한다.\n다시 말해, 전제와 가설 2개 문장을 각각 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수로 바꿔주는 역할을 한다."]}]
38:["$","p","p-15",{"children":["(",["$","strong","strong-0",{"children":"entailment: 0, contradiction: 1, neutral: 2"}],")"]}]
39:["$","p","p-16",{"children":"KlueNLICorpus와 classificationDataset의 역할과 자세한 구현 내용은 아래의 링크를 참고하자!\n(현재는 교재링크를 올려두지만, 추후 본인의 깃허브에 구현 예정)"}]
3a:["$","ul","ul-0",{"children":["\n",["$","li","li-0",{"children":"ratsgo.github.io/nlpbook/docs/pair_cls/detail"}],"\n"]}]
3b:["$","h3","h3-10",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"학습 데이터 로더 구축"}]
3c:["$","p","p-17",{"children":["code 3-9를 실행하면 학습할 때 쓰이는 데이터 로더를 만들 수 있다. 학습용 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배크 크기(",["$","em","em-0",{"children":"code 3-3 에서 정의한"}]," ",["$","code","code-0",{"children":"args"}],["$","em","em-1",{"children":"의"}]," ",["$","code","code-1",{"children":"batch_size"}],")만큼의 인스턴스들을 비복원(",["$","code","code-2",{"children":"replacement=False"}],")랜덤 추출(",["$","code","code-3",{"children":"RandomSampler"}],")한 뒤 이를 배치 형태로 가공(",["$","code","code-4",{"children":"nlpbook.data_collator"}],")해 모델에 공급하는 역할을 수행한다."]}]
3d:["$","h4","h4-9",{"children":"code 3-9"}]
3e:["$","pre","pre-16",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," torch.utils.data ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," DataLoader, RandomSampler\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=args.batch_size,\n    sampler=RandomSampler(train_dataset, replacement=",["$","span","span-2",{"className":"hljs-literal","children":"False"}],"),\n    collate_fn=nlpbook.data_collator,\n    drop_last=",["$","span","span-3",{"className":"hljs-literal","children":"False"}],",\n    num_workers=args.cpu_workers,\n)\n"]}]}]
3f:["$","h3","h3-11",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"평가용 데이터 로더 구축"}]
40:["$","p","p-18",{"children":["code 3-10을 실행하면 평가용 데이터 로더를 구축할 수 있다. 평가용 데이터 로더는 배치 크기(code 3-3에서 정의한 ",["$","code","code-0",{"children":"args"}],"의 ",["$","code","code-1",{"children":"batch_size"}],")만큼의 인스턴스를 순서대로 추출(",["$","strong","strong-0",{"children":"Sequential Sampler"}],")한 후 이를 배치 형태로 가공(",["$","code","code-2",{"children":"nlpbook.data_collator"}],")해 모델에 공급한다."]}]
41:["$","h4","h4-10",{"children":"code 3-10"}]
42:["$","pre","pre-17",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," torch.utils.data ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," SequentialSampler\nval_dataset = ClassificationDataset(\n    args=args,\n    corpus=corpus,\n    tokenizer=tokenizer,\n    mode=",["$","span","span-2",{"className":"hljs-string","children":"\"test\""}],",\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=args.batch_size,\n    sampler=SequentialSampler(val_dataset),\n    collate_fn=nlpbook.data_collator,\n    drop_last=",["$","span","span-3",{"className":"hljs-literal","children":"False"}],",\n    num_workers=args.cpu_workers,\n)\n"]}]}]
43:["$","pre","pre-18",{"children":["$","code","code-0",{"children":"INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.116 s]\n"}]}]
44:["$","h2","h2-4",{"id":"5","className":"text-2xl font-bold mt-8 mb-4","children":"5. 모델 불러오기"}]
45:["$","h3","h3-12",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"모델 초기화"}]
46:["$","p","p-19",{"children":["code 3-11을 수행해 모델을 초기화 한다. 프리트레인을 마친 BERT로 ",["$","code","code-0",{"children":"kcbert-base"}],"를 사용한다. code 3-3에서 ",["$","code","code-1",{"children":"pretrained_model_name"}],"을 ",["$","code","code-2",{"children":"beomi/kcber-base"}],"로 지정했기 때문이다. 물론 허깅페이스 모델 허브에 등록된 모델이라면 다른 모델 역시 사용할 수 있다."]}]
47:["$","p","p-20",{"children":[["$","code","code-0",{"children":"BertForSequenceClassification"}],"은 프리트레인을 마친 BERT모델 위에 문서 분류용 태스크 모듈을 덧붙인 형태의 모델 클래스이다. 이 클래스는 ",["$","strong","strong-0",{"children":"문서 분류 모델"}],"에서 사용한 것과 동일하다."]}]
48:["$","h4","h4-11",{"children":"code 3-11"}]
49:["$","pre","pre-19",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," transformers ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," BertConfig, BertForSequenceClassification\npretrained_model_config = BertConfig.from_pretrained(\n    args.pretrained_model_name,\n    num_labels=corpus.num_labels,\n)\nmodel = BertForSequenceClassification.from_pretrained(\n    args.pretrained_model_name,\n    config=pretrained_model_config,\n)\n"]}]}]
5e:T508,Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]


Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
4a:["$","pre","pre-20",{"children":["$","code","code-0",{"children":"$5e"}]}]
4b:["$","h2","h2-5",{"id":"6","className":"text-2xl font-bold mt-8 mb-4","children":"6. 모델 학습시키기"}]
4c:["$","p","p-21",{"children":["code 3-12를 실행하면 문장 쌍 분류용 태스크를 정의할 수 있다. 모델은 code 3-11에서 준비한 모델 클래스를 ",["$","code","code-0",{"children":"ClassificationTask"}],"에 포함한다. ",["$","code","code-1",{"children":"ClassificationTask"}]," 클래스에는 옵티마이저, 러닝 레이트 스케줄러가 정의 되 있는데, 옵티마이저로는 아담(",["$","code","code-2",{"children":"Adam"}],"), 러닝 레이트 스케줄러로는 ",["$","code","code-3",{"children":"ExponentialLR"}],"을 사용한다."]}]
4d:["$","h3","h3-13",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"태스크 정의"}]
4e:["$","h4","h4-12",{"children":"code 3-12"}]
4f:["$","pre","pre-21",{"children":["$","code","code-0",{"className":"hljs language-python","children":[["$","span","span-0",{"className":"hljs-keyword","children":"from"}]," ratsnlp.nlpbook.classification ",["$","span","span-1",{"className":"hljs-keyword","children":"import"}]," ClassificationTask\ntask = ClassificationTask(model, args)\n"]}]}]
50:["$","h3","h3-14",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"트레이너 정의"}]
51:["$","p","p-22",{"children":["code 3-13을 실행하면 트레이너를 정의할 수 있다. 이 트레이너는 ",["$","strong","strong-0",{"children":"파이토치 라이트닝 라이브러리"}],"의 도움을 받아 ",["$","strong","strong-1",{"children":"GPU/TPU 설정"}],", ",["$","strong","strong-2",{"children":"로그 및 체크포인트"}]," 등 귀찮은 설정들을 알아서 해준다."]}]
52:["$","h4","h4-13",{"children":"code 3-13"}]
53:["$","pre","pre-22",{"children":["$","code","code-0",{"className":"hljs language-python","children":"trainer = nlpbook.get_trainer(args)\n"}]}]
54:["$","pre","pre-23",{"children":["$","code","code-0",{"children":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\n"}]}]
55:["$","h3","h3-15",{"id":"","className":"text-xl font-bold mt-6 mb-3","children":"학습 개시"}]
56:["$","p","p-23",{"children":["code 3-14와 같이 트레이너의 ",["$","code","code-0",{"children":"fit()"}],"함수를 호출하면 학습을 시작한다."]}]
57:["$","h4","h4-14",{"children":"code 3-14"}]
58:["$","pre","pre-24",{"children":["$","code","code-0",{"className":"hljs language-python","children":"trainer.fit(\n    task,\n    train_dataloader=train_dataloader,\n    val_dataloaders=val_dataloader,\n)\n"}]}]
59:["$","pre","pre-25",{"children":["$","code","code-0",{"children":"LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name  | Type                          | Params\n--------------------------------------------------------\n0 | model | BertForSequenceClassification | 108 M \n--------------------------------------------------------\n108 M     Trainable params\n0         Non-trainable params\n108 M     Total params\n435.683   Total estimated model params size (MB)\n\n\n\nTraining: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n\n\n\nValidating: 0it [00:00, ?it/s]\n"}]}]
5a:["$","h1","h1-1",{"id":"object-object","className":"text-3xl font-bold mt-8 mb-4","children":[["$","strong","strong-0",{"children":"문장 쌍 분류"}],"는 문서 분류 과제와 태스크 모듈 구조 등에서 본질적으로 다르지 않다. 입력문서가 1개냐(문서분류), 2개냐(문장 쌍 분류)의 차이가 있을 뿐이다."]}]
5b:["$","$L5f",null,{}]
61:T71d2,
자연어처리의 예제를 학습하여 보자.
다음은 이전 글에서 설명하였던 문장 쌍 분류 모델을 구현한 것이다.

본 파일은 이기창님의 'Do it! 자연어 처리'에 기초하여 작성되었다. :)

# 문장 쌍 분류 모델 학습하기

전제와 가설을 검증하는 자연어 추론 모델 만들기

## 1. 각종 설정하기

### TPU 관련 패키지 설치

코랩 노트북 초기화 과정에서 하드웨어 가속기로 TPU를 선택했다면 다음 코드를 실행하고, GPU를 선택했다면 실행하지 않는다.

#### code 3-0


```python
!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl
```

### 의존성 패키지 설치

code 3-1을 실행해 TPU 이외의 의존성 있는 패키지를 설치한다.

#### code 3-1


```python
!pip install ratsnlp
```

    Requirement already satisfied: ratsnlp in /usr/local/lib/python3.7/dist-packages (1.0.1)
    Requirement already satisfied: pytorch-lightning==1.3.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.3.4)
    Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.10.0+cu111)
    Requirement already satisfied: Korpora>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.2.0)
    Requirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (1.1.4)
    Requirement already satisfied: flask-cors>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (3.0.10)
    Requirement already satisfied: transformers==4.10.0 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (4.10.0)
    Requirement already satisfied: flask-ngrok>=0.0.25 in /usr/local/lib/python3.7/dist-packages (from ratsnlp) (0.0.25)
    Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (1.21.5)
    Requirement already satisfied: fsspec[http]>=2021.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2022.2.0)
    Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (4.62.3)
    Requirement already satisfied: pyDeprecate==0.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.3.0)
    Requirement already satisfied: torchmetrics>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.7.2)
    Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (0.18.2)
    Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (21.3)
    Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.8.0)
    Requirement already satisfied: PyYAML<=5.4.1,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (5.4.1)
    Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.10.3)
    Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.4.0)
    Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (0.0.47)
    Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (4.11.1)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2019.12.20)
    Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (3.6.0)
    Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0->ratsnlp) (2.23.0)
    Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)
    Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)
    Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)
    Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)
    Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)
    Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (3.8.1)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0->ratsnlp) (3.10.0.2)
    Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)
    Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (2.0.1)
    Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.7/dist-packages (from Korpora>=0.2.0->ratsnlp) (0.6)
    Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning==1.3.4->ratsnlp) (3.0.7)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2021.10.8)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0->ratsnlp) (3.0.4)
    Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.35.0)
    Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.17.3)
    Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (57.4.0)
    Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.6)
    Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.0.0)
    Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.8.1)
    Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.37.1)
    Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.44.0)
    Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.3.6)
    Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.6.1)
    Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.2.4)
    Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.2.8)
    Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (4.8)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.1)
    Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0->ratsnlp) (3.7.0)
    Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (0.4.8)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.4->ratsnlp) (3.2.0)
    Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (6.0.2)
    Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (0.13.0)
    Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (2.0.12)
    Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.2.0)
    Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (21.4.0)
    Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.3.0)
    Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (1.7.2)
    Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.4.0->pytorch-lightning==1.3.4->ratsnlp) (4.0.2)
    Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.1.0)
    

### 구글 드라이브와 연결

코랩 노트북은 일정시간 사용하지 않으면 당시까지의 모든 결과물이 날아갈 수 있다. 모델 체크포인트 등을 저장해 주기 위해 자신의 구글 드라이브를 코랩 노트북과 연결한다.

#### code 3-2


```python
from google.colab import drive
drive.mount('/gdrive', force_remount=True)
```

    Mounted at /gdrive
    

### 모델 환경 설정

kcbert-base모델을 인공지능 기업 업스테이지가 공개한 KLUE-NLI데이터* 로 파인튜닝


> **klue-benchmark.com/tasks/68/data/description*



#### code 3-3


```python
import torch
from ratsnlp.nlpbook.classification import ClassificationTrainArguments
args = ClassificationTrainArguments(
    pretrained_model_name="beomi/kcbert-base",
    downstream_task_name="pair-classification",
    downstream_corpus_name="klue-nli",
    downstream_model_dir="/gdrive/My Drive/nlpbook/checkpoint-paircls",
    batch_size=32 if torch.cuda.is_available() else 4,
    learning_rate=5e-5,
    max_seq_length=64,
    epochs=5,
    tpu_cores=0 if torch.cuda.is_available() else 8,
    seed=7,
)
```

### 랜덤 시드 고정

랜덤 시드를 설정

code 3-4는 `args`에 지정된 시드로 고정하는 역할을 한다.

#### code 3-4


```python
from ratsnlp import nlpbook
nlpbook.set_seed(args)
```

    set seed: 7
    

### 로거 설정

각종 로그를 출력하는 로거를 설정

#### code 3-5


```python
nlpbook.set_logger(args)
```

    INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/My Drive/nlpbook/checkpoint-paircls', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=5, batch_size=32, cpu_workers=2, fp16=False, tpu_cores=0)
    

## 2. 말뭉치 내려받기

### 말뭉치 내려받기

KLUE-NLI 데이터를 내려받는다. `corpus_name`에 해당하는 말뭉치(`klue_nli`)를 `downstream_corpus_root_dir`아래(`/root/Korpora`)에 저장해둔다.

#### code 3-6


```python
nlpbook.download_downstream_dataset(args)
```

    Downloading: 100%|██████████| 12.3M/12.3M [00:00<00:00, 42.3MB/s]
    Downloading: 100%|██████████| 1.47M/1.47M [00:00<00:00, 35.6MB/s]
    

## 3. 토크나이저 준비하기

### 토크나이저 준비

code 3-7을 실행해 `pretrained_model_name`에 해당하는 모델(**kcbert-base**)이 사용하는 토크나이저를 선언한다.

#### code 3-7


```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(
    args.pretrained_model_name,
    do_lower_case=False,
)
```


    Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]


## 4. 데이터 전처리하기

### 학습 데이터셋 구축

code 3-8을 수행하면 학습 데이터셋 을 만들 수 있다. **KlueNLICorpus** 클래스는 JSON 파일 형식의 KLUE-NLI 데이터를 문장(전제 + 가설)과 레이블(참, 거짓, 중립)로 읽어들인다. `KlueNLICorpus`는 `ClassificationDataset`이 요구하면 이 문장과 레이블을 `ClassificationDataset`에 제공한다.



#### code 3-8


```python
from ratsnlp.nlpbook.paircls import KlueNLICorpus
from ratsnlp.nlpbook.classification import ClassificationDataset
corpus = KlueNLICorpus()
train_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="train",
)
```

    INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli
    INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json
    INFO:ratsnlp:tokenize sentences, it could take a lot of time...
    INFO:ratsnlp:tokenize sentences [took 15.747 s]
    INFO:ratsnlp:*** Example ***
    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.
    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
    INFO:ratsnlp:label: contradiction
    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
    INFO:ratsnlp:*** Example ***
    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.
    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
    INFO:ratsnlp:label: neutral
    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
    INFO:ratsnlp:*** Example ***
    INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.
    INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
    INFO:ratsnlp:label: neutral
    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
    INFO:ratsnlp:*** Example ***
    INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.
    INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
    INFO:ratsnlp:label: contradiction
    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)
    INFO:ratsnlp:*** Example ***
    INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.
    INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
    INFO:ratsnlp:label: neutral
    INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)
    INFO:ratsnlp:Saving features into cached file, it could take a lot of time...
    INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.934 s]
    

### ClassificationDataset 클래스가 하는 역할

이 클래스는 **KlueNLICorpus**와 code 3-7에서 선언해 둔 **토크나이저**를 품고 있다.  

**ClassificationDataset**은 제공받은 문장과 레이블 각각을 tokenizer를 활용해 모델이 학습할 수 있는 형태(**ClassificationFeature**)로 가공한다.
다시 말해, 전제와 가설 2개 문장을 각각 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수로 바꿔주는 역할을 한다.

(**entailment: 0, contradiction: 1, neutral: 2**)


KlueNLICorpus와 classificationDataset의 역할과 자세한 구현 내용은 아래의 링크를 참고하자!
(현재는 교재링크를 올려두지만, 추후 본인의 깃허브에 구현 예정)

- ratsgo.github.io/nlpbook/docs/pair_cls/detail 

### 학습 데이터 로더 구축

code 3-9를 실행하면 학습할 때 쓰이는 데이터 로더를 만들 수 있다. 학습용 데이터 로더는 ClassificationDataset 클래스가 들고 있는 전체 인스턴스 가운데 배크 크기(*code 3-3 에서 정의한* `args`*의* `batch_size`)만큼의 인스턴스들을 비복원(`replacement=False`)랜덤 추출(`RandomSampler`)한 뒤 이를 배치 형태로 가공(`nlpbook.data_collator`)해 모델에 공급하는 역할을 수행한다. 

#### code 3-9


```python
from torch.utils.data import DataLoader, RandomSampler
train_dataloader = DataLoader(
    train_dataset,
    batch_size=args.batch_size,
    sampler=RandomSampler(train_dataset, replacement=False),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)
```

### 평가용 데이터 로더 구축

code 3-10을 실행하면 평가용 데이터 로더를 구축할 수 있다. 평가용 데이터 로더는 배치 크기(code 3-3에서 정의한 `args`의 `batch_size`)만큼의 인스턴스를 순서대로 추출(**Sequential Sampler**)한 후 이를 배치 형태로 가공(`nlpbook.data_collator`)해 모델에 공급한다.

#### code 3-10


```python
from torch.utils.data import SequentialSampler
val_dataset = ClassificationDataset(
    args=args,
    corpus=corpus,
    tokenizer=tokenizer,
    mode="test",
)
val_dataloader = DataLoader(
    val_dataset,
    batch_size=args.batch_size,
    sampler=SequentialSampler(val_dataset),
    collate_fn=nlpbook.data_collator,
    drop_last=False,
    num_workers=args.cpu_workers,
)
```

    INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.116 s]
    

## 5. 모델 불러오기

### 모델 초기화

code 3-11을 수행해 모델을 초기화 한다. 프리트레인을 마친 BERT로 `kcbert-base`를 사용한다. code 3-3에서 `pretrained_model_name`을 `beomi/kcber-base`로 지정했기 때문이다. 물론 허깅페이스 모델 허브에 등록된 모델이라면 다른 모델 역시 사용할 수 있다.

`BertForSequenceClassification`은 프리트레인을 마친 BERT모델 위에 문서 분류용 태스크 모듈을 덧붙인 형태의 모델 클래스이다. 이 클래스는 **문서 분류 모델**에서 사용한 것과 동일하다.

#### code 3-11


```python
from transformers import BertConfig, BertForSequenceClassification
pretrained_model_config = BertConfig.from_pretrained(
    args.pretrained_model_name,
    num_labels=corpus.num_labels,
)
model = BertForSequenceClassification.from_pretrained(
    args.pretrained_model_name,
    config=pretrained_model_config,
)
```


    Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]


    Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
    

## 6. 모델 학습시키기

code 3-12를 실행하면 문장 쌍 분류용 태스크를 정의할 수 있다. 모델은 code 3-11에서 준비한 모델 클래스를 `ClassificationTask`에 포함한다. `ClassificationTask` 클래스에는 옵티마이저, 러닝 레이트 스케줄러가 정의 되 있는데, 옵티마이저로는 아담(`Adam`), 러닝 레이트 스케줄러로는 `ExponentialLR`을 사용한다.

### 태스크 정의

#### code 3-12


```python
from ratsnlp.nlpbook.classification import ClassificationTask
task = ClassificationTask(model, args)
```

### 트레이너 정의

code 3-13을 실행하면 트레이너를 정의할 수 있다. 이 트레이너는 **파이토치 라이트닝 라이브러리**의 도움을 받아 **GPU/TPU 설정**, **로그 및 체크포인트** 등 귀찮은 설정들을 알아서 해준다.

#### code 3-13


```python
trainer = nlpbook.get_trainer(args)
```

    GPU available: True, used: True
    TPU available: False, using: 0 TPU cores
    

### 학습 개시

code 3-14와 같이 트레이너의 `fit()`함수를 호출하면 학습을 시작한다.

#### code 3-14


```python
trainer.fit(
    task,
    train_dataloader=train_dataloader,
    val_dataloaders=val_dataloader,
)
```

    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
    
      | Name  | Type                          | Params
    --------------------------------------------------------
    0 | model | BertForSequenceClassification | 108 M 
    --------------------------------------------------------
    108 M     Trainable params
    0         Non-trainable params
    108 M     Total params
    435.683   Total estimated model params size (MB)
    


    Training: 0it [00:00, ?it/s]



    Validating: 0it [00:00, ?it/s]



    Validating: 0it [00:00, ?it/s]



    Validating: 0it [00:00, ?it/s]



    Validating: 0it [00:00, ?it/s]



    Validating: 0it [00:00, ?it/s]


# **문장 쌍 분류**는 문서 분류 과제와 태스크 모듈 구조 등에서 본질적으로 다르지 않다. 입력문서가 1개냐(문서분류), 2개냐(문장 쌍 분류)의 차이가 있을 뿐이다.

5c:["$","$L60",null,{"content":"$61"}]
9:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
b:[["$","title","0",{"children":"Sehoon's Workspace"}],["$","meta","1",{"name":"description","content":"Welcome to my page!"}]]
7:null
